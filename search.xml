<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[推荐系统之深度学习模型]]></title>
    <url>%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1、Wide&amp;Deep模型Wide &amp; Deep于2016年google在发表在DLRS 2016上的论文《Wide &amp; Deep Learning for Recommender Systems》提出。Wide&amp;Deep的核心思想是结合线性模型的记忆能力和DNN泛化能力，进而提升模型的整体性能。 1.1 背景推荐系统的主要挑战之一，是同时解决Memorization和Generalization，理解这两个概念是理解全文思路的关键，下面分别进行解释。 Memorization:面对拥有大规模离散sparse特征的CTR预估问题时，将特征进行非线性转换，然后再使用线性模型是在业界非常普遍的做法，最流行的即「LR+特征叉乘」。Memorization 通过一系列人工的特征叉乘（cross-product）来构造这些非线性特征，捕捉sparse特征之间的高阶相关性，即“记忆” 历史数据中曾共同出现过的特征对。 典型代表是LR模型，使用大量的原始sparse特征和叉乘特征作为输入，很多原始的dense特征通常也会被分桶离散化构造为sparse特征。这种做法的优点是模型可解释高，实现快速高效，特征重要度易于分析，在工业界已被证明是很有效的。Memorization的缺点是： 需要更多的人工设计； 可能出现过拟合。可以这样理解：如果将所有特征叉乘起来，那么几乎相当于纯粹记住每个训练样本，这个极端情况是最细粒度的叉乘，我们可以通过构造更粗粒度的特征叉乘来增强泛化性； 无法捕捉训练数据中未曾出现过的特征对。 Generalization:Generalization 为sparse特征学习低维的dense embeddings 来捕获特征相关性，学习到的embeddings 本身带有一定的语义信息。可以联想到NLP中的词向量，不同词的词向量有相关性，因此文中也称Generalization是基于相关性之间的传递。这类模型的代表是DNN和FM。 优点:更少的人工参与，对历史上没有出现的特征组合有更好的泛化性 。 缺点:但在推荐系统中，当user-item matrix非常稀疏时，例如有和独特爱好的users以及很小众的items，NN很难为users和items学习到有效的embedding。这种情况下，大部分user-item应该是没有关联的，但dense embedding 的方法还是可以得到对所有 user-item pair 的非零预测，因此导致 over-generalize并推荐不怎么相关的物品。此时Memorization就展示了优势，它可以“记住”这些特殊的特征组合。 Memorization根据历史行为数据，产生的推荐通常和用户已有行为的物品直接相关的物品。而Generalization会学习新的特征组合，提高推荐物品的多样性。 论文作者结合两者的优点，提出了一个新的学习算法——Wide &amp; Deep Learning，其中Wide &amp; Deep分别对应Memorization &amp; Generalization。 1.2 模型结构Wide &amp; Deep模型结合了LR和DNN，其框架图如下所示。Wide 该部分是广义线性模型，即$y=W^{T}[x,\phi(x)]+b$,其中$x$和$\phi(x)$表示原始特征与叉乘特征。Deep 该部分是前馈神经网络，网络会对一些sparse特征（如ID类特征）学习一个低维的dense embeddings（维度量级通常在O(10)到O(100)之间），然后和一些原始dense特征一起作为网络的输入。每一层隐层计算：$a^{l+1}=f(W^{l}a^{l}+b^{l})$,其中$a^{l},b^{l},W^{l}$是第$l$层的激活值、偏置和权重，$f$是激活函数。损失函数 模型选取logistic loss作为损失函数，此时Wide &amp; Deep最后的预测输出为：$$p(y=1|x)=\sigma(W^{T}_{wide}[x,\phi(x)]+W^{T}_{deep}a^{l_{f}}+b)$$ 其中$\sigma$表示sigmoid函数，$\phi(x)$表示叉乘特征，$a^{l_{f}}$表示NN最后一层激活值。 1.3 联合训练联合训练（Joint Training）和集成（Ensemble）是不同的，集成是每个模型单独训练，再将模型的结果汇合。相比联合训练，集成的每个独立模型都得学得足够好才有利于随后的汇合，因此每个模型的model size也相对更大。而联合训练的wide部分只需要作一小部分的特征叉乘来弥补deep部分的不足，不需要 一个full-size 的wide 模型。 在论文中，作者通过梯度的反向传播，使用 mini-batch stochastic optimization 训练参数，并对wide部分使用带L1正则的Follow- the-regularized-leader (FTRL) 算法，对deep部分使用 AdaGrad算法。 1.4 总结 详细解释了目前常用的 Wide 与 Deep 模型各自的优势：Memorization 与 Generalization。 结合 Wide 与 Deep 的优势，提出了联合训练的 Wide &amp; Deep Learning。相比单独的 Wide / Deep模型，实验显示了Wide &amp; Deep的有效性，并成功将之成功应用于Google Play的app推荐业务。 目前Wide 结合 Deep的思想已经非常流行，结构虽然简单，从业界的很多反馈来看，合理地结合自身业务借鉴该结构，实际效果确实是efficient。 1.5 参考 详解 Wide &amp; Deep 结构背后的动机 2、FM算法FM算法2010年由Rendle提出的，但是真正在各大厂大规模在CTR预估和推荐领域广泛使用，其实也就是最近几年的事。paper地址。在计算广告和推荐系统中，CTR预估(click-through rate)是非常重要的一个环节，判断一个物品是否进行推荐需要根据CTR预估的点击率排序决定。业界常用的方法有人工特征工程 + LR（Logistic Regression）、GBDT（Gradient Boosting Decision Tree） + LR、FM（Factorization Machine）和FFM（Field-aware Factorization Machine）模型。最近几年出现了很多基于FM改进的方法，如deepFM，FNN，PNN，DCN，xDeepFM等。 2.1 FM优缺点FM（factor Machine，因子分解机）算法是一种基于矩阵分解的机器学习算法，是为了解决大规模稀疏矩阵中特征组合问题。为什么需要FM呢？ 特征组合是许多机器学习建模过程中遇到的问题，如果直接建模，可能忽略特征与特征之间的关联信息，因此，可通通过构建新的交叉特征 这一特征组合方式提高模型效果。其实就是增加特征交叉项。 在一般的线性模型中，是各个特征独立思考的，没有考虑到特征之间的相互关系。但是实际上，大量特征之间是关联。 一般女性用户看化妆品服装之类的广告比较多，而男性更青睐各种球类装备。那很明显，女性这个特征与化妆品类服装类商品有很大的关联性，男性这个特征与球类装备的关联性更为密切。如果我们能将这些有关联的特征找出来，显然是很有意义的。 高维的稀疏矩阵是实际工程中常见的问题，并直接会导致计算量过大，特征权重更新缓慢。而FM的优势，就是在于这两方面问题的处理。首先是特征组合，通过两两特征组合，引入交叉特征，提高模型得分。其次是高维灾难，通过引入隐向量，对特征参数进行估计。 可以在非常稀疏的数据中进行合理的参数估计；FM模型的时间复杂度是线性的；FM是一个通用模型，它可以用于任何特征为实值的情况；同时解决了特征组合问题。 引入FM的目的：旨在解决稀疏数据下的特征组合问题。优势：高度稀疏的数据场景具有线性计算复杂度。 FM的优点：可以在非常稀疏的数据中，进行合理的参数估计；FM模型的复杂度是线性的，优化效果好，不需要像svm一样依赖于支持向量；FM是一个通用的模型，他咳哟用于任何特征为实值得情况。而其他得因式分解模型只能用于一些输入数据比较固定得情况。 但是FM也有其缺点，就是组合特征的泛化性比较差。 2.2 FM特征组合多项式模型是包含特征组合的最直观的模型。在多项式模型中，特征$x_{i}$和$x_{j}$的组合采用$x_{i}x_{j}$表示，即$x_{i}$和$x_{j}$都为非零时特征组合$x_{i}x_{j}$才有意义。本文只讨论二阶多项式模型，模型的表达式如下:$$y(X)=w_{0}+\sum^{n}_{i=1}w_{i}x_{i}+\sum^{n}_{i=1}\sum^{n}_{j=n+1}w_{ij}x_{i}x_{j}$$从公式中，我们可以看出组合特征得参数一共有1/2n(n-1)个，任意两个参数都是独立的。然而在数据稀疏性普遍存在的实际场景中，二次项参数的训练是很困难的。原因是：每个参数wij的训练都需要大量xi 和xj都非零的样本；由于样本数据本来就比较稀疏，满足xi，xj都非零的样本将会非常的少。训练样本不足，很容易导致参数wij不准确，最终将严重影响模型的性能。那么，交叉项参数的训练问题可以用矩阵分解来近似解决，有下面的公式:$$y(X)=w_{0}+\sum^{n}_{i=1}w_{i}x_{i}+\sum^{n}_{i=1}\sum^{n}_{j=n+1}&lt;v_{i},v_{j}&gt;x_{i}x_{j}$$其中模型需要估计的参数是：$$w\in R,w \in R^{n},V \in R^{n \times k}$$ $&lt;\cdot,\cdot&gt;$是两个$K$维向量的内积:$$&lt;v_{i},v_{j}&gt;=\sum^{k}_{k=1}v_{i,f}v_{j,f}$$ 对任意正定矩阵$W$ ，只要$k$足够大，就存在矩阵$W$ ，使得$W=VV^{T}$ 。然而在数据稀疏的情况下，应该选择较小的$k$，因为没有足够的数据来估计$w_{ij}$ 。限制k的大小提高了模型更好的泛化能力。 为什么说可以提高模型的泛化能力呢？假设我们要计算用户$A$与电影$ST$的交叉项，很显然，训练数据里没有这种情况，这样会导致$w_{A,ST}=0$ ，但是我们可以近似计算出$&lt;V_{A},V_{ST}&gt;$ 。首先，用户$B$ 和$C$ 有相似的向量$V_{B}$ 和$V_{C}$ ，因为他们对$SW$的预测评分比较相似， 所以$&lt;V_{B},V_{SW}&gt;$和$&lt;V_{C},V_{SW}&gt;$是相似的。用户$A$和$C$有不同的向量，因为对$TI$ 和 $SW$的预测评分完全不同。接下来，$ST$和$SW$的向量可能相似，因为用户$B$对这两部电影的评分也相似。最后可以看出，$&lt;V_{A},V_{ST}&gt;$与$&lt;V_{A},V_{SW}&gt;$ 是相似的。 如果按照上述的方式计算特征交叉时间复杂度是$O(kn^{2})$,因为所有的交叉特征都需要计算。但是通过公式变换，可以减少到线性复杂度，方法如下：可以看到这时的时间复杂度为$O(kn)$ 2.3 FM的预测FM算法可以应用在多种的预测任务中，包括: Regression :$y(X)$可以直接用作预测，并且最小平方误差来优化。 Binary classification ::$y(X)$作为目标函数并且使用hinge loss或者logit loss来优化。 Ranking : 向量$X$ 通过$y(X)$的分数排序，并且通过pairwise的分类损失来优化成对的样本$(x^{(a)},x^{(b)})$ 对以上的任务中，正则化项参数一般加入目标函数中以防止过拟合。 2.4 FM的参数学习从上面的描述可以知道FM可以在线性的时间内进行预测。因此模型的参数可以通过梯度下降的方法（例如随机梯度下降）来学习，对于各种的损失函数。FM模型的梯度是：$$\frac{\partial }{\partial \theta}\hat{y}(X)=\begin{cases}1 &amp; \mbox{if }\theta\mbox{ is }w_{0} \\x_{i} &amp; \mbox{if }\theta\mbox{ is }w_{i} \\x_{i}\sum^{n}_{j=1}v_{j,f}x_{i}-v_{i,f}x^{2}_{i} &amp; \mbox{if }\theta\mbox{ is }v_{i,f}\end{cases}$$由于$\sum^{n}_{j=1}v_{j,f}x_{i}$只与$f$有关，与$i$是独立的，可以提前计算出来，并且每次梯度更新可以在常数时间复杂度内完成，因此FM参数训练的复杂度也是$O(kn)$。综上可知，FM可以在线性时间训练和预测，是一种非常高效的模型。 2.5 多阶FM2阶FM可以很容易泛化到高阶：$$y(X)=w_{0}+\sum^{n}_{i=1}w_{i}x_{i}+\sum^{d}_{l=2}\sum^{n}_{i_{1}=1}……\sum^{n}_{i_{l}=i_{l-1}+1}(\prod^{l}_{j=1}x_{ij})(\sum^{k_{l}}_{f=1}\sum^{l}_{j=1}v^{(l)}_{i_{j},f})$$其中对第$l$个交互参数是由PARAFAC模型的参数因子分解得到：$$V^{(l)}\in R^{n \times k_{l}},k_{l} \in N^{+}_{0}$$直接计算公式的时间复杂度是$O(k_{d}n^{d})$ 。通过调整也可以在线性时间内运行。 2.6 总结FM模型有两个优势： 在高度稀疏的情况下特征之间的交叉仍然能够估计，而且可以泛化到未被观察的交叉 参数的学习和模型的预测的时间复杂度是线性的 FM模型的优化点： 特征为全交叉，耗费资源，通常user与user，item与item内部的交叉的作用要小于user与item的交叉 使用矩阵计算，而不是for循环计算 高阶交叉特征的构造 2.7 参考 FM算法 FM的理论与实践 一文读懂FM 推荐系统召回四模型之：全能的FM模型 3、DeepFM算法待更新… 4、xDeepFM算法待更新… 5、AutoInt算法待更新… 6、DIN网络待更新… 7、MLR算法待更新… 8、NCF算法待更新… 9、YoutubeNet待更新…]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐算法</tag>
        <tag>CTR算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BERT算法详解及其应用]]></title>
    <url>%2FNLP%2FBERT%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、背景最近几年自然语言处理领域取得了飞速的发展，基本几项重要的成果都集中在了预训练上包括18年火爆一时的BERT，而后续发表的XLNet，近期发布的RoBERTa都是以更好的预训练为目的的。 什么是BERT？BERT是基于Transformer Encoder来构建的一种预训练模型，在BERT的论文中称之为Masked Lanauge Model(MLM),严格意义上而言BERT并不属于语言模型，因为它并未按照语言模型的方式来进行训练，其实就是随机的将一些单词通过MASK的标签来替换，然后去猜测被MASK的单词，这整个过程就是基于DAE(Denoising Autoencoder)框架做的。对于Transformer的介绍我之前的文章有介绍过。 BERT的本质上是通过在海量的语料的基础上运行自监督学习方法为单词学习一个好的特征表示，所谓自监督学习是指在没有人工标注的数据上运行的监督学习。在以后特定的NLP任务中，我们可以直接使用BERT的特征表示作为该任务的词嵌入特征。所以BERT提供的是一个供其它任务迁移学习的模型，该模型可以根据任务微调或者固定之后作为特征提取器。BERT的源码和模型10月31号已经在Github上开源，简体中文和多语言模型也于11月3号开源。BERT有两种主要训练好的模型，分别是BERT-Small和BERT-Large， 其中BERT-Large使用了12层的Encoder结构。 整个的模型具有非常多的参数。 在bert之前，将预训练的embedding应用到下游任务的方式大致可以分为2种，一种是feature-based，例如ELMo这种将经过预训练的embedding作为特征引入到下游任务的网络中；一种是fine-tuning，例如GPT这种将下游任务接到预训练模型上，然后一起训练。然而这2种方式都会面临同一个问题，就是无法直接学习到上下文信息，像ELMo只是分别学习上文和下文信息，然后concat起来表示上下文信息，抑或是GPT只能学习上文信息。因此，作者提出一种基于transformer encoder的预训练模型，可以直接学习到上下文信息，叫做bert。bert使用了12个transformer encoder block，在13G的数据上进行了预训练，可谓是nlp领域大力出奇迹的代表。 论文地址:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 二、BERT详解2017年之前,语言模型都是通过rnn，lstm来建模，这样虽然可以学习上下文之间的关系，但是无法并行化，给模型的训练和推理带来了困难，因此论文提出了一种完全基于attention来对语言建模的模型，叫做transformer。transformer摆脱了nlp任务对于rnn，lstm的依赖，使用了self-attention的方式对上下文进行建模，提高了训练和推理的速度，transformer也是后续更强大的nlp预训练模型的基础,因此Bert网络结构也是基于Transformer结构发展而来的。 BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。接下来我们先对BERT的核心结构Transformer进行介绍。 2.1 Transormer结构Transormer是一个encoder-decoder网络，由若干个编码器和解码器堆叠形成，如下图所示:我们将encoder和decoder展开来看的更细致后，就如下所示：接下来我们针对以上结构进行逐个解释: Inputs是经过padding的输入数据，大小是[batch size, max seq length]。 初始化embedding matrix，通过embedding lookup将Inputs映射成token embedding，大小是[batch size, max seq length, embedding size]，然后乘以embedding size的开方。 通过sin和cos函数创建positional encoding，表示一个token的绝对位置信息，并加入到token embedding中，然后dropout。 multi-head attention &lt;4.1&gt; 输入token embedding，通过Dense生成Q，K，V，大小是[batch size, max seq length, embedding size]，然后按第2维split成num heads份并按第0维concat，生成新的Q，K，V，大小是[num heads*batch size, max seq length, embedding size/num heads]，完成multi-head的操作。 &lt;4.2&gt; 将K的第1维和第2维进行转置，然后Q和转置后的K的进行点积，结果的大小是[num heads*batch size, max seq length, max seq length]。 &lt;4.3&gt; 将&lt;4.2&gt;的结果除以hidden size的开方(在transformer中，hidden size=embedding size)，完成scale的操作。 &lt;4.4&gt; 将&lt;4.3&gt;中padding的点积结果置成一个很小的数(-2^32+1)，完成mask操作，后续softmax对padding的结果就可以忽略不计了。 &lt;4.5&gt; 将经过mask的结果进行softmax操作。 &lt;4.6&gt; 将softmax的结果和V进行点积，得到attention的结果，大小是[num heads*batch size, max seq length, hidden size/num heads]。 &lt;4.7&gt; 将attention的结果按第0维split成num heads份并按第2维concat，生成multi-head attention的结果，大小是[batch size, max seq length, hidden size]。Figure 2上concat之后还有一个linear的操作，但是代码里并没有。 将token embedding和multi-head attention的结果相加，并进行Layer Normalization。 将的结果经过2层Dense，其中第1层的activation=relu，第2层activation=None。 功能和一样。 Outputs是经过padding的输出数据，与Inputs不同的是，Outputs的需要在序列前面加上一个起始符号”&lt;s>”，用来表示序列生成的开始，而Inputs不需要。 功能和一样。 功能和一样。 功能和类似，唯一不同的一点在于mask，中的mask不仅将padding的点积结果置成一个很小的数，而且将当前token与之后的token的点积结果也置成一个很小的数。 功能和一样。 功能和类似，唯一不同的一点在于Q，K，V的输入，的Q的输入来自于Outputs 的token embedding，的K，V来自于的结果。 功能和一样。 功能和一样。 功能和一样，结果的大小是[batch size, max seq length, hidden size]。 将的结果的后2维和embedding matrix的转置进行点积，生成的结果的大小是[batch size, max seq length, vocab size]。 将的结果进行softmax操作，生成的结果就表示当前时刻预测的下一个token在vocab上的概率分布。 计算得到的下一个token在vocab上的概率分布和真实的下一个token的one-hot形式的cross entropy，然后sum非padding的token的cross entropy当作loss，利用adam进行训练。 2.2 Transormer的技术细节transformer中的self-attention是从普通的点积attention中演化出来的，但是对于上述结构我们还有很多细节可以探讨。 1.为什么要乘以embedding size的开方？论文并没有讲为什么这么做，看了代码，猜测是因为embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。 2.为什么inputs embedding要加入positional encoding？因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。positional encoding的公式如下:$$PE(pos,2i)=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\PE(pos,2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$至于positional encoding为什么能表示位置信息，可以看如何理解Transformer论文中的positional encoding，和三角函数有什么关系？ 3.为什么&lt;4.2&gt;的结果要scale？以数组为例，2个长度是len，均值是0，方差是1的数组点积会生成长度是len，均值是0，方差是len的数组。而方差变大会导致softmax的输入推向正无穷或负无穷，这时的梯度会无限趋近于0，不利于训练的收敛。因此除以len的开方，可以是数组的方差重新回归到1，有利于训练的收敛。 4.为什么要将multi-head attention的输入和输出相加？类似于resnet中的残差学习单元，有ensemble的思想在里面，解决网络退化问题。 5. 为什么attention需要multi-head，一个大head行不行？multi-head相当于把一个大空间划分成多个互斥的小空间，然后在小空间内分别计算attention，虽然单个小空间的attention计算结果没有大空间计算得精确，但是多个小空间并行然后concat有助于网络捕捉到更丰富的信息，类比cnn网络中的channel。 6.为什么multi-head attention后面要加一个ffn？类比cnn网络中，cnn block和fc交替连接，效果更好。相比于单独的multi-head attention，在后面加一个ffn，可以提高整个block的非线性变换的能力。 7.为什么要mask当前时刻的token与后续token的点积结果？自然语言生成(例如机器翻译，文本摘要)是auto-regressive的，在推理的时候只能依据之前的token生成当前时刻的token，正因为生成当前时刻的token的时候并不知道后续的token长什么样，所以为了保持训练和推理的一致性，训练的时候也不能利用后续的token来生成当前时刻的token。这种方式也符合人类在自然语言生成中的思维方式。 如果想更多的了解Attention和Tranformer的知识，可以参考这篇《Attention机制和Transformer框架详解》博客。 三、BERT应用由于模型的构成元素Transformer已经解析过，就不多说了，BERT模型的结构如下图最左：BERT对比这两个算法的优点是只有BERT表征会基于所有层中的左右两侧语境。BERT能做到这一点得益于Transformer中Attention机制将任意位置两个单词的距离转换成了1。 对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向rnn和双向rnn的区别，直觉上来讲效果会好一些。 对比ELMo，虽然都是“双向”，但目标函数其实是不同的。ELMo是分别以$P(w_{i}|w_{1},….,w_{i-1})$和$P(w_{i}|w_{i+1},….,w_{n})$作为目标函数，独立训练处两个representation然后拼接，而BERT则是以$P(w_{i}|w_{1},….,w_{i-1},w_{i+1},….,w_{n})$作为目标函数训练LM。 BERT提供了简单和复杂两个模型，对应的超参数分别如下： $BERT_{BASE}$:L=12，H=768，A=12，参数总量110M $BERT_{LARGE}$:L=24，H=1024，A=16，参数总量340M 在BERT的应用中主要分为预训练(Pre-training)和微调(Fine-Tuning)两部分。如下图所示预训练能够很优雅的通过无监督的方式将数据中的语言知识引入进来，微调就是将预训练的模型结合自身业务和模型进行调整，调整后的模型与自身目标更相关。 3.1 BERT的Embedding这里的Embedding由三种Embedding求和而成： Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务。 Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务 Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的 上图中在输入部分是WordPiece,是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。例如上图中的示例中‘playing’被拆分成了‘play’和‘ing’。 3.2 BERT的预训练BERT是一个多任务模型，它的任务是由两个自监督任务组成，即Masked LM和Next Sentence Prediction。 3.2.1 Task #1： Masked Language ModelMasked Language Model（MLM）和核心思想取自Wilson Taylor在1953年发表的一篇论文《cloze procedure: A new tool for measuring readability》。所谓MLM是指在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测该单词，该任务非常像我们在中学时期经常做的完形填空。正如传统的语言模型算法和RNN匹配那样，MLM的这个性质和Transformer的结构是非常匹配的。 在BERT的实验中，15%的WordPiece Token会被随机Mask掉。在训练模型时，一个句子会被多次喂到模型中用于参数学习，但是Google并没有在每次都mask掉这些单词，而是在确定要Mask掉的单词之后，80%的时候会直接替换为[Mask]，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token。 80%：my dog is hairy -&gt; my dog is [mask] 10%：my dog is hairy -&gt; my dog is apple 10%：my dog is hairy -&gt; my dog is hairy 这么做的原因是如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词。加入随机Token的原因是因为Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘。至于单词带来的负面影响，因为一个单词被随机替换掉的概率只有15%*10% =1.5%，这个负面影响其实是可以忽略不计的。另外文章指出每次只预测15%的单词，因此模型收敛的比较慢。 3.2.2 Task #2: Next Sentence PredictionNext Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文。如果是的话输出’IsNext‘，否则输出’NotNext‘。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的[CLS]符号中。 3.3 BERT的FineTuning在海量单预料上训练完BERT之后，便可以将其应用到NLP的各个任务中了。对于NSP任务来说，其条件概率表示为$P=softmax(CW^{T})$，其中$C$是BERT输出中的[CLS]符号，$W$ 是可学习的权值矩阵。 对于其它任务来说，我们也可以根据BERT的输出信息作出对应的预测。下图展示了BERT在11个不同任务中的模型，它们只需要在BERT的基础上再添加一个输出层便可以完成对特定任务的微调。这些任务类似于我们做过的文科试卷，其中有选择题，简答题等等。图5中其中Tok表示不同的Token，$E$表示嵌入向量，$T_{i}$表示第$i$个Token在经过BERT处理之后得到的特征向量。 上图中(a)表示对句子分类任务: MNLI：给定一个前提 (Premise) ，根据这个前提去推断假设 (Hypothesis) 与前提的关系。该任务的关系分为三种，蕴含关系 (Entailment)、矛盾关系 (Contradiction) 以及中立关系 (Neutral)。所以这个问题本质上是一个分类问题，我们需要做的是去发掘前提和假设这两个句子对之间的交互信息。 QQP：基于Quora，判断 Quora 上的两个问题句是否表示的是一样的意思。 QNLI：用于判断文本是否包含问题的答案，类似于我们做阅读理解定位问题所在的段落。 STS-B：预测两个句子的相似性，包括5个级别。 MRPC：也是判断两个句子是否是等价的。 RTE：类似于MNLI，但是只是对蕴含关系的二分类判断，而且数据集更小。 SWAG：从四个句子中选择为可能为前句下文的那个。 上图中(a)表示单个句子分类任务: SST-2：电影评价的情感分析。 CoLA：句子语义判断，是否是可接受的（Acceptable）。 对于GLUE数据集的分类任务（MNLI，QQP，QNLI，SST-B，MRPC，RTE，SST-2，CoLA），BERT的微调方法是根据[CLS]标志生成一组特征向量$C$ ，并通过一层全连接进行微调。损失函数根据任务类型自行设计，例如多分类的softmax或者二分类的sigmoid。 SWAG的微调方法与GLUE数据集类似，只不过其输出是四个可能选项的softmax：$$P_{i}=\frac{e^{V C_{i}}}{\sum^{4}_{j=1}e^{V C_{i}}}$$ (c)问答任务 SQuAD v1.1：给定一个句子（通常是一个问题）和一段描述文本，输出这个问题的答案，类似于做阅读理解的简答题。如上图(c)表示的，SQuAD的输入是问题和描述文本的句子对。输出是特征向量，通过在描述文本上接一层激活函数为softmax的全连接来获得输出文本的条件概率，全连接的输出节点个数是语料中Token的个数。$$P_{i}=\frac{e^{S T_{i}}}{\sum_{j}e^{S T_{i}}}$$ (d)命名实体识别 CoNLL-2003 NER：判断一个句子中的单词是不是Person，Organization，Location，Miscellaneous或者other（无命名实体）。微调CoNLL-2003 NER时将整个句子作为输入，在每个时间片输出一个概率，并通过softmax得到这个Token的实体类别。 四、总结4.1 优缺点 优点:BERT是截至2018年10月的最新state of the art模型，通过预训练和精调横扫了11项NLP任务，这首先就是最大的优点了。而且它还用的是Transformer，也就是相对rnn更加高效、能捕捉更长距离的依赖。对比起之前的预训练模型，它捕捉到的是真正意义上的bidirectional context信息。 缺点:作者在文中主要提到的就是MLM预训练时的mask问题： [MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现 每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢（它们会预测每个token） 4.2 BERT的贡献 使用Transformer的结构将已经走向瓶颈期的Word2Vec带向了一个新的方向，并再一次炒火了《Attention is All you Need》这篇论文； 11个NLP任务的精度大幅提升足以震惊整个深度学习领域 无私的开源了多种语言的源码和模型，具有非常高的商业价值 迁移学习又一次胜利，而且这次是在NLP领域的大胜，狂胜 五、参考 BERT详解 从BERT, XLNet, RoBERTa到ALBERT 从transformer到bert 从transformer到albert]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《推荐系统》读书笔记]]></title>
    <url>%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[本文为《推荐系统》蒋凡 译这本书的学习笔记，书本链接,提取密码:cm77。 第一章 引言本章主要是介绍了一些推荐系统的背景和基本概念以及简要的介绍了下本书的结构。 1.1 协同过滤推荐协同过滤推荐的思想主要是如果用户在过去有相同的偏好，那么他们在未来也会有相同的偏好。其任务主要是从大量候选集合中推荐出最有可能感兴趣的物品，而且用户是在隐式的与他人协作，因此这种技术也别称为协同过滤(CF,Collaborative Filtering)。而在协同过滤的推荐场景中，常见的问题如下： 如何发现与目标用户有着相似兴趣的用户？ 相似度怎么衡量？ 新用户如何处理？(因为没有相关的购物记录或者行为记录) 如果只有很少的评分怎么处理？ 除了利用相似的之外，还可以使用什么样的技术方式来判定用户是否会喜欢这个物品？ 纯粹的协同过滤是不需要使用到有关物品的任何知识，更多的应该是一种集体智慧的体现吧！ 1.2 基于内容的推荐一般而言，推荐系统有两个目的:激发用户去做某件事情和解决信息过载。其实在流程上是分为召回和排序的，召回是筛选出用户感兴趣的集合主要是用于解决第一个目的，排序主要是解决信息过载问题。 基于内容推荐的核心主要是能够获取物品的描述(不管是人工生成的还是自动抽取的)和这些特征记录的重要记录，同时也需要抽取用户的特征，用户特征的获取主要可以分析用户的行为和反馈，或者询问用户的兴趣和偏好。 基于内容的推荐一般会面临以下几个问题： 系统如何自动获取并持续改进用户记录？ 如何决定哪个物品匹配或者至少能接近、符合用户兴趣？ 在对物品描述进行抽取的过程中，如何减少人工标注？ 不过基于内容的推荐也有以下优点： 不需要大量用户就可以达到适度的推荐精度 一旦得到物品的属性就立即能推荐新的物品 1.3 基于知识的推荐基于知识的推荐是利用额外的因果知识生成推荐，在这种基于知识的方法中，推荐系统通常会用到有关当前用户和有效物品的额外信息(这些信息一般都是人工提供的)。基于知识推荐一般会面临以下几个问题： 哪些知识能够生成知识库 什么机制可根据用户的特点来选择和物品排名 在没有购买记录的领域中如何获取用户信息？如何处理用户直接给到的偏好信息？ 交互方式如何选择？ 设计对话获取信息时，考虑哪些个性化信息能够准确的捕捉到用户的偏好信息？ 1.4 混合推荐方法混合推荐方法就是组合不同的技术产生更好或者更精确的推荐。在使用混合推荐的方法中一般面临以下几个问题： 那些方法能被组合，特定组合的前提是什么？ 两个或多个推荐算法应该是顺序计算还是其他的混合方式 不同方法的结果对应权重如何确定？能否动态决定？ 1.5 推荐系统的解释解释主要是为了让用户更容易理解推荐系统的推理脉络，后续章节中推荐系统的解释主要是围绕解释协同过滤结果来解释以下问题： 推荐系统在解释其推荐结果的同时如何提高用户对系统的信任度？ 推荐策略如何影响解释推荐的方式？ 能通过解释让用户相信系统给出的建议是“公正的”或者不偏颇的么？ 1.6 评估推荐系统推荐系统研究的主要动力就是提高推荐质量，但是随之而来的我们如何衡量或者量化这些指标。在后面的章节中将围绕实验、半实验和非实验进行研究，一般会面临如下几个问题： 哪些研究设计适用于评估推荐系统 如何利用历史数据实验评估推荐系统 什么衡量标准适合不同的评估目标 现有的评估技术局限是什么？特别是在推荐系统的会话性和商业性价值而言 第二章 协同过滤推荐协同过滤推荐方法的主要思想是利用已有用户群体的过去的行为或意见预测当前用户最可能喜欢哪些东西或者对哪些东西感兴趣。这么多年来，人们提出了各种算法和技术并在实际环境和人工测试环节得到了有效的验证。纯粹的协同过滤方法输入只有给定的 用户-物品 评分矩阵，输出一般有以下几种类型： 表示当前用户对物品喜欢或不喜欢的程度的预测值 n项推荐物品的列表，其中topN不包含用户已经购买的物品。 2.1 基于用户的最近邻推荐基于用户的最近邻推荐的主要思想是：给定一个评分集和当前的活跃用户id，找出与当前用户过去有相同偏好的用户，这些用户被称为最近邻或者对等用户。然后对于对当前用户没见过的产品P，利用最近邻的评分进行估算。不过这种方法前提是基于一种假设的: 如果用户过去有相似的偏好，那么他们未来也会有相同的偏好 用户偏好不会随着时间变化 接下来我们用一组实例来具体介绍基于用户的最近邻推荐，下图为一个评分表，展示了Alice和其他用户的评分数据。依据以上的评分数据集，我们想预测出Alice对物品5的评分。在计算评分之前，先介绍相关的符号: 用户集:$U={u_{1},u_{2},……,u_{n}}$ 物品集:$P={p_{1},p_{2},……,p_{m}}$ 评分矩阵:R代表评分项$r_{ij}$的评分矩阵，其中$i \in 1….n$，$j \in 1……m$ 分值定义:从1(非常不喜欢)到5(非常喜欢) 如何确定相似用户集，推荐系统中通用的方法是Pearon相关系数，给定评分矩阵R，计算用户a和用户b的相似度sim(a,b)公式如下：$$sim(a,b)=\frac{\sum_{p \in P}(r_{a,p}-\hat{r}_{a})(r_{b,p}-\hat{r}_{b})}{\sqrt{\sum_{p \in P}(r_{a,p}-\hat{r}_{a})^2} \sqrt{\sum_{p \in P}(r_{b,p}-\hat{r}_{b})^2}}$$上述式子中$r_{a,p}$和$r_{b,p}$为用户对同一个物品的评分，$\hat{r}_{a}$和$\hat{r}_{b}$为用户的平均评分。计算Alice和用户1的相似度($\hat{r}_{Alice}=\hat{r}_{a}=4$，$\hat{r}_{user1}=\hat{r}_{b}=2.4$)$$sim(a,b)=\frac{(5-\hat{r}_{a})(3-\hat{r}_{b})+(3-\hat{r}_{a})(1-\hat{r}_{b})+…+(4-\hat{r}_{a})(3-\hat{r}_{b})}{\sqrt{(5-\hat{r}_{a})^2+(3-\hat{r}_{a})^2+…}\sqrt{(3-\hat{r}_{b})^2+(1-\hat{r}_{b})+…}}=0.85$$Pearson相关系数取值从1(强正相关)到-1(强负相关)。Alice和其他用户，即用户2、用户3和用户4的相似度分别为：0.70，0.00，-0.79。从以上可以看出用户1和用户2的历史评分和Alice相近，同时也看到了用户4和Alice的一个差异性。为了预测物品5的评分，我们需要考虑哪些近邻的的评分应该被重视以及如何评价他们的评分，以下公式给给出了最相似的N个近邻与用户a平均评分$\hat{r}_{a}$的偏差，计算用户a对物品p的预测值:$$pred(a,p)=\hat{r}_{a}+\frac{\sum_{b \in N}sim(a,b)(r_{b,p}-\hat{r}_{b})}{\sum_{b \in N}sim(a,b)}$$则依据用户1和用户2测算Alice度物品5的评分:$$pred(Alice,物品5)=4+\frac{0.85*(3-2.4)+0.70*(5-3.8)}{0.85+0.7}=4.87$$通过以上的计算我们大致了解到了，计算评分我们首先要确定用户的相似度、当前用的近邻N以及近邻用户评分如何赋权。 用户相似度 :上述例子中我们采用的是Pearson相关系数，根据文献的调研情况而言 改进余弦相似度、Spearman相关系数 或 均方差 等其他方法也能用于计算用户相似度。 赋权体系 :因为许多领域存在一些所有人都喜欢的物品，让两个用户对有争议的物品打成共识会比对广受欢迎的物品达成共识更有价值。为了解决这个问题，Breese et al(1998)提出对物品评分进行变换，降低对广受欢迎的物品有相同看法的相对重要性。类似于信息检索的idf值的概念提出 反用户频率(iuf) 。同时 方差权重因子 也解决了同样的问题。 选择近邻 :为了避免对计算时间造成负面影响我们不必考虑所有的近邻，我们只包含了所有的与当前用户正相关的用户。但是有时候正相关的用户集合太多，为了降低近邻的规模我们可以直接设定一个阈值或者将规模限定为一个值。近邻个数K的选择太大太小都不好，在大多数情况下20~50个比较合理。 2.2 基于物品的最近邻推荐基于用的的最近邻推荐已经成功应用在不同领域，但是随着用户和物品的日益增多带来了很多严峻的挑战。尤其是当需要扫描大量潜在的近邻时候很难做到实时预测。因此大型电子商务网站会采用 基于物品的最近邻推荐。这种方法非常适合做离线计算，因此在评分矩阵非常大的时候也能做到实时推荐。基于物品的算法主要思想是利用物品的相似度而不是用户间相似度来计算预测值。 在基于物品的推荐方法中，余弦相似度由于效果精确已经被证实是一种标准的度量体系。两个物品a和b对应的评分向量用$\vec{a}$和$\vec{a}$来表示，其相似度定义如下：$$sim(a,b)=\frac{\vec{a} \cdot \vec{b}}{|\vec{a}| * |\vec{b}| }$$符号$\cdot$表示向量的点积，$|\vec{a}|$表示向量的欧式长度，即向量自身点积的平方根。依据上一小节的例子，物品5和物品1的评分向量分别为(3,5,4,1)和(3,4,3,1),其相似度计算如下:$$sim(I_{5},I_{1})=\frac{3*3+5*4+4*3+1*1}{\sqrt{3^2+5^2+4^2+1^2}*\sqrt{3^2+4^2+3^2+1^2}}=0.99$$相似度值介于0到1之间，越接近1则表示越相似。基本的余弦相似度不会考虑用户评分平均值之间的差异。改进版的余弦方法能够解决这个问题，做法是在评分值中减去平均值。相应的改进的余弦相似度的范围在-1和1之间，就像Pearson系数一样。设U为所有同时给物品a和b评分的用户集，改进的余弦相似度如下:$$sim(a,b)=\frac{\sum_{u \in U}(r_{u,a}-\hat{u}_{a})(r_{u,b}-\hat{u}_{b})}{\sqrt{\sum_{u \in U}(r_{u,a}-\hat{u}_{a})^2}\sqrt{\sum_{u \in U}(r_{u,b}-\hat{u}_{b})^2}}$$因此我们可以对原始的评分数据集进行变换，使用 评分值-平均评分值 的偏差来替代，如下表所示:则物品5和物品1使用改进评分数据集的评分向量分别为(0.60,1.20,0.80,-1.80)和(0.60,0.20,-0.20,-1.80),则其改进余弦相似度为:$$\begin{split}sim(I_{5},I_{1}){} &amp;=\frac{0.60*0.60+1.20*0.20+0.80*(-0.20)+(-1.80)*(-1.80)}{\sqrt{0.60^2+1.20^2+0.80^2+1.80^2}*\sqrt{0.60^2+0.20^2+0.20^2+1.80^2}}\\{} &amp;=0.80\end{split}$$确定物品的相似度之后，我们可以通过计算Alice对所有与物品5相似物品的加权评分总和来预测Alice对物品5的评分。形式上，我们预测用户u对物品p的评分为:$$pred(u,p)=\frac{\sum_{i \in relatedItems(u)}sim(i,p)* r_{u,i}}{\sum_{i \in relatedItems(a)}sim(i,p)}$$推荐要求线上环境必须在及其短的时间内返回结果时，当用户数量M达到几百万时，实时计算预测值仍然不可行。为了在不牺牲推荐精度的情况下人们通常选择离线事先计算好的 物品相似度矩阵 ，描述所有物品两两之间的相似度。在运行时候通过确定与p最相似的物品并计算u对这些物品评分的加权总和来得到用户u对物品p的预测评分。近邻数量受限于当前用户评分过的物品个数，这个一般都较少，所以预测值可以在很短的时间内完成计算。原则上离线计算这个思路也可以在基于用户的近邻推荐中实现，但是在实际情况下两个用户的评分重叠的情况非常少见，相对而言物品相似度更稳定。 2.3 关于评分关于评分分为隐式和显式评分。显式评分一般是直接询问用户对物品的评分，但是其问题主要是在于需要用户额外的付出，而且用户很可能看不到好处而不愿意提供真实评分。因此会导致显式评分会非常稀疏且推荐质量并不高。隐式评分主要是基于用的行为对物品进行评分，如当用户购买了一个物品就表示一个正向评分，很多推荐系统也会跟踪用户的浏览行为。尽管隐式评分不需要用户额外付出而且可以持续的筹集，但是用户的行为能否被正确的解释还是难以确定的。 第三章 基于内容的推荐待添加…… 第四章 基于知识的推荐待添加…… 第五章 混合推荐方法待添加…… 第六章 推荐系统的解释待添加…… 第七章 评估推荐系统待添加…… 第八章 案例分析:移动互联网个性化游戏推荐待添加……]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TextRank算法]]></title>
    <url>%2FNLP%2FTextRank%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1、简介TextRank算法的思想来源于google的PageRank算法，其主要是将文本分割成若干的组成单元(单词、句子)并建立图模型，利用投票机制对文本中的重要成分进行排序，仅仅利用单文档的信息就可以完成关键词抽取和信息摘要的任务。和LDA、HMM等模型不同, TextRank不需要事先对多篇文档进行学习训练, 因其简洁有效而得到广泛应用。 2、PageRank2.1、PageRank算法原理PageRank,即网页排名，又称网页级别、Google左侧排名或佩奇排名。是Google创始人拉里·佩奇和谢尔盖·布林于1997年构建早期的搜索系统原型时提出的链接分析算法，自从Google在商业上获得空前的成功后，该算法也成为其他搜索引擎和学术界十分关注的计算模型。PageRank的计算充分利用了两个假设：数量假设和质量假设。步骤如下： 在初始阶段：网页通过链接关系构建起Web图，每个页面设置相同的PageRank值，通过若干轮的计算，会得到每个页面所获得的最终PageRank值。随着每一轮的计算进行，网页当前的PageRank值会不断得到更新。 在一轮中更新页面PageRank得分的计算方法：在一轮更新页面PageRank得分的计算中，每个页面将其当前的PageRank值平均分配到本页面包含的出链上，这样每个链接即获得了相应的权值。而每个页面将所有指向本页面的入链所传入的权值求和，即可得到新的PageRank得分。当每个页面都获得了更新后的PageRank值，就完成了一轮PageRank计算。 对于一个页面A，则他的PR值为：$$S(V_{i})=(1-d)+d* \sum_{V_{j} \in In(V_{i})}\frac{1}{|Out(V_{j})|}S(V_{j})$$在以上公式中： $S(V_i)$: 网页$V_i$的重要度（权重），初始值可设为1。 $d$： 阻尼系数，一般为0.85。 $In(V_i)$：能跳转到网页$V_i$的页面，在图中对应入度对应的点。 $Out(V_j)$：网页$V_j$能够跳转到的页面，在图中对应出度的点。 另外一个版本的公式:$$S(V_{i})=\frac{(1-d)}{N}+d* \sum_{V_{j} \in In(V_{i})}\frac{1}{|Out(V_{j})|}S(V_{j}),其中N为页面总数$$ 2.2、具体实例为了便于计算，我们假设每个页面的PR初始值为1，d为0.5。接下来我们针对每个节点计算对应的PageRank值: 页面A的PR值计算:$$PR(A)=0.5+0.5*PR(C)=0.5+0.5=1$$ 页面B的PR值计算:$$PR(B)=0.5+0.5*\Big(PR(A) \div 2\Big)=0.5+0.5 \cdot 0.5=0.75$$ 页面C的PR值计算:$$PR(C)=0.5+0.5*\Big(PR(A) \div 2 +PR(B)\Big)=0.5+0.5 \cdot (0.5+0.75)=1.125$$ 从以上的计算可以看出，每个节点投票只能投一次，但是由于其指向其他节点多了即节点的出度(C(T))，其对每个节点的投票需要加权，加权值为$\frac{1}{C(T)}$。在上述的计算过程中，节点A对B节点的投票值只有1/2的权重，因为A节点的出度为2。其他的依次类推。入度表示每个节点被多少节点指向，其PR值为这些入度的路径的PR值求和。 下面是迭代计算12轮之后，各个页面的PR值：既然算法需要迭代训练，那么肯定需要知道什么时候收敛。所以一般需要设置收敛条件：比如上次迭代结果与本次迭代结果小于某个误差；比如还可以设置最大循环次数。 3、基于TextRank的关键词提取关键词抽取的任务就是从一段给定的文本中自动抽取出若干有意义的词语或词组。TextRank算法是利用局部词汇之间关系（共现窗口）对后续关键词进行排序，直接从文本本身抽取。其主要步骤如下： 把给定的文本T按照完整句子进行分割，即 对于每个句子，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即，其中是保留后的候选关键词。 构建候选关键词图G = (V,E)，其中V为节点集，由（2）生成的候选关键词组成，然后采用共现关系（co-occurrence）构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为K的窗口中共现，K表示窗口大小，即最多共现K个单词。 根据上面公式，迭代传播各节点的权重，直至收敛。 对节点权重进行倒序排序，从而得到最重要的T个单词，作为候选关键词。 由（5）得到最重要的T个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词。例如，文本中有句子“Matlab code for plotting ambiguity function”，如果“Matlab”和“code”均属于候选关键词，则组合成“Matlab code”加入关键词序列。 4、基于TextRank的自动文摘基于TextRank的自动文摘属于自动摘录，通过选取文本中重要度较高的句子形成文摘。将每个句子看成图中的一个节点，若两个句子之间有相似性，认为对应的两个节点之间有一个无向有权边，权值是相似度，通过pagerank算法计算得到的重要性最高的若干句子可以当作摘要。其主要步骤如下： 预处理：将输入的文本或文本集的内容分割成句子得$T=[S_{1},S_{2},…..,S_{m}]$,构建图$G=(V,E)$其中V为句子集，对句子进行分词、去除停止词，得$S_{i}=[t_{i,1},t_{i,2},…….,t_{i,n}]$，其中$t_{i,j} \in S_{j}$是保留后的候选关键词。 句子相似度计算：构建图G中的边集E，基于句子间的内容覆盖率，给定两个句子$S_{i},S_{j}$,采用如下公式进行计算：$$Similarity(S_{i},S_{j})=\frac{|{w_{k}|w_{k}\in S_{i} \cap w_{k}\in S_{j}}|}{log(|S_{i}|)+log(|S_{j}|)}$$ $S_i$：第i个句子。$w_k$：第k个单词。$|S_i|$：句子i中单词数。简单来说就是，两个句子单词的交集除以两个句子的长度（至于为什么用log，没想明白，论文里也没提）。然后还有一点，就是，其他计算相似度的方法应该也是可行的，比如余弦相似度，最长公共子序列之类的，不过论文里一笔带过了。若两个句子之间的相似度大于给定的阈值，就认为这两个句子语义相关并将它们连接起来，即边的权值$w_{i,j}=Similarity(S_{i},S_{j})$ 句子权重计算：迭代传播权重计算各句子的得分，计算公式如下:$$WS(V_{i})=(1-d)+d* \sum_{V_{j} \in In(V_{i})}\frac{w_{ji}}{\sum_{V_{k} \in Out(V_{j})}w_{jk}}S(V_{j})$$ 上面公式与PageRank中的公式相比而言，PageRank中的出度权重每条边的权重都是相等的，在上述公式中使用相似度来进行加权赋值，这样每条出度的边的权重就不一定一样了，而在PageRanK中是等权的。 抽取文摘句：将（3）得到的句子得分进行倒序排序，抽取重要度最高的T个句子作为候选文摘句。 形成文摘：根据字数或句子数要求，从候选文摘句中抽取句子组成文摘 在上述生成自动摘要的过程中与PageRank不同的节点间是否产生边和边的权重是以句子相似度来决定的。在上述的过程中给出了原始的衡量句子相似度的方式。但是其实这部分我们可以使用各种方式来衡量句子的相似度，比如引入Sentence Embedding 然后计算cos距离，杰卡德距离等等。至于句子如何向量化方式就很多了，深度学习中的方法也很多，在此就不展开了。 5、结论TextRank的思想主要来源于PageRank，而在PageRank中边的产生是由网页之间的相互连接来产生的，在TextRank中边的产生是由切分句子后窗口共现产生的(关键词抽取任务时)和句子相似度卡阈值产生的(自动文摘任务时)。对于边的权重而言，在自动文摘任务时候边是带权重的，每条边的出度权值不是等值的，在稳重的PageRank中是等值的，在关键词抽取的任务中出度值也是等值的。 参考 textrank算法原理与提取关键词、自动提取摘要 PageRank百度百科 PageRank算法原理与实现]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>textrank</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention机制和Transformer框架详解]]></title>
    <url>%2FNLP%2FAttention%E6%9C%BA%E5%88%B6%E5%92%8CTransformer%E6%A1%86%E6%9E%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1、前言在之前的一篇文章中Attention Mechanism综述讲述了Attention的背景和其应用场景，主要来自知乎的一篇精品博文摘抄。不过看完这篇博文只是对Attention的框架有了一个大概的了解，但是深入其里还是有些模糊。接下来本文就对Attention和Transformer进行更多细节上的探讨。 2、为什么Attention一个新的结构或者模型出现都是为了解决某个问题的，目前的情况而言前馈网络和循环网络都有很强的能力,为什么还需要引入attention机制呢。 计算能力的限制：当要记住很多“信息“，模型就要变得更复杂，然而目前计算能力依然是限制神经网络发展的瓶颈。 优化算法的限制：虽然局部连接、权重共享以及pooling等优化操作可以让神经网络变得简单一些，有效缓解模型复杂度和表达能力之间的矛盾；但是，在循环神经网络中的长距离，信息“记忆”能力并不高。 Attention机制就是借助人脑处理信息方式：在处理某些信息的时候，对于输入的信息的注意力的分布会不一样，会容易去注意和目前目标有关的信息。 3、Attention机制分类当用神经网络来处理大量的输入信息时，也可以借鉴人脑的注意力机制，只选择一些关键的信息输入进行处理，来提高神经网络的效率。按照认知神经学中的注意力，可以总体上分为两类： 聚焦式（focus）注意力：自上而下的有意识的注意力，主动注意——是指有预定目的、依赖任务的、主动有意识地聚焦于某一对象的注意力； 显著性（saliency-based）注意力：自下而上的有意识的注意力，被动注意——基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关；可以将max-pooling和门控（gating）机制来近似地看作是自下而上的基于显著性的注意力机制。 在人工神经网络中，注意力机制一般就特指聚焦式注意力。 4、Attention机制的计算流程 Attention机制的实质其实就是一个寻址（addressing）的过程，如上图所示：给定一个和任务相关的查询Query向量q，通过计算与Key的注意力分布并附加在Value上，从而计算Attention Value，这个过程实际上是Attention机制缓解神经网络模型复杂度的体现：不需要将所有的N个输入信息都输入到神经网络进行计算，只需要从X中选择一些和任务相关的信息输入给神经网络。 如上图所示，Attention机制大致可以分为三步： 信息输入：用$X=[x_1,x_2,…… ,x_N]$表示N 个输入信息 计算注意力分布$\alpha$:令$Key=Value=X$，则可以给出注意力分布:$$\alpha_i=softmax(s(key_{i},q))=softmax(s(X_{i},q))$$ 根据注意力分布$\alpha$来计算输入信息的加权平均:注意力分布$\alpha_i$可以解释为在上下文查询q时，第i个信息受关注的程度，采用一种“软性”的信息选择机制对输入信息X进行编码为：$$att(q,X)=\sum_{i=1}^{N}{\alpha_i X_i}$$ 在上面的计算流程中，我们将$\alpha_i$称之为注意力分布（概率分布），$s(X_i,q)$为注意力打分机制，有几种打分机制： 加性模型：$s(X_i,q)=V^{T} tanh(WX_{i}+Uq)$ 点积模型：$s(X_i,q)=X^{T}_{i}q$ 缩放点积模型：$s(X_i,q)=\frac{X^{T}_{i}q}{\sqrt{d}}$ 双线性模型：：$s(X_i,q)=X^{T}_{i}Wq$ 这种编码方式为 软性注意力机制（soft Attention），软性注意力机制有两种：普通模式(Key=Value=X) 和 键值对模式(Key！=Value)。 4.1 self attention计算流程因为Attention实现的方式有很多种，我们难以每种都介绍，下面我们就介绍最基础和常见的Multi-Head Attention 与 Scaled Dot-Product Attention。首先我们介绍Scaled Dot-Product Attention，如果要先理解Scaled Dot-Product Attention我们必须先理解Q、K、V从哪来。以下计算步骤我们先使用self-attention的计算过程来帮助大家理解attention的计算过程。第一步 就是将输入的X通过转换矩阵对输入的X做线性转换后转换为Q、K、V。如下图所示:从上图中可以看出输入$X_{1}$与转换矩阵$W^{Q}$相乘之后的到$q_{1}$,同理我们经过矩阵$W^{K}$和$W^{V}$得到$k_{1}$和$v_{1}$。对于计算出来的query, key以及value向量他们对于理解attention的计算机制很有帮助，且attention接下来的计算机制都需要依赖这几个向量。第二步 接下来就是一句第一步算好的q、k、v向量计算score。我们以上述例子中首个单词‘thinking’的计算过程如下：score是由query vector和key vector相乘而得来，$score_{1}$是由$k_{1}$和$v_{1}$计算而来，$score_{2}$是由$k_{2}$和$v_{2}$计算而来,’thinking’会依次和所有的输入序列算一个score值，计算方式与$score_{1}$、$score_{2}$一致。第三步及第四步 我们将得到的结果除以8(key vectors的维数的平方根，论文中解释是为了得到更加稳定的梯度),然后将得到的结果输入到softmax进行归一化使得输出的结果相加后为1。softmax输出的socre会算出序列中每个单词与当前计算序列点的贡献度，有时其他单词对当前单词的翻译结果还是很重要的。第五步 将softmax socre与value vertor相乘，这里目的是为了让我们关注的单词尽量保持不变，并忽略掉我们不关注的单词的值(比如socre值很小，乘以晓得值后值会变得很小)。第六步 对加权后的value vector进行求和得到$Z$,这一步的目的是为了获取当前序列点的输出。以上我们介绍了self attention是如何具体计算的，接下来我们看看self attention的矩阵计算方式。第一步 计算出Query, Key, Value矩阵，如下所示:输入是一个[2x4]的矩阵（单词嵌入），每个运算是[4x3]的矩阵，求得Q,K,V。第二步 计算attention的输出Q对K转置做点乘，除以$d_k$的平方根。做一个softmax得到合为1的比例，对V做点乘得到输出Z。那么这个Z就是一个考虑过thinking周围单词的输出。依据以上得出结果:$$Attention=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$$以上公式中$QK^{T}$其实会构成一个word2word的attention map，在加入了softmax后只是将数值进行了归一化处理。如下图所示翻译任务中的attention矩阵:self-attention这里就出现一个问题，如果输入的句子特别长，那就为形成一个 NxN的attention map，这就会导致内存爆炸…所以要么减少batch size多gpu训练，要么剪断输入的长度，还有一个方法是用conv对K,V做卷积减少长度。 4.2 Multi-Head Attention计算流程在 Multi-Head Attention中，我们为每一个头部保持单独的q/k/v权重矩阵，从而得到不同的q/k/v矩阵。如前所述，我们将$X$乘以$W^{Q}//W^{K}/W^{V}$矩阵，得到q/k/v矩阵。由接下来我们可以算出不同head得到的不同的输出$Z_{0},Z_{1},Z_{2},…,Z_{7}$,我们得到不同head的输出后，将结果进行拼接(concatenate),然后将得到拼接后的$\hat{Z}$与矩阵$W^{0}$进行点乘得到最终输出Z。Multi-Head Attention整个过程如下如所示: 5、Transformer框架Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响。在“Attention is all you need”论文中说的Transformer指的是完整的Encoder-Decoder框架，如下图所示:在Encoders部分是由若干个Encoder单元组成，Dencoders部分是由若干个Decoder单元组成，其展开结构如下所示: 5.1 Encoder单元然后我们在针对以上的Ender单元在继续展开，发现Encoder部分主要是由self-attention部分和前馈神经网络构成，如下图所示:通过self-attention得到$Z$后，它会被送到encoder的下一个模块，即Feed Forward Neural Network。这个全连接有两层，第一层的激活函数是ReLU，第二层是一个线性激活函数，可以表示为：$$FFN(Z)=max(0,ZW_{1}+b_{1})W_{2}+b_{2}$$Transformer框架因为为了解决深度网络带来的退化问题，引入了残差网络部分(Residuals)如下所示:我们展开layer-norm看的更细致，如下图所示：从上面我们可以看到还有一个positition embedding没有介绍。在transformer机制中加入位置信息主要是为了增加捕捉顺序序列的能力，如果不加入位置信息那么就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。为了解决这个问题，论文中在编码词向量时引入了位置编码（Position Embedding）的特征。具体地说，位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。那么怎么编码这个位置信息呢？常见的模式有：a. 根据数据学习；b. 自己设计编码规则。在这里作者采用了第二种方式。那么这个位置编码该是什么样子呢？通常位置编码是一个长度为 [公式] 的特征向量，这样便于和词向量进行单位加的操作,如下图所示:论文给出的编码公式如下：$$PE(pos,2i)=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\PE(pos,2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})$$在上式中，$pos$表示单词的位置，意为 token 在句中的位置，设句子长度为 L ，则$pos=0,1,2,3,….,L-1$;$i$表示向量的某一维度,若$d_{model}=512$时，$i=0,1,…,255$.作者这么设计的原因是考虑到在NLP任务重，除了单词的绝对位置，单词的相对位置也非常重要。根据公式$sin(\alpha+\beta)=sin\alpha cos\beta+cos\alpha sin\beta$及$cos(\alpha+\beta)=cos\alpha cos\beta-sin\alpha sin\beta$，这表明位置$k+p$的位置向量可以表示为位置$k$的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。我们假设embedding大小为4，那么其真实的positional encodings如下所示: 5.2 Decoder单元然后我们把Decoder单元展开，发现Decoder部分和Encoder部分非常接近，但是多了一个Encoder-Decoder Attention单元，如下所示:Decnoder单元中的两个Attention分别用于计算输入和输出的权值： Self-Attention：当前翻译和已经翻译的前文之间的关系； Encoder-Decnoder Attention：当前翻译和编码的特征向量之间的关系。 在encoder-decoder attention中， $Q$来自于解码器的上一个输出，$K$和$V$则来自于与编码器的输出,计算方式和self attention的计算方式是一致的。其中$K$和$V$的转换过程如下:encoder-decoder attention有助于解码器聚焦在对当前解码器有用的输入序列位置上。上面展示了编码器到解码器的过程，接下来我们展示解码器逐步解码的过程，如下图:以上步骤一直重复知道到结束符号为止，每一步的输出在下一个时间被输入到底层解码器，解码器输出结果和编码器是一样的，只是解码器是逐个输出。我们在解码器输入中嵌入并添加位置编码来指示每个单词的位置，这点和编码器的做法是一致的。 5.3 损失层解码器解码之后，解码的特征向量经过一层激活函数为softmax的全连接层之后得到反映每个单词概率的输出向量。此时我们便可以通过CTC等损失函数训练模型了。而一个完整可训练的网络结构便是encoder和decoder的堆叠(各N个)。我们可以得到transformer的整体结构如下图所示: 6、总结通过上文的分析其实我们能够对Attention和Transformer有个很清晰地了解了，从我个人的理解而言这两个都不是某一个固定的模型，而是一个思想框架，所以其实现有很多种即有很多变种，类似RNN中的LSTM、GRU等等。Transformer中Attention是其很重要的一个单元，在加上位置信息来捕捉时序信息，其主要承担了RNN部分的功能。而Transformer依然使用的是Encoder-Decoder框架，只是在Encoder和Decoder部分的单元做了设计和构思。Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向，同时算法的并行性非常好，符合目前的硬件（主要指GPU）环境。但是Transformer并非没有缺点的，粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉局部特征的能力;Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。 参考 Attention is All You Need The Illustrated Transformer 详解Transormer]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>深度学习</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow模型保存与跨平台上线]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%B7%A5%E5%85%B7%2Ftensorflow%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%B7%A8%E5%B9%B3%E5%8F%B0%E4%B8%8A%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[1、tensorflow模型跨平台的方案tensorflow模型的跨平台上线的备选方案一般有三种：PMML方式、tensorflow serving方式以及跨语言API方式。 PMML方式：与普通机器学习模型通过PMML上线方式一致，唯一的区别是转化生成PMML文件需要用一个Java库jpmml-tensorflow来完成，生成PMML文件后，跨语言加载模型和其他PMML模型文件基本类似。 tensorflow serving:该方式是tensorflow官方推荐的模型上线预测方式，它需要一个专门的tensorflow服务器，用来提供预测的API服务。如果你的模型和对应的应用是比较大规模的，那么使用tensorflow serving是比较好的使用方式。但是它也有一个缺点，就是比较笨重，如果你要使用tensorflow serving，那么需要自己搭建serving集群并维护这个集群。所以为了一个小的应用去做这个工作，有时候会觉得麻烦。 跨语言API:该方式是本文要讨论的方式，它会用tensorflow自己的Python API生成模型文件，然后用tensorflow的客户端库比如Java或C++库来做模型的在线预测 2、tensorflow模型保存的方式和跨平台加载tensorflow训练模型保存模型有两种方式: 以checkpoint方式保存模型文件及其参数(多个文件)； 以pb固化图的方式保存模型文件和参数(只有一个文件)。 一般大部分训练模型的时候都是以ckpt的方式保存模型，本文不详细说明。接下来主要讲解如何将模型以pb的形式保存，且通过跨平台的方式来调用训练好的模型预测。可以通过以下两种方式将图保存为pb的形式:12345678910output_graph_def = tf.graph_util.convert_variables_to_constants(sess=sess, input_graph_def=input_graph_def, output_node_names=["output"])#方式1tf.train.write_graph(output_graph_def,".",output_graph_path,as_text=False)#方式2with tf.gfile.GFile("./rf03.pb","wb") as f: f.write(output_graph_def.SerializeToString()) input_graph_def是指的训练过程中的图，不带参数，以上两种方式固化图的结果是一致的。 2.1、模型训练保存完整的demo以下给出一个完整的列子: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from sklearn.datasets.samples_generator import make_classificationimport tensorflow as tfX1, y1 = make_classification(n_samples=4000, n_features=6, n_redundant=0, n_clusters_per_class=1, n_classes=3)def train(): #参数部分 learning_rate = 0.01 training_epochs = 600 batch_size = 100 #模型部分 x = tf.placeholder(tf.float32, [None, 6], name='input') # 6 features y = tf.placeholder(tf.float32, [None, 3]) # 3 classes W = tf.Variable(tf.zeros([6, 3])) b = tf.Variable(tf.zeros([3])) # softmax回归 pred = tf.nn.softmax(tf.matmul(x, W) + b, name="softmax") cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred), reduction_indices=1)) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) prediction_labels = tf.argmax(pred, axis=1, name="output") #训练部分 #如果模型文件存在继续训练就需要restore restore_flag=False saver = tf.train.Saver(tf.global_variables()) sess = tf.Session() if restore_flag: saver.restore(sess, tf.train.latest_checkpoint("./ckpt/")) else: init = tf.global_variables_initializer() sess.run(init) y2 = tf.one_hot(y1, 3) y2 = sess.run(y2) for epoch in range(training_epochs): _, c = sess.run([optimizer, cost], feed_dict=&#123;x: X1, y: y2&#125;) if (epoch + 1) % 10 == 0: print("Epoch:", '%04d' % (epoch + 1), "cost=", "&#123;:.9f&#125;".format(c)) print("优化完毕!") correct_prediction = tf.equal(prediction_labels, tf.argmax(y2, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) acc = sess.run(accuracy, feed_dict=&#123;x: X1, y: y2&#125;) print(acc) saver.save(sess,"./ckpt/model") #图固化 graph = tf.graph_util.convert_variables_to_constants(sess,input_graph_def= sess.graph_def,output_node_names=["output"]) tf.train.write_graph(graph, '.', 'rf.pb', as_text=False) with tf.gfile.GFile("rf01.pb","wb") as f: f.write(graph.SerializeToString())if __name__=="__main__": train() 2.2、python的api加载pb文件并预测使用python加载.pb文件预测demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546import tensorflow as tfimport numpy as npdef load_graph(frozen_graph_file_name): with tf.gfile.GFile(frozen_graph_file_name,"rb") as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) with tf.Graph().as_default() as graph: tf.import_graph_def( graph_def, input_map=None, return_elements=None, name="prefix", op_dict=None,producer_op_list=None ) return graphdef show_nodes_graph(graph): for op in graph.get_operations(): print(op.name,op.values)def get_input(): inputs = [[0.0 for i in range(0,6)] for j in range(0,4)] for i in range(0,4): for j in range(0,6): if i&lt;2: inputs[i][j]=2*i-5*j-6 else: inputs[i][j] = 2 * i + 5 * j - 6 return np.array(inputs)def predict(path="./ckpt/rf02.pb"): graph = load_graph(path) inputs = get_input() with tf.Session(graph=graph) as sess: outputs = sess.run("prefix/output:0",feed_dict=&#123;"prefix/input:0":inputs&#125;) print(type(outputs)) print(list(outputs))if __name__=="__main__": # graph = load_graph("./ckpt/rf02.pb") # show_nodes_graph(graph) predict("./ckpt/rf02.pb") 2.3、Java的api加载pb文件并预测Java的api加载.pb文件并预测首先maven的依赖中加入以下内容:123456&lt;!-- https://mvnrepository.com/artifact/org.tensorflow/tensorflow --&gt;&lt;dependency&gt; &lt;groupId&gt;org.tensorflow&lt;/groupId&gt; &lt;artifactId&gt;tensorflow&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt;&lt;/dependency&gt; 接下来Java加载模型和预测代码如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.tensorflowonline;import org.tensorflow.*;import org.tensorflow.Graph;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Paths;public class OnlineDemo &#123; public static void main(String[] args)&#123; String modelPath="/Users/xiachi/PycharmProjects/credit_card/WorkSpace/learn_dir/online_code/rf03.pb"; byte[] graphDef = loadTensoflowModel(modelPath); float[][] inputs = new float[4][6]; for(int i=0;i&lt;4;i++)&#123; for(int j=0;j&lt;6;j++)&#123; if(i&lt;2)&#123; inputs[i][j]=2*i-5*j-6; &#125;else&#123; inputs[i][j]=2*i+5*j-6; &#125; &#125; &#125; Tensor&lt;Float&gt; input = covertArrayToTensor(inputs); Graph graph = new Graph(); graph.importGraphDef(graphDef); Session session = new Session(graph); Tensor result = session.runner().feed("input",input).fetch("output").run().get(0); long[] rshape = result.shape(); int rs = (int) rshape[0]; long realResult[] = new long[rs]; result.copyTo(realResult); for(long a :realResult)&#123; System.out.println(a); &#125; &#125; private static Tensor&lt;Float&gt; covertArrayToTensor(float[][] inputs)&#123; return Tensors.create(inputs); &#125; private static byte[] loadTensoflowModel(String path) &#123; try &#123; return Files.readAllBytes(Paths.get(path)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; 3、将checkpoint文件转为pb文件以上给出了在训练的时候保存模型为pb文件，且给出了python和java两种调用pb预测的方式。如果我们已经训练完了，其实不用在运行源代码那么麻烦，只需要通过以下方式直接将checkpoint形式的模型文件转为pb形式的文件。 checkpoint模型文件中一般有如图以下几个文件: .meta文件保存的是图结构，通俗地讲就是神经网络的网络结构。一般而言网络结构是不会发生改变，所以可以只保存一个就行了; .data文件是数据文件，保存的是网络的权值，偏置，操作等等; .index是一个不可变得字符串表，每一个键都是张量的名称，它的值是一个序列化的BundleEntryProto。 每个BundleEntryProto描述张量的元数据：“数据”文件中的哪个文件包含张量的内容，该文件的偏移量，校验和，一些辅助数据等等; checkpoint是检查点文件，文件保存了一个目录下所有的模型文件列表； 拿到了这些文件，然后通过以下步骤完成pb文件的保存:1、通过传入CKPT模型的路径得到模型的图和变量数据2、通过 import_meta_graph 导入模型中的图3、通过 saver.restore 从模型中恢复图中各个变量的数据4、通过graph_util.convert_variables_to_constants将模型持久化 完整的demo如下:123456789101112131415161718192021222324252627282930313233343536import tensorflow as tf"""1、通过传入CKPT模型的路径得到模型的图和变量数据2、通过 import_meta_graph 导入模型中的图3、通过 saver.restore 从模型中恢复图中各个变量的数据4、通过graph_util.convert_variables_to_constants将模型持久化"""output_graph_path = "./ckpt/rf02.pb"checkpoint_prefix=tf.train.latest_checkpoint("./ckpt/")def freeze_graph(): output_node_name="ouput" meta_file_path = checkpoint_prefix+".meta" saver = tf.train.import_meta_graph(meta_file_path,clear_devices=True) graph = tf.get_default_graph() #获得默认图 input_graph_def = graph.as_graph_def() #返回一个序列化的图代表当前图 with tf.Session() as sess: saver.restore(sess,checkpoint_prefix) output_graph_def = tf.graph_util.convert_variables_to_constants(sess=sess, input_graph_def=input_graph_def, output_node_names=["output"]) #使用tf.train.write_graph固化 # tf.train.write_graph(output_graph_def,".","./rf02.pb",as_text=False) #使用tf.gfile来固化模型文件 with tf.gfile.GFile(output_graph_path,"wb") as f: f.write(output_graph_def.SerializeToString())if __name__=="__main__": freeze_graph() 4、总结对于tensorflow来说，模型上线一般选择tensorflow serving或者client API库来上线，前者适合于较大的模型和应用场景，后者则适合中小型的模型和应用场景。因此算法工程师使用在产品之前需要做好选择和评估。 5、参考 tensorflow将训练好的模型freeze,即将权重固化到图里面,并使用该模型进行预测 tensorflow机器学习模型的跨平台上线 tensorflow实现将ckpt转pb文件]]></content>
      <categories>
        <category>工程工具</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVD和LSI/LSA]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FSVD%E5%92%8CLSI-LSA%2F</url>
    <content type="text"><![CDATA[1、SVD定义SVD(singular value decomposition)，翻译成中文就是奇异值分解。SVD的用处有很多，比如：LSA（隐性语义分析）、推荐系统、特征压缩（或称数据降维）。SVD可以理解为： 将一个比较复杂的矩阵用更小更简单的3个子矩阵的相乘来表示，这3个小矩阵描述了大矩阵重要的特性 。 2、SVD的原理2.1、普通方阵的矩阵分解（EVD）我们知道如果一个矩阵 A 是方阵，即行列维度相同（mxm），一般来说可以对 A 进行特征分解：$$A = U \Lambda U^{-1}$$其中，U 的列向量是 A 的特征向量，Λ 是对角矩阵，Λ 对角元素是对应特征向量的特征值。 举个简单的例子，例如方阵 A 为：$$\begin{equation}A=\left[\begin{matrix}1&amp;2\\2&amp;2\end{matrix}\right]\end{equation}$$ 那么对其进行特征分解，相应的 Python 代码为：1234567import numpy as npA = np.array([[2,2],[1,2]])lamda,U=np.linalg.eig(A)print('方阵 A: ',A)print('特征值 lamda: ',lamda)print('特征向量 U: ',U) 结果为:12345方阵 A: [[2 2][1 2]]特征值 lamda: [ 3.41421356 0.58578644]特征向量 U: [[ 0.81649658 -0.81649658][ 0.57735027 0.57735027]] $$\begin{equation}A=\left[\begin{matrix}1&amp;2\\2&amp;2\end{matrix}\right] = \\\left[\begin{matrix}0.81649658&amp;-0.81649658\\0.57735027&amp;0.57735027\end{matrix}\right]\left[\begin{matrix}3.41421356&amp;0\\0&amp;0.58578644\end{matrix}\right]\left[\begin{matrix}0.81649658&amp;-0.81649658\\0.57735027&amp;0.57735027\end{matrix}\right]^{-1}\end{equation}$$ $$A = U \Lambda U^{-1}$$ 其中，特征值$\lambda_{1}=3.41421356$，对应的特征向量$u_1$=[0.81649658 0.57735027]；特征值$\lambda_{2}=0.58578644$，对应的特征向量 $u_2$=[-0.81649658 0.57735027]，特征向量均为列向量。 值得注意的是，特征向量都是单位矩阵，相互之间是线性无关的，但是并不正交。得出的结论是对于任意方阵，不同特征值对应的特征向量必然线性无关，但是不一定正交。 2.2、对称矩阵的矩阵分解（EVD）如果方阵 A 是对称矩阵，例如：$$\begin{equation}A=\left[\begin{matrix}2&amp;1\\1&amp;1\end{matrix}\right]\end{equation}$$对称矩阵特征分解满足以下公式：$$A = U \Lambda U^{T}$$分解后结果如下：12345方阵 A: [[2 1][1 1]]特征值 lamda: [ 2.61803399 0.38196601]特征向量 U: [[ 0.85065081 -0.52573111][ 0.52573111 0.85065081]] 其中，特征值$\lambda_{1}$=2.61803399，对应的特征向量$u_1$=[0.85065081 0.52573111]；特征值$\lambda_{2}$=0.38196601，对应的特征向量 $u_2$=[-0.52573111 0.85065081]，特征向量均为列向量。 注意，我们发现对阵矩阵的分解和非对称矩阵的分解除了公式不同之外，特征向量也有不同的特性。对称矩阵的不同特征值对应的特征向量不仅线性无关，而且是相互正交的。什么是正交呢？就是特征向量内积为零。验证如下： $$0.85065081 * -0.52573111 + 0.52573111 * 0.85065081 \\= 00.85065081 * −0.52573111+0.52573111 * 0.85065081=0$$ 重点来了，对称矩阵 A 经过矩阵分解之后，可以写成以下形式：$$A=\lambda_{1}u_{1}u_{1}^{T}+\lambda_{2}u_{2}u_{2}^{T}$$对上式进行验证： 2.3、奇异值分解（SVD）我们发现，在矩阵分解里的A是方阵或者是对称矩阵，行列维度都是相同的。但是实际应用中，很多矩阵都是非方阵、非对称的。那么如何对这类矩阵进行分解呢？因此，我们就引入了针对维度为 mxn 矩阵的分解方法，称之为奇异值分解（Singular Value Decomposition）假设矩阵A的维度为mxn，虽然A不是方阵，但是下面的矩阵却是方阵，且维度分别为 mxm、nxn。$$AA^T 和 A^{T}A$$因此，我们就可以分别对上面的方阵进行分解：$$AA^{T}=P \Lambda_{1} P^{T} \\A^{T}A=Q \Lambda_{2} Q^{T}$$其中,$\Lambda_{1}$和$\Lambda_{2}$是对焦矩阵，且对角线上非零元素均相同，即两个方阵具有相同的非零特征值，特征值令为$\sigma_1, \sigma_2, … , \sigma_k$。值得注意的是，k&lt;=m 且 k&lt;=n。 根据$\sigma_1, \sigma_2, … , \sigma_k$就可以得到矩阵A的特征值为:$$\lambda_{1}=\sqrt{\sigma_1},\lambda_{2}=\sqrt{\sigma_2},\lambda_{3}=\sqrt{\sigma_3},……,\lambda_{k}=\sqrt{\sigma_k}$$接下来，我们就能够得到奇异值分解的公式：$$A=P \Lambda Q^{T}$$其中，P 称为左奇异矩阵，维度是 mxm，Q称为右奇异矩阵，维度是 nxn。$\Lambda$并不是方阵，其维度为 mxn，$\Lambda$对角线上的非零元素就是A的特征值$\lambda_{1},\lambda_{2},…,\lambda_{k}$。图形化表示奇异值分解如下图所示： 2.4、SVD的一些性质对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：$$A_{m \times n} = P_{m \times m} \Lambda_{m \times n} Q_{n \times n}^{T} \approx P_{m \times k} \Lambda_{k \times k} Q_{k \times n}^{T}$$ 其中k要比n小很多，也就是一个大的矩阵A可以用三个小的矩阵$P_{m \times k},\Lambda_{k \times k},Q_{k \times n}^{T}$来表示。如下图所示，现在我们的矩阵A只需要灰色的部分的三个小矩阵就可以近似描述了。 由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。 3、SVD的应用3.1、SVD在PCA中的应用在PCA降维的过程中，需要找到样本协方差矩阵$X^{T}X$的最大的d个特征向量，然后用这最大的d个特征向量张成的矩阵来做低维投影降维。可以看出，在这个过程中需要先求出协方差矩阵$X^{T}X$，当样本数多样本特征数也多的时候，这个计算量是很大的。 注意到我们的SVD也可以得到协方差矩阵$X^{T}X$最大的d个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵$X^{T}X$，也能求出我们的右奇异矩阵Q。也就是说，我们的PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是我们我们认为的暴力特征分解。 另一方面，注意到PCA仅仅使用了我们SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？ 假设我们的样本是m×n的矩阵X，如果我们通过SVD找到了矩阵$XX^{T}$最大的d个特征向量张成的m×d维矩阵P，则我们如果进行如下处理：$$X_{d \times n}^{’}=P_{d \times m}^{T}X_{m \times n}$$ 可以得到一个d×n的矩阵$X^{’}$,这个矩阵和我们原来的m×n维样本矩阵X相比，行数从m减到了d，可见对行数进行了压缩。也就是说，左奇异矩阵可以用于行数的压缩。相对的，右奇异矩阵可以用于列数即特征维度的压缩，也就是我们的PCA降维。 3.2、SVD在主题模型中的应用(LSI/LSA)潜在语义索引(LSI)，又称为潜在语义分析(LSA)，主要是在解决两类问题，一类是一词多义，如“bank”一词，可以指银行，也可以指河岸；另一类是一义多词，即同义词问题，如“car”和“automobile”具有相同的含义，如果在检索的过程中，在计算这两类问题的相似性时，依靠余弦相似性的方法将不能很好的处理这样的问题。所以提出了潜在语义索引的方法，利用SVD降维的方法将词项和文本映射到一个新的空间。 LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。主要的问题有： SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。 主题值的选取对结果的影响非常大，很难选择合适的k值。 LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。 对于问题1，主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。对于问题2，这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。对于问题3，牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。 回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP。 3.2.1、算法原理在文本挖掘的过程中，通常使用基于单词的出现与否以及TF-IDF等进行向量空间模型的表示，而这种方式只是简单的表达了文本的向量，并未能够将语义的相似考虑进去。 我们希望找到一种模型，能够捕获到单词之间的相关性。如果两个单词之间有很强的相关性，那么当一个单词出现时，往往意味着另一个单词也应该出现(同义词)；反之，如果查询语句或者文档中的某个单词和其他单词的相关性都不大。LSA(LSI)使用SVD来对单词-文档矩阵进行分解。SVD可以看作是从单词-文档矩阵中发现不相关的索引变量(因子)，将原来的数据映射到语义空间内。在单词-文档矩阵中不相似的两个文档，可能在语义空间内比较相似。 SVD，亦即奇异值分解，是对矩阵进行分解的一种方法，一个t*d维的矩阵(单词-文档矩阵)X，可以分解为T*S*DT，其中T为t*m维矩阵，T中的每一列称为左奇异向量(left singular bector)，S为m*m维对角矩阵，每个值称为奇异值(singular value)，D为d*m维矩阵,D中的每一列称为右奇异向量。在对单词文档矩阵X做SVD分解之后，我们只保存S中最大的K个奇异值，以及T和D中对应的K个奇异向量，K个奇异值构成新的对角矩阵S’，K个左奇异向量和右奇异向量构成新的矩阵T’和D’：$X^{’}=T^{’} \times S^{’}\times D^{’}T$形成了一个新的t*d矩阵。这里举一个简单的LSI实例，假设我们有下面这个有11个词三个文本的词频TF对应矩阵如下： 这里我们没有使用预处理，也没有使用TF-IDF，在实际应用中最好使用预处理后的TF-IDF值矩阵作为输入。我们假定对应的主题数为2，则通过SVD降维后得到的三矩阵为： 从矩阵$U_k$我们可以看到词和词义之间的相关性。而从Vk可以看到3个文本和两个主题的相关性。大家可以看到里面有负数，所以这样得到的相关度比较难解释。 3.2.2、文档相似度计算在上面我们通过LSI得到的文本主题矩阵即Vk可以用于文本相似度计算。而计算方法一般是通过余弦相似度。比如对于上面的三文档两主题的例子。我们可以计算第一个文本和第二个文本的余弦相似度如下 ： 到这里也许你对整个过程也许还有点懵逼，为什么要用SVD分解，分解后得到的矩阵是什么含义呢？别着急下面将为你解答。 潜在语义索引（Latent Semantic Indexing）与PCA不太一样，至少不是实现了SVD就可以直接用的，不过LSI也是一个严重依赖于SVD的算法，之前吴军老师在矩阵计算与文本处理中的分类问题中谈到： “三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。” 上面这段话可能不太容易理解，不过这就是LSI的精髓内容，我下面举一个例子来说明一下，下面的例子来自LSA tutorial： 这就是一个矩阵，不过不太一样的是，这里的一行表示一个词在哪些title（文档）中出现了（一行就是之前说的一维feature），一列表示一个title中有哪些词，（这个矩阵其实是我们之前说的那种一行是一个sample的形式的一种转置，这个会使得我们的左右奇异向量的意义产生变化，但是不会影响我们计算的过程）。比如说T1这个title中就有guide、investing、market、stock四个词，各出现了一次，我们将这个矩阵进行SVD，得到下面的矩阵： 左奇异向量表示词的一些特性，右奇异向量表示文档的一些特性，中间的奇异值矩阵表示左奇异向量的一行与右奇异向量的一列的重要程序，数字越大越重要。继续看这个矩阵还可以发现一些有意思的东西，首先，左奇异向量的第一列表示每一个词的出现频繁程度，虽然不是线性的，但是可以认为是一个大概的描述，比如book是0.15对应文档中出现的2次，investing是0.74对应了文档中出现了9次，rich是0.36对应文档中出现了3次；其次，右奇异向量中一的第一行表示每一篇文档中的出现词的个数的近似，比如说，T6是0.49，出现了5个词，T2是0.22，出现了2个词。然后我们反过头来看，我们可以将左奇异向量和右奇异向量都取后2维（之前是3维的矩阵），投影到一个平面上，可以得到（如果对左奇异向量和右奇异向量单独投影的话也就代表相似的文档和相似的词）： 在图上，每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说stock 和 market可以放在一类，因为他们老是出现在一起，real和estate可以放在一类，dads，guide这种词就看起来有点孤立了，我们就不对他们进行合并了。按这样聚类出现的效果，可以提取文档集合中的近义词，这样当用户检索文档的时候，是用语义级别（近义词集合）去检索了，而不是之前的词的级别。这样一减少我们的检索、存储量，因为这样压缩的文档集合和PCA是异曲同工的，二可以提高我们的用户体验，用户输入一个词，我们可以在这个词的近义词的集合中去找，这是传统的索引无法做到的。 4、参考 如何让奇异值分解(SVD)变得不“奇异”？ 奇异值分解(SVD)原理与在降维中的应用 LSI/LSA算法原理]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVD</tag>
        <tag>LSI/LSA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow小技巧]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%B7%A5%E5%85%B7%2Ftensorflow%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[1、使用timeline来优化优化性能timeline可以分析整个模型在forward和backward的时候,每个操作消耗的时间，由此可以针对性的优化耗时的操作。案例：我之前尝试使用tensorflow多卡来加速训练的时候， 最后发现多卡速度还不如单卡快，改用tf.data来 加速读图片还是很慢，最后使用timeline分析出了速度慢的原因，timeline的使用如下12345678910run_metadata = tf.RunMetadata()run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)config = tf.ConfigProto(graph_options=tf.GraphOptions( optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))with tf.Session(config=config) as sess: c_np = sess.run(c,options=run_options,run_metadata=run_metadata) tl = timeline.Timeline(run_metadata.step_stats) ctf = tl.generate_chrome_trace_format()with open('timeline.json','w') as wd: wd.write(ctf) 然后到谷歌浏览器中打卡chrome://tracing 并导入 timeline.json ，最后可以看得如下图所示的每个操作消耗的时间， 这里横坐标为时间，从左到右依次为模型一次完整的forward and backward过程中，每个操作分别在cpu,gpu 0, gpu 1上消耗的时间，这些操作可以放大，非常方便观察具体每个操作在哪一个设备上消耗多少时间。这里我们cpu上主要有QueueDequeue操作，这是进行图片预期过程，这个时候gpu在并行计算的所以gpu没有空等；另外我的模型还有一个PyFunc在cpu上运行，如红框所示，此时gpu在等这个结果，没有任何操作运行，这个操作应该要优化的。另外就是如黑框所示，gpu上执行的时候有很大空隙，如黑框所示，这个导致gpu上的性能没有很好的利用起来，最后分析发现是我bn在多卡环境下没有使用正确，bn有一个参数updates_collections我设置为None 这时bn的参数mean,var是立即更新的，也是计算完当前layer的mean,var就更新，然后进行下一个layer的操作，这在单卡下没有问题的， 但是多卡情况下就会写等读的冲突，因为可能存在gpu0更新（写）mean但此时gpu1还没有计算到该层，所以gpu0就要等gpu1读完mean才能写，这样导致了 如黑框所示的空隙，这时只需将参数设置成updates_collections=tf.GraphKeys.UPDATE_OPS 即可，表示所以的bn参数更新由用户来显示指定更新，如123update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)with tf.control_dependencies(update_ops): train_op = optimizer.minimize(loss) 2、使用tf.addchecknumerics_ops检查NaN12check= tf.addchecknumerics_opssess.run([check, ...]) 来检查NaN问题 ,该操作会报告所有出现NaN的操作，从而方便找到NaN的源头。 3、使用 tf.data API创建数据流在比较 naive 的 feed_dict 工作流中，GPU 始终“坐冷板凳”，必须等 CPU 向它提供下个批次的数据。而在tf.data工作流中，能够以异步方式预读取下个批次的数据，从而使 GPU 的整体时间降到最小。我们还可以进一步通过将加载和预处理操作并行化来加快工作流的速度。要想创建一个简单的数据工作流，你需要两个对象： tf.data.Dataset:存储你的数据集; tf.data.Iterator:从数据集中逐个提取数据项。 用于图像工作流中的 tf.data.Dataset 如下所示：12345[ [Tensor(image), Tensor(label)], [Tensor(image), Tensor(label)], ...] 整个流程如下：12345678910111213141516171819202122232425262728293031323334353637383940# 来源def load_image(path): image_string = tf.read_file(path) # 不要使用 tf.image.decode_image，不然输出的形状会不明确 image = tf.image.decode_jpeg(image_string, channels=3) # 这会转换为 [0, 1]之间的浮点值 image = tf.image.convert_image_dtype(image, tf.float32) image = tf.image.resize_images(image, [image_size, image_size]) return image# 将函数load_image 应用到数据集中的每个文件名dataset = dataset.map(load_image, num_parallel_calls=8)接着使用tf.data.Dataset.batch()来创建批次：# 每个批次创建64张图像dataset = dataset.batch(64)#buffer_size为GPU预读批次dataset = dataset.prefetch(buffer_size=1)#创建一个迭代器来迭代数据集iterator = dataset.make_initializable_iterator()#创建一个占位符向量batch_of_images = iterator.get_next()#会话with tf.Session() as session: for i in range(epochs): session.run(iterator.initializer) try: # 迭代整个数据集 while True: image_batch = session.run(batch_of_images) except tf.errors.OutOfRangeError: print('End of Epoch.') Shuffle数据使用 tf.data.Dataset.shuffle() 将文件名打乱（shuffle），指明元素数量的参数每次都应被打乱。总的来说，建议打乱整个列表。12dataset = tf.data.Dataset.from_tensor_slices(files)dataset = dataset.shuffle(len(files)) 标签创建初始数据集时，将标签（或其它元数据）连同图像一起加载：123# 文件是一个图像文件名的Python列# 标签是一个Numpy数组，包含每张图像的标签数据dataset = tf.data.Dataset.from_tensor_slices((files, labels)) 在应用到数据集的所有函数中一定要包含 .map()，让标签数据能够得以传递：123456def load_image(path, label): # 加载图像 return image, labeldataset = dataset.map(load_image) 3、多卡GPU设置12345import osimport tensorflow as tfos.environ['CUDA_VISIBLE_DEVICES'] = '0' #指定哪张卡：0, 1, 2, ...session_conf = tf.ConfigProto()session_conf.gpu_options.allow_growth = True #随着进程逐渐增加显存占用，而不是一下占满 4、限制CPU个数1234567cpu_num = int(os.environ.get('CPU_NUM', 1)) config = tf.ConfigProto(device_count=&#123;"CPU": cpu_num&#125;, inter_op_parallelism_threads = cpu_num, intra_op_parallelism_threads = cpu_num, log_device_placement=True)with tf.Session(config=config) as s: pass 5、word2vec的小技巧经常搞各种乱七八糟的 word2vec ，以前笨的时候，要在非 tf 框架下映射好 id，然后写 pair，然后存文件训练，文件几百 G 的话，做起来炒鸡麻烦。。特么要天天这么搬砖累死人。其实 tf 高层抽象用得好，可以直接读文件，hash 映射，做 pair，一路写到 loss 不带眨眼，不到100行代码可以解决任意任意 csv 格式文件的 embedding 训练问题，还支持 hadoop 上读取，这就炒鸡方便了。 1234567891011121314151617181920212223242526272829303132333435363738import tensorflow as tffrom tensorflow.contrib.lookup import HashTablefrom tensorflow.contrib.lookup import TextFileIdTableInitializerfrom tensorflow.contrib.lookup import IdTableWithHashBuckets# files looks like: key a b c\n ...dataset = tf.data.TextLineDataset(files)dataset = dataset.map(lambda x: tf.string_split(x, "\t").values[1:fix_len])feature = dataset.batch(batch_size)iterator = dataset.make_one_shot_iterator()feature = iterator.get_next() # feature.shape = [batch_size, fix_len]# vocab_file looks like: item id\n ...table_file = TextFileIdTableInitializer(vocab_file)hash_table = HashTable(table_file)table = IdTableWithHashBuckets(hash_table)feature = table.lookup(feature) # feature has shape [batch, fix_len]# n is the number of sampled pairs, skip_win is context window# e.g. n = 3, skip_win = 5ff = [feature[:, k:-skip_win + k] for k in range(skip_win)]feature = tf.stack([ff[skip_win//2]]*skip_win, axis=2)feature = tf.reshape(feature, [-1, skip_win])[:, 0:n]# not the best way to sample target, but simpletargets = tf.reshape(tf.stack(ff, axis=2), [-1, skip_win])targets = tf.random_shuffle(tf.transpose(targets))targets = tf.transpose(targets)[:, 0:n]# mask the unwanted pair in the following way if needed# feature = tf.boolean_mask(feature, mask)# label = tf.boolean_mask(label, mask)embedding = tf.nn.embedding_lookup(tf.Variable(&lt;init_emb&gt;), feature)loss = tf.nn.nce_loss( weights=tf.Variable(&lt;init_weight&gt;), biases=tf.Variable(&lt;init_bias&gt;), labels=targets, inputs=embedding) 参考 tensorflow的一些小技巧 巧妙使用 TensorFlow 之 TensorLayer]]></content>
      <categories>
        <category>工程工具</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention Mechanism综述]]></title>
    <url>%2FNLP%2FAttention-Mechanism%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[本文主要来自文章模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用摘抄，在此基础上加上了更加通俗化表达的内容。因为觉得该文章写的很好，故搬运到博客中来学习。针对文章中部分错误修改，图片尽量更改为清晰图片。 1、简介Attention Mechanism目前非常流行，广泛应用于机器翻译、语音识别、图像标注(Image Caption)等很多领域，之所以它这么受欢迎，是因为Attention给模型赋予了区分辨别的能力，例如，在机器翻译、语音识别应用中，为句子中的每个词赋予不同的权重，使神经网络模型的学习变得更加灵活(soft)，同时Attention本身可以做为一种对齐关系，解释翻译输入/输出句子之间的对齐关系，解释模型到底学到了什么知识，为我们打开深度学习的黑箱，提供了一个窗口，如下图所示(NLP中的attention可视化):又比如在图像标注应用中，可以解释图片不同的区域对于输出Text序列的影响程度,下图展示了图像标注中的attention可视化。通过上述Attention Mechanism在图像标注应用的case可以发现，Attention Mechanism与人类对外界事物的观察机制很类似，当人类观察外界事物的时候，一般不会把事物当成一个整体去看，往往倾向于根据需要选择性的去获取被观察事物的某些重要部分，比如我们看到一个人时，往往先Attention到这个人的脸，然后再把不同区域的信息组合起来，形成一个对被观察事物的整体印象。因此，Attention Mechanism可以帮助模型对输入的X每个部分赋予不同的权重，抽取出更加关键及重要的信息，使模型做出更加准确的判断，同时不会对模型的计算和存储带来更大的开销，这也是Attention Mechanism应用如此广泛的原因。 2、Attention Mechanism原理2.1、Attention Mechanism主要需要解决的问题在论文《Sequence to Sequence Learning with Neural Networks》中介绍了一种基于RNN的Seq2Seq机器翻译模型。该模型基于一个编码器(Encoder)和一个解码器(Decoder)来构建基于神经网络的端到端的机器翻译模型。但是该模型通过Encoder将输入$X$编码为一个固定长度的隐向量$Z$,然后解码器通过该隐向量$Z$来解码出目标输出$Y$。该模型由google于2014年提出，但是该模型存在以下几个缺点： 把输入X的所有信息有压缩到一个固定长度的隐向量Z，会造成信息的损失，且忽略了输入X的长度，当输入句子长度很长，特别是比训练集中最初的句子长度还长时，模型的性能急剧下降。 把输入X编码成一个固定的长度，对于句子中每个词都赋予相同的权重，这样做是不合理的，比如，在机器翻译里，输入的句子与输出句子之间，往往是输入一个或几个词对应于输出的一个或几个词。因此，对输入的每个词赋予相同权重，这样做没有区分度，往往是模型性能下降。 同样的问题也存在于图像识别领域，卷积神经网络CNN对输入的图像每个区域做相同的处理，这样做没有区分度，特别是当处理的图像尺寸非常大时，问题更明显。因此，2015年，Dzmitry Bahdanau等人在《Neural machine translation by jointly learning to align and translate》提出了Attention Mechanism，用于对输入X的不同部分赋予不同的权重，进而实现软区分的目的。 2.2、Attention Mechanism原理在介绍Attention Mechanism结构和原理之前，我们首先介绍下Seq2Seq模型的结构。基于RNN的Seq2Seq模型主要由两篇论文介绍，只是采用了不同的RNN模型。Ilya Sutskever等人与2014年在论文《Sequence to Sequence Learning with Neural Networks》中使用LSTM来搭建Seq2Seq模型。随后，2015年，Kyunghyun Cho等人在论文《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》提出了基于GRU的Seq2Seq模型。两篇文章所提出的Seq2Seq模型，想要解决的主要问题是，如何把机器翻译中，变长的输入X映射到一个变长输出Y的问题,其主要结构如下图所示： 其中，Encoder把一个变成的输入序列$x_{1}，x_{2}，x_{3}….x_{t}$编码成一个固定长度隐向量c(上下文向量context)，c的作用有两个: 做为初始向量初始化Decoder的模型，做为decoder模型预测y1的初始向量。 做为背景向量，指导y序列中每一个step的y的产出。Decoder主要基于背景向量c和上一步的输出yt-1解码得到该时刻t的输出yt，直到碰到结束标志(&lt; EOS &gt;)为止。 如上文所述，传统的Seq2Seq模型对输入序列X缺乏区分度，因此，2015年，Kyunghyun Cho等人在论文《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》中，引入了Attention Mechanism来解决这个问题，他们提出的模型结构如下图所示 在该模型中，定义了一个条件概率：$$p(y_{i}|y_{1},y_{2},…,y_{i-1};X)=g(y_{i-1},s_{i},c_{i})$$其中，si是decoder中RNN在在i时刻的隐状态，如图中所示，其计算公式为：$$s_{i}=f(s_{i-1},y_{i-1},c_{i})$$这里的背景向量$c_{i}$的计算方式，与传统的Seq2Seq模型直接累加的计算方式不一样，这里的ci是一个权重化(Weighted)之后的值，其表达式如公式所示：$$c_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}$$其中，i表示decoder端的第i个词，$h_{j}$表示encoder端的第j和词的隐向量，$a_{ij}$表示encoder端的第j个词与decoder端的第i个词之间的权值，表示源端第j个词对目标端第i个词的影响程度，$a_{ij}$的计算公式如下所示:$$\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$$ $$e_{ij} = a(s_{i-1},h_{j})$$在上述公式中，$a_{ij}$是一个softmax模型输出，概率值的和为1。$e_{ij}$表示一个对齐模型，用于衡量encoder端的位置j个词，对于decoder端的位置i个词的对齐程度(影响程度)，换句话说：decoder端生成位置i的词时，有多少程度受encoder端的位置j的词影响。对齐模型$e_{ij}$的计算方式有很多种，不同的计算方式，代表不同的Attention模型，最简单且最常用的的对齐模型是dot product乘积矩阵，即把target端的输出隐状态$h_{t}$与source端的输出隐状态进行矩阵乘。常见的对齐计算方式如下：$$score(h_t,\hat h_s) = \begin{cases}h_{t}^{T}\hat h_s &amp; dot \\h_{t}^{T}W_{\alpha}\hat h_s &amp; general \\v_{a}^{T}tanh(W_{a}[h_{t};\hat h_s]) &amp; concat\end{cases}$$其中,$score(h_t,\hat h_s) =a_{ij}$表示源端与目标单单词对齐程度。可见，常见的对齐关系计算方式有，点乘(Dot product)，权值网络映射(General)和concat映射几种方式。 2.3、通俗理解Attention Mechanism原理1、核心思想Attention的思想理解起来比较容易，就是在decoding阶段对input中的信息赋予不同权重。在nlp中就是针对sequence的每个time step input，在cv中就是针对每个pixel。 2、原理解析针对Seq2seq翻译来说，rnn-based model差不多是下图的样子： 而比较基础的加入attention与rnn结合的model是下面的样子（也叫soft attention）：其中$\alpha_{0}^1$ 是$h_{0}^1$对应的权重，算出所有权重后会进行softmax和加权，得到$c^0$。 可以看到Encoding和decoding阶段仍然是rnn，但是decoding阶使用attention的输出结果$c^0$, $c^1$作为rnn的输入。 那么重点来了， 权重$\alpha$是怎么来的呢？常见有三种方法： 方式1(dot):$\alpha_{0}^1=cos\_ sim(z_0, h_1)$ 方式2(concat):$\alpha_0 =neural\_ network(z_0, h)$ 方式3(general):$\alpha_0 = h^TWz_0$ 思想就是根据当前解码“状态”判断输入序列的权重分布。 如果把attention剥离出来去看的话，其实是以下的机制：输入是query(Q), key(K), value(V)，输出是attention value。如果与之前的模型对应起来的话，query就是$z_0$,$z_1$，key就是$h_1$,$h_2$, $h_3$,$h_4$，value也是$h_1$,$h_2$,$h_3$,$h_4$。模型通过Q和K的匹配计算出权重，再结合V得到输出：$$Attention(Q, K, V) = softmax(sim(Q, K))V$$再深入理解下去，这种机制其实做的是寻址（addressing），也就是模仿中央处理器与存储交互的方式将存储的内容读出来，可以看一下李宏毅老师的课程。 3、Attention Mechanism分类Attention Mechanism的基本结构包含soft Attention 与Hard Attention，组合结构包含Hierarchical Attention、Attention over Attention、Multi-Step Attention等。 3.1、基本attention结构3.1.1、soft Attention 与Hard AttentionKelvin Xu等人与2015年发表论文《Show, Attend and Tell: Neural Image Caption Generation with Visual Attention》，在Image Caption中引入了Attention，当生成第i个关于图片内容描述的词时，用Attention来关联与i个词相关的图片的区域。Kelvin Xu等人在论文中使用了两种Attention Mechanism，即Soft Attention和Hard Attention。我们之前所描述的传统的Attention Mechanism就是Soft Attention。Soft Attention是参数化的(Parameterization)，因此可导，可以被嵌入到模型中去，直接训练。梯度可以经过Attention Mechanism模块，反向传播到模型其他部分。 相反，Hard Attention是一个随机的过程。Hard Attention不会选择整个encoder的输出做为其输入，Hard Attention会依概率$S_i$来采样输入端的隐状态一部分来进行计算，而不是整个encoder的隐状态。为了实现梯度的反向传播，需要采用蒙特卡洛采样的方法来估计模块的梯度。 两种Attention Mechanism都有各自的优势，但目前更多的研究和应用还是更倾向于使用Soft Attention，因为其可以直接求导，进行梯度反向传播。 3.1.2、Global Attention 和 Local AttentionGlobal Attention：传统的Attention model一样。所有的hidden state都被用于计算Context vector 的权重，即变长的对齐向量$a_t$，其长度等于encoder端输入句子的长度。在t时刻，首先基于decoder的隐状态ht和源端的隐状态$h_s$，计算一个变长的隐对齐权值向量$a_t$，其计算公式如下：$$\begin{split}a_{t}(s) {} &amp; =align(h_{t},\hat h_s) \\ {} &amp; =\frac{exp(score(h_t,\hat h_s))}{\sum_{s^{’}}exp(score(h_t,\hat h_{s^{’}}))}\end{split}$$ 其中，score是一个用于评价$h_t$与$h_s$之间关系的函数，即对齐函数，一般有三种计算方式，我们在上文中已经提到了。公式如下：$$score(h_t,\hat h_s) = \begin{cases}h_{t}^{T}\hat h_s &amp; dot \\h_{t}^{T}W_{\alpha}\hat h_s &amp; general \\v_{a}^{T}tanh(W_{a}[h_{t};\hat h_s]) &amp; concat\end{cases}$$得到对齐向量$a_t$之后，就可以通过加权平均的方式，得到上下文向量$c_t$。 Local Attention：Global Attention有一个明显的缺点就是，每一次，encoder端的所有hidden state都要参与计算，这样做计算开销会比较大，特别是当encoder的句子偏长，比如，一段话或者一篇文章，效率偏低。因此，为了提高效率，Local Attention应运而生。 Local Attention是一种介于Kelvin Xu所提出的Soft Attention和Hard Attention之间的一种Attention方式，即把两种方式结合起来。其结构如下图所示Local Attention首先会为decoder端当前的词，预测一个source端对齐位置(aligned position)$p_t$，然后基于$p_t$选择一个窗口，用于计算背景向量$c_t$。Position $p_t$的计算公式如下:$$p_{t}=S \cdot sigmoid(v_{p}^{T}tanh(W_{p}h_{t}))$$其中，S是encoder端句子长度，$v_p$和$w_p$是模型参数。此时，对齐向量$a_t$的计算公式如下：$$a_{t}(s)=align(h_{t},\hat h_s)exp\big(-\frac{(s-p_{t})^2}{2\delta^2}\big)$$总之，Global Attention和Local Attention各有优劣，在实际应用中，Global Attention应用更普遍，因为local Attention需要预测一个位置向量p，这就带来两个问题： 当encoder句子不是很长时，相对Global Attention，计算量并没有明显减小。 位置向量pt的预测并不非常准确，这就直接计算的到的local Attention的准确率。 3.1.3、Self AttentionSelf Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量(hidden state)计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention不同，它分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。因此，self Attention Attention比传统的Attention mechanism效果要好，主要原因之一是，传统的Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系，如下图所示：Self Attention的具体计算方式如下图所示Encoder的输入inputs和decoder的输入outputs，加上position embedding，做为各自的最初的输入，那么问题来了，self Attention具体是怎么实现的呢？从All Attention的结构示意图可以发现，Encoder和decoder是层叠多了类似的Multi-Head Attention单元构成，而每一个Multi-Head Attention单元由多个结构相似的Scaled Dot-Product Attention单元组成，结构如下图所示。Self Attention也是在Scaled Dot-Product Attention单元里面实现的，如上图左图所示，首先把输入Input经过线性变换分别得到Q、K、V，注意，Q、K、V都来自于Input，只不过是线性变换的矩阵的权值不同而已。然后把Q和K做dot Product相乘，得到输入Input词与词之间的依赖关系，然后经过尺度变换(scale)、掩码(mask)和softmax操作，得到最终的Self Attention矩阵。尺度变换是为了防止输入值过大导致训练不稳定，mask则是为了保证时间的先后关系。 最后，把encoder端self Attention计算的结果加入到decoder做为k和V，结合decoder自身的输出做为q，得到encoder端的attention与decoder端attention之间的依赖关系。 3.2、组合的attention结构3.2.1、Hierarchical AttentionZichao Yang等人在论文《Hierarchical Attention Networks for Document Classification》提出了Hierarchical Attention用于文档分类。Hierarchical Attention构建了两个层次的Attention Mechanism，第一个层次是对句子中每个词的attention，即word attention；第二个层次是针对文档中每个句子的attention，即sentence attention。网络结构如下图所示: 整个网络结构由四个部分组成：一个由双向RNN(GRU)构成的word sequence encoder，然后是一个关于词的word-level的attention layer；基于word attention layar之上，是一个由双向RNN构成的sentence encoder，最后的输出层是一个sentence-level的attention layer。 3.2.2、Attention over AttentionYiming Cui与2017年在论文《Attention-over-Attention Neural Networks for Reading Comprehension》中提出了Attention Over Attention的Attention机制,结构如下图所示:两个输入，一个Document和一个Query，分别用一个双向的RNN进行特征抽取，得到各自的隐状态h(doc)和h(query)，然后基于query和doc的隐状态进行dot product，得到query和doc的attention关联矩阵。然后按列(colum)方向进行softmax操作，得到query-to-document的attention 值a(t)；按照行(row)方向进行softmax操作，得到document-to-query的attention值b(t)，再按照列方向进行累加求平均得到平均后的attention值b(t)。最后再基于上一步attention操作得到a(t)和b(t)，再进行attention操作，即attention over attention得到最终query与document的关联矩阵。 3.2.3、Multi-Step Attention2017年，FaceBook 人工智能实验室的Jonas Gehring等人在论文《Convolutional Sequence to Sequence Learning》提出了完全基于CNN来构建Seq2Seq模型，除了这一最大的特色之外，论文中还采用了多层Attention Mechanism，来获取encoder和decoder中输入句子之间的关系,如下图所示: 完全基于CNN的Seq2Seq模型需要通过层叠多层来获取输入句子中词与词之间的依赖关系，特别是当句子非常长的时候，我曾经实验证明，层叠的层数往往达到10层以上才能取得比较理想的结果。针对每一个卷记得step（输入一个词）都对encoder的hidden state和decoder的hidden state进行dot product计算得到最终的Attention 矩阵，并且基于最终的attention矩阵去指导decoder的解码操作。 4、Attention的应用场景4.1、机器翻译(Machine Translation)给定一个法语句子做为输入序列，翻译并输出一个英文句子做为输出序列。Attention用于关联输出序列中每个单词与输入序列中的某个特定单词的关联程度。 “我们扩展了传统的编码器-解码器结构，赋予decoder，在生成目标端（target）的词时，可以自动（软）搜索一组与之相关的输入序列的能力。这使得模型不必将整个源句子编码成一个固定长度的向量，并且还使模型只关注源端与下一个目标词的生成有关的信息。” Dzmitry Bahdanau等人，《Neural machine translation by jointly learning to align and translate》，2015。通过Attention来解释法语到英语单词之间的对应关系。摘自Dzmitry Bahdanau的论文 4.2、图像标注(Image Captain)基于序列的Attention Mechanism可以应用于计算机视觉问题，以帮助理解如何最好地利用卷积神经网络来省长一段关于图片内容的描述，也称为Caption。 给定输入图像，输出图像的英文描述。使用Attention是为输出序列中的每个单词关注图像中不同部分。 “我们提出了一种基于Attention mechanism的方法，并在在三个标准数据集上都取得了最好的成绩…我们还展示了如何利用学到的Attention来提供更多对模型生成过程的解释，并且证明Attention学习到的对齐与人类视觉感知非常一致。” Kelvin Xu等人，《Attend and Tell: Neural Image Caption Generation with Visual Attention》, 2016 4.3、关系抽取(EntailMent Extraction)给定一个用英语描述前景描述（premise scenario）和假设（hypothesis），判读假设（premise）与假设（hypothesis）的关系：矛盾，相关或包含。 例如： 前提：“一场婚礼中拍照” 假设：“有人结婚” Attention被用来把假设中的每个单词与前提中的单词联系起来，反之亦然。 “我们提出了一个基于LSTM的神经模型，它一次读取两个句子来确定两个句子之间的蕴含关系，而不是将每个句子独立映射到一个语义空间。我们引入逐字的（word-by-word）Attention Mechanism来扩展这个模型，来强化模型对单词或短语对的关系推理能力。该模型比传统的仅基于LSTM的模型高2.6个百分点，取得了一个最高成就” -Tim Rocktäschel，《Reasoning about Entailment with Neural Attention》, 2016基于Attention来解释前提和假设中词与词之间的对应关系 4.4、语音识别(Speech Recognition)给定一段英语语音片段做为输入序列，输出对应的音素序列。 Attention被用联将输出序列中的每个音素与输入序列中的特定音频帧相关联。 “基于混合Attention机制的新型端到端可训练语音识别体系结构，其结合内容和位置信息帮助选择输入序列中的下一个位置用于解码。所提出的模型的一个理想特性就是它可以识别比训练集中句子的更长的句子。”-Jan Chorowski，《Attention-Based Models for Speech Recognition》, 2015.。 4.5、自动摘要生成(Text Summarization)给定一篇英文文章做为输入顺序，输出一个总结英文文章注意内容的摘要句子。 Attention用于将输出摘要中的每个单词与输入文档中的特定单词相关联。 “将基于Attention的神经网络用语摘要抽取。我们将这个概率模型与可以产生准确的摘要的生成算法相结合。” -Alexander M. Rush，《A Neural Attention Model for Abstractive Sentence Summarization》, 2015基于Attention来解释输入Sentence与输出Summary之间单词的对应关系 Attention Mechanism现在应用非常广泛，这里就列出这几个case供大家参考。 5、参考文献 模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用 Attention Is All You Need Attention in Long Short-Term Memory Recurrent Neural Networks 【NLP】Attention原理和源码解析]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>attention</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习评价指标]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[1、ROC和AUC如果要理解AUC和ROC曲线，首先要理解混淆矩阵的定义。混淆矩阵中有着Positive、Negative、True、False的概念，其意义如下： 称预测类别为1的为Positive（阳性），预测类别为0的为Negative（阴性）。 预测正确的为True（真），预测错误的为False（伪）。 然后，由此引出True Positive Rate（真阳率）、False Positive（伪阳率）两个概念： $$TPRate=\frac{ TP }{ TP+FN } $$ $$FPRate= \frac{ FP }{ FP+TN } $$ 仔细看这两个公式，发现其实TPRate就是TP除以TP所在的列，FPRate就是FP除以FP所在的列，二者意义如下： TPRate的意义是所有真实类别为1的样本中，预测类别为1的比例 FPRate的意义是所有真是类别为0的样本中，预测类别为1的比例 按照定义，AUC即ROC曲线下的面积，而ROC曲线的横轴是FPRate，纵轴是TPRate，当二者相等时，即y=x，如下图，表示的意义是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的.换句话说，和抛硬币并没有什么区别，一个抛硬币的分类器是我们能想象的最差的情况，因此一般来说我们认为AUC的最小值为0.5（当然也存在预测相反这种极端的情况，AUC小于0.5）。而我们希望分类器达到的效果是：对于真实类别为1的样本，分类器预测为1的概率（即TPRate），要大于真实类别为0而预测类别为1的概率（即FPRate），这样的ROC曲线是在y=x之上的，因此大部分的ROC曲线长成下面这个样子： AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。例如在反欺诈场景，设非欺诈类样本为正例，负例占比很少（假设0.1%），如果使用准确率评估，把所有的样本预测为正例便可以获得99.9%的准确率。但是如果使用AUC，把所有样本预测为正例，TPRate和FPRate同时为1，AUC仅为0.5，成功规避了样本不均匀带来的问题。 auc的真实(物理)含义 :AUC是指 随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性。 2、精确率Precision、召回率Recall和F1值还是一句上图的混淆矩阵精确率(precision)定义为:$$P=\frac{TP}{TP+FP}$$ 准确率(accuracy)定义为:$$P=\frac{TP+TN}{TP+FP+TN+FN}$$ 在正负样本不平衡的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用acc，即使全部预测成负类（不点击）acc 也有 99% 以上，没有意义。 召回率(recall,sensitivity,true positive rate)定义为：$$R=\frac{TP}{TP+FN}$$ 此外，还有$F_{1}$值，是精确率和召回率的调和均值，$$\frac{2}{F_{1}}= \frac{ 1 }{ P } + \frac{ 1 }{ R } \\F_{1} = \frac{2TP}{2TP+FP+FN}= \frac{2 \ast P \ast R}{P+R}$$ 精确率和准确率都高的情况下，F1 值也会高 精确率（正确率）和召回率是广泛用于信息检索和统计学分类领域的两个度量值，用来评价结果的质量。其中精度是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率；召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率。 一般来说，Precision就是检索出来的条目（比如：文档、网页等）有多少是准确的，Recall就是所有准确的条目有多少被检索出来了，两者的定义分别如下： 查准率＝检索出的相关信息量 / 检索出的信息总量 查全率＝检索出的相关信息量 / 系统中的相关信息总量 举个例子： 假设我们手上有60个正样本，40个负样本，我们要找出所有的正样本，系统查找出50个，其中只有40个是真正的正样本，计算上述各指标。TP: 将正类预测为正类数 40FN: 将正类预测为负类数 20FP: 将负类预测为正类数 10TN: 将负类预测为负类数 30 准确率(accuracy) = 预测对的/所有 = (TP+TN)/(TP+FN+FP+TN) = 70% 精确率(precision) = TP/(TP+FP) = 80% 召回率(recall) = TP/(TP+FN) = 2/3 注:精确率(precision)和准确率(accuracy)是不一样的. 3、平均绝对误差和平均平方误差 平均绝对误差平均绝对误差MAE（Mean Absolute Error）又被称为 l1 范数损失（l1-norm loss）：$$MAE(y_{i},\hat{y}_{i})=\frac{1}{n_{samples}}\sum_{i=1}^{n_{samples}}|y_{i}-\hat{y}_{i}|$$ 平均平方误差平均平方误差 MSE（Mean Squared Error）又被称为 l2 范数损失（l2-norm loss）：$$MSE(y_{i},\hat{y}_{i})=\frac{1}{n_{samples}}\sum_{i=1}^{n_{samples}}(y_{i}-\hat{y}_{i})^2$$ 4、ks值KS(Kolmogorov-Smirnov)：KS用于模型风险区分能力进行评估,指标衡量的是好坏样本累计分部之间的差值。好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。通常来讲，KS&gt;0.2即表示模型有较好的预测准确性。 KS的计算步骤如下： 计算每个评分区间的好坏账户数。 计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。 计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的K-S值。 ks的通俗步骤: 按照分类模型返回的概率降序排列 把0-1之间等分N份，等分点为阈值，计算TPR、FPR 对TPR、FPR描点画图即可 5、基尼系数GINI系数:也是用于模型风险区分能力进行评估。GINI统计值衡量坏账户数在好账户数上的的累积分布与随机分布曲线之间的面积，好账户与坏账户分布之间的差异越大，GINI指标越高，表明模型的风险区分能力越强。 GINI系数的计算步骤如下： 计算每个评分区间的好坏账户数。 计算每个评分区间的累计好账户数占总好账户数比率（累计good%）和累计坏账户数占总坏账户数比率(累计bad%)。 按照累计好账户占比和累计坏账户占比得出下图所示曲线ADC。 计算出图中阴影部分面积，阴影面积占直角三角形ABC面积的百分比，即为GINI系数。 6、困惑度(perplexity)在信息论中，perplexity(困惑度)用来度量一个概率分布或概率模型预测样本的好坏程度。它也可以用来比较两个概率分布或概率模型。低困惑度的概率分布模型或概率模型能更好地预测样本。困惑度分为三种： 概率分布的困惑度(Perplexity of a probability distribution) 概率模型的困惑度(Perplexity of a probability model) 每个分词的困惑度(Perplexity per word) 6.1、概率分布的困惑度定义离散概率分布的困惑度如下：$$2^{H(p)}=2^{-\sum_{x}p(x)log_{2}p(x)}$$其中H(p)是概率分布p的熵，x是样本点。因此一个随机变量X的困惑度是定义在X的概率分布上的（X所有”可能”取值为x的部分） 一个特殊的例子是k面均匀骰子的概率分布，它的困惑度恰好是k。一个拥有k困惑度的随机变量有着和k面均匀骰子一样多的不确定性，并且可以说该随机变量有着k个困惑度的取值（k-ways perplexed）。 困惑度有时也被用来衡量一个预测问题的难易程度。但这个方法不总是精确的。例如：在概率分布B(1,P=0.9)中，即取得1的概率是0.9，取得0的概率是0.1。可以计算困惑度是：$$2^{-0.9log_{2}0.9-0.1log_{2}0.1}=1.38$$同时自然地，我们预测下一样本点的策略将是：预测其取值为1，那么我们预测正确的概率是0.9。而困惑度的倒数是1/1.38=0.72而不是0.9。（但当我们考虑k面骰子上的均匀分布时，困惑度是k，困惑度的倒数是1/k，正好是预测正确的概率） 困惑度是信息熵的指数 6.2、概率模型的困惑度用一个概率模型q去估计真实概率分布p，那么可以通过测试集中的样本来定义这个概率模型的困惑度。$$b^{-\frac{1}{N}\sum_{i=1}^{N}log_{b}q(x_{i})}$$其中测试样本$x_1, x_2, …, x_N$是来自于真实概率分布p的观测值，b通常取2。因此，低的困惑度表示q对p拟合的越好，当模型q看到测试样本时，它会不会“感到”那么“困惑”。我们指出，指数部分是交叉熵。$$H(\hat{p},q)=-\sum_{x}\hat{p}(x)log_{2}q(x)$$其中$\hat{p}$表示我们对真实分布下样本点x出现概率的估计。比如用$p(x)=n/N$ 6.3、每个分词的困惑度在自然语言处理中，困惑度是用来衡量语言概率模型优劣的一个方法。一个语言概率模型可以看成是在整过句子或者文段上的概率分布。例如每个分词位置上有一个概率分布，这个概率分布表示了每个词在这个位置上出现的概率；或者每个句子位置上有一个概率分布，这个概率分布表示了所有可能句子在这个位置上出现的概率。 比如，i这个句子位置上的概率分布的信息熵可能是190，或者说，i这个句子位置上出现的句子平均要用190 bits去编码，那么这个位置上的概率分布的困惑度就是$2^{190}$。（译者：相当于投掷一个$2^{190}$面筛子的不确定性）通常，我们会考虑句子有不同的长度，所以我们会计算每个分词上的困惑度。比如，一个测试集上共有1000个单词，并且可以用7.95个bits给每个单词编码，那么我们可以说这个模型上每个词有$2^{7.95}=247$困惑度。相当于在每个词语位置上都有投掷一个247面骰子的不确定性。 在Brown corpus (1 million words of American English of varying topics and genres) 上报告的最低的困惑度就是247per word，使用的是一个trigram model（三元语法模型）。在一个特定领域的语料中，常常可以得到更低的困惑度。 要注意的是，这个模型用的是三元语法。直接预测下一个单词是”the”的正确率是7%。但如果直接应用上面的结果，算出来这个预测是正确的概率是1/247=0.4%，这就错了。（译者：不是说算出来就一定是0.4%，而是说这样算本身是错的）因为直接预测下一个词是”the“的话，我们是在使用一元语法，而247是来源于三元语法的。当我们在使用三元语法的时候，会考虑三元语法的统计数据，这样做出来的预测会不一样并且通常有更好的正确率。 参考 神秘的KS值和GINI系数 二分类模型评价指标-KS值 困惑度]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT算法原理解析]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FGBDT%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文为博客GBDT算法原理深入解析和博客xgboost的原理没你想像的那么难的摘抄，这两篇文章写得通俗易懂，所以就将两个文章写得较好的部分整合摘抄形成本博文。 一、前言1.1 梯度提升梯度提升（Gradient boosting）是一种用于回归、分类和排序任务的机器学习技术，属于Boosting算法族的一部分。Boosting是一族可将弱学习器提升为强学习器的算法，属于集成学习（ensemble learning）的范畴。Boosting方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断要好。通俗地说，就是“三个臭皮匠顶个诸葛亮”的道理。梯度提升同其他boosting方法一样，通过集成（ensemble）多个弱学习器，通常是决策树，来构建最终的预测模型。 1.2 集成学习Boosting、bagging和stacking是集成学习的三种主要方法。不同于bagging方法，boosting方法通过分步迭代（stage-wise）的方式来构建模型，在迭代的每一步构建的弱学习器都是为了弥补已有模型的不足。Boosting族算法的著名代表是AdaBoost，AdaBoost算法通过给已有模型预测错误的样本更高的权重，使得先前的学习器做错的训练样本在后续受到更多的关注的方式来弥补已有模型的不足。与AdaBoost算法不同，梯度提升方法在迭代的每一步构建一个能够沿着梯度最陡的方向降低损失（steepest-descent）的学习器来弥补已有模型的不足。经典的AdaBoost算法只能处理采用指数损失函数的二分类学习任务，而梯度提升方法通过设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。另一方面，AdaBoost算法对异常点（outlier）比较敏感，而梯度提升算法通过引入bagging思想、加入正则项等方法能够有效地抵御训练数据中的噪音，具有更好的健壮性。这也是为什么梯度提升算法（尤其是采用决策树作为弱学习器的GBDT算法）如此流行的原因，有种观点认为GBDT是性能最好的机器学习算法，这当然有点过于激进又固步自封的味道，但通常各类机器学习算法比赛的赢家们都非常青睐GBDT算法，由此可见该算法的实力不可小觑。 1.3 基于梯度提升算法的学习器基于梯度提升算法的学习器叫做GBM(Gradient Boosting Machine)。理论上，GBM可以选择各种不同的学习算法作为基学习器。现实中，用得最多的基学习器是决策树。为什么梯度提升方法倾向于选择决策树（通常是CART树）作为基学习器呢？这与决策树算法自身的优点有很大的关系。决策树可以认为是if-then规则的集合，易于理解，可解释性强，预测速度快。同时，决策树算法相比于其他的算法需要更少的特征工程，比如可以不用做特征标准化，可以很好的处理字段缺失的数据，也可以不用关心特征间是否相互依赖等。决策树能够自动组合多个特征，它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。不过，单独使用决策树算法时，有容易过拟合缺点。所幸的是，通过各种方法，抑制决策树的复杂性，降低单颗决策树的拟合能力，再通过梯度提升的方法集成多个决策树，最终能够很好的解决过拟合的问题。由此可见，梯度提升方法和决策树学习算法可以互相取长补短，是一对完美的搭档。至于抑制单颗决策树的复杂度的方法有很多，比如限制树的最大深度、限制叶子节点的最少样本数量、限制节点分裂时的最少样本数量、吸收bagging的思想对训练样本采样（subsample），在学习单颗决策树时只使用一部分训练样本、借鉴随机森林的思路在学习单颗决策树时只采样一部分特征、在目标函数中添加正则项惩罚复杂的树结构等。现在主流的GBDT算法实现中这些方法基本上都有实现，因此GBDT算法的超参数还是比较多的，应用过程中需要精心调参，并用交叉验证的方法选择最佳参数。 本文对GBDT算法原理进行介绍，从机器学习的关键元素出发，一步一步推导出GBDT算法背后的理论基础，读者可以从这个过程中了解到GBDT算法的来龙去脉。对于该算法的工程实现，本文也有较好的指导意义，实际上对机器学习关键概念元素的区分对应了软件工程中的“开放封闭原则”的思想，基于此思想的实现将会具有很好的模块独立性和扩展性。 二、机器学习的关键元素先复习下监督学习的关键概念：模型（model）、参数（parameters）、目标函数（objective function）模型就是所要学习的条件概率分布或者决策函数，它决定了在给定特征向量$x$时如何预测出目标$y$。定义$x_{i} \in R^{d}$为训练集中的第$i$个训练样本,则线性模型（linear model）可以表示为：$\hat{y_{i}}=\sum_{j}w_{j}x_{ij}$。模型预测的分数$\hat{y}_{i}$在不同的任务中有不同的解释。例如在逻辑回归任务中,$1/(exp(-\hat{y}_{i})$表示模型预测为正例的概率；而在排序学习任务中，$\hat{y}_{i}$表示排序分。参数就是我们要从数据中学习得到的内容。模型通常是由一个参数向量决定的函数。例如，线性模型的参数可以表示为:$\Theta = {w_{i}|j=1,….,s}$。目标函数通常定义为如下形式：$$Obj(\Theta)=L(\Theta)+\Omega(\Theta)$$其中，$L(\Theta)$是损失函数，用来衡量模型拟合训练数据的好坏程度；$\Omega(\Theta)$称之为正则项，用来衡量学习到的模型的复杂度。训练集上的损失（Loss）定义为：$L=\sum_{i=1}^{n}l(y_{i},\hat{y}_{i})$。常用的损失函数有如下两个： 平方损失函数(square loss):$l(y_{i},\hat{y}_{i})=(y_{i}-\hat{y}_{i})^2$; Logistic损失：$l(y_{i},\hat{y}_{i})=y_{i}ln(\hat{y}_{i})+(1-y_{i})ln(1-\hat{y}_{i})$ 正则项有L1正则和L2正则: L1正则:$\Omega(\Theta)=\lambda||w||_{1} $ L2正则:$\Omega(\Theta)=\lambda||w||_{2} $ Ridge regression就是指使用平方损失和L2范数正则项的线性回归模型；Lasso regression就是指使用平方损失和L1范数正则项的线性回归模型；逻辑回归（Logistic regression）指使用logistic损失和L2范数或L1范数正则项的线性模型。目标函数之所以定义为损失函数和正则项两部分，是为了尽可能平衡模型的偏差和方差（Bias Variance Trade-off）。最小化目标函数意味着同时最小化损失函数和正则项，损失函数最小化表明模型能够较好的拟合训练数据，一般也预示着模型能够较好地拟合真实数据（groud true）；另一方面，对正则项的优化鼓励算法学习到较简单的模型，简单模型一般在测试样本上的预测结果比较稳定、方差较小（奥坎姆剃刀原则）。也就是说，优化损失函数尽量使模型走出欠拟合的状态，优化正则项尽量使模型避免过拟合。 从概念上区分模型、参数和目标函数给学习算法的工程实现带来了益处，使得机器学习的各个组成部分之间耦合尽量松散。 三、加法模型（additive model）加法模型本质上是一个元算法，适用于所有的加法模型，它是一种启发式算法。GBDT算法可以看成是由K棵树组成的加法模型：$$\hat{y}_{i}=\sum_{k=1}^{K}f_{k}(x_{i}),f_{k} \in F$$其中$F$为所有树组成的函数空间，以回归任务为例，回归树可以看作为一个把特征向量映射为某个score的函数。该模型的参数为：$\Theta={f_{1},f_{2},f_{3},….,f_{k}}$。于一般的机器学习算法不同的是，加法模型不是学习d维空间中的权重，而是直接学习函数（决策树）集合。上述加法模型的目标函数定义为：$$Obj=\sum_{i=1}^{n}l(y_{i},\hat{y}^{i})+\sum_{k=1}^{K}\Omega(f_{k})$$其中$\Omega$表示决策树的复杂度，那么该如何定义树的复杂度呢？比如，可以考虑树的节点数量、树的深度或者叶子节点所对应的分数的L2范数等等。如何来学习加法模型呢？解这一优化问题，可以用前向分布算法（forward stagewise algorithm）。因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数（结构），逐步逼近优化目标函数，那么就可以简化复杂度。这一学习过程称之为Boosting。具体地，我们的目标不再是直接优化整个目标函数，这已经被我们证明是行不通的。而是分步骤优化目标函数，首先优化第一棵树，完了之后再优化第二棵树，直至优化完K棵树。整个过程如下图所示：$$\begin{split}{} &amp; \hat{y}_{i}^{(0)}=0 \\{} &amp; \hat{y}_{i}^{(1)}=f_{1}(x_{i})=\hat{y}_{i}^{(0)}+f_{1}(x_{1}) \\{} &amp; \hat{y}_{i}^{(2)}=f_{1}(x_{i})+f_{2}(x_{i})=\hat{y}_{i}^{(1)}+f_{2}(x_{i}) \\{} &amp; ……\\{} &amp; \hat{y}_{i}^{(t)}=\sum_{k=1}^{t}f_{k}(x_{i}) = \hat{y}_{i}^{(t-1)}+f_{t}(x_{i})\end{split}$$那么，在每一步如何决定哪一个函数f被加入呢？指导原则还是最小化目标函数。在第t步，模型对xi的预测为：$\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+f_{t}(x_{i})$,其中$f_{t}(x_{i})$为这一轮我们要学习的函数（决策树）。这个时候目标函数可以写为：$$\begin{split}Obj^{(t)} {} &amp; = \sum_{i=1}^{n}l(y_{i},\hat{y}_{i}^{t})+\sum_{i=1}^{t}\Omega(f_{i}) \\ {} &amp; = \sum_{i=1}^{n}l(y_{i},\hat{y}_{i}^{t-1}+f_{t}(x_{i}))+\Omega(f_{t})+constant\end{split}$$constant就是前t-1棵树的复杂度,后面会如何衡量树的复杂度。假如我们使用的损失函数时MSE，那么上述表达式会变成这个样子：$$\begin{split}Obj^{(t)} {} &amp; = \sum_{i=1}^{n}(y_{i}-(\hat{y}_{i}^{t-1}+f_{t}(x_{i})))^2+\Omega(f_{t})+constant \\ {} &amp; = \sum_{i=1}^{n}[2(\hat{y}_{i}^{t-1}-y_{i})f_{t}(x_{i})+f_{t}(x_{i})^2]+\Omega(f_{t})+constant\end{split}$$其中,$\hat{y}_{i}^{t-1}-y_{i}$称之为残差（residual）。因此，使用平方损失函数时，GBDT算法的每一步在生成决策树时只需要拟合前面的模型的残差。 $f_t(x_i)$是什么？它其实就是$f_t$的某个叶子节点的值。之前我们提到过，叶子节点的值是可以作为模型的参数的。 泰勒公式： 设n是一个正整数，如果定义在一个包含a的区间上的函数f在a点处n+1次可导，那么对于这个区间上的任意x都有：$$f(x)=\sum_{n=0}^{N}\frac{f^{(n)}(a)}{n!}(x-a)^n+R_{n}(x)$$其中的多项式称为函数在a处的泰勒展开式，$R_{n}(x)$是泰勒公式的余项且是$(x−a)^n$的高阶无穷小。 根据泰勒公式把函数f(x+Δx)在点x处二阶展开，可得到如下等式：$$f(x+\Delta{x})≈f(x)+f^{′}(x)\Delta{x}+\frac{1}{2}f^{″}(x)\Delta{x}^{2}$$有目标函数的公式可知，目标函数是关于变量$\hat{y}_{i}^{t-1}+f_{t}(x_{i})$的函数，若把变量$\hat{y}_{i}^{t-1}$看成是泰勒展开式中的x,把变量$f_{t}(x_{i})$看成是泰勒展开式中的$\Delta{x}$,则目标函数可转为:$$Obj^{(t)}=\sum_{i=1}^{n}[l(y_{i},\hat{y}_{i}^{t-1})+g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})]+\Omega(f_{t})+constant$$其中:$$g_{i}=\frac{\partial l(y_{i},\hat{y}_{i}^{(t-1)})}{\partial \hat{y}_{i}^{(t-1)}} \\h_{i}=\frac{\partial^2 l(y_{i},\hat{y}_{i}^{(t-1)})}{\partial \hat{y}_{i}^{(t-1)}}$$ 简要说明下$g_{i}$和$h_{i}$的含义。$g_{i}$怎么理解呢？现有t-1棵树是不是？这t-1棵树组成的模型对第i个训练样本有一个预测值$\hat{y}_{i}$是不是？这个$\hat{y}_{i}$与第i个样本的真实标签$y_i$肯定有差距是不是？这个差距可以用$l(y_i,\hat{y}_{i})$这个损失函数来衡量是不是？现在gi和hi的含义你已经清楚了是不是？ 假设损失函数为平方损失函数,则有:$$\begin{split}{} &amp; g_{i}=\frac{\partial l(y_{i},\hat{y}_{i}^{(t-1)})}{\partial \hat{y}_{i}^{(t-1)}}=\frac{\partial (\hat{y}^{t-1}-y_{i})^2}{\partial \hat{y}_{i}^{(t-1)}}=2(\hat{y}_{i}^{(t-1)}-y_{i}) \\{} &amp; h_{i}=\frac{\partial^2 l(y_{i},\hat{y}_{i}^{(t-1)})}{\partial \hat{y}_{i}^{(t-1)}}=\frac{\partial^{2} (\hat{y}^{t-1}-y_{i})^2}{\partial \hat{y}_{i}^{(t-1)}}=2\end{split}$$ 我们的目标是让这个目标函数最小化，常数项显然没有什么用，我们把它们去掉，就变成了下面这样：$$Obj^{(t)}≈\sum_{i=1}^{N}[g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})]+\Omega(f_{t})$$ 由于要学习的函数仅仅依赖于目标函数，从上式可以看出只需为学习任务定义好损失函数，并为每个训练样本计算出损失函数的一阶导数和二阶导数，通过在训练样本集上最小化目标函数即可求得每步要学习的函数f(x)，从而根据加法模型可得最终要学习的模型。 四、模型正则化项上面的式子已然很漂亮，但是，后面的$\Omega(f_t)$仍然是云遮雾罩，不清不楚。现在我们就来定义如何衡量一棵树的正则化项。这个事儿并没有一个客观的标准，可以见仁见智。为此，我们先对CART树作另一番定义，如下所示：$$f_{t}(x)=w_{q(x)},w \in R^{T},q:R^{d} \rightarrow {1,2,3,…..,T}$$一颗生成好的CART树，假设其叶子节点个数为T，该CART树是由所有叶子节点对应的值组成的向量$w \in R^{T}$,以及一个把特征向量映射到叶子节点索引（Index）的函数$q:R^{d} \rightarrow {1,2,3,…..,T}$组成的。因此，CART树可以定义为$f_{t}(x)=w_{q(x)}$。 CART树复杂度可以由正则项$\Omega(f_{t})=\gamma T +\frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^{2}$来定义，即CART树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。 注意:这里出现了$\lambda$和$\gamma$,显然，$\gamma$越大，表示越希望获得结构简单的树，因为此时对较多叶子节点的树的惩罚越大。$\lambda$越大也是越希望获得结构简单的树。xgboost选择的就是这样的正则化，为什么呢？很简单，好使！效果好才是真的好。 五、GBDT算法至此，我们关于第t棵树的优化目标已然很清晰，下面我们对它做下变形，将正则项加入其中则有:$$\begin{split}Obj^{(t)} {} &amp;≈\sum_{i=1}^{N}[g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})]+\Omega(f_{t}) \\ {} &amp;=\sum_{i=1}^{N}[g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}^{2}(x_{i})]+\gamma T +\frac{1}{2}\lambda \sum_{j=1}^{T}w_{j}^{2} \\ {} &amp;=\sum_{j=1}^{T}\Bigg[(\sum_{i \in I_{j}}g_{i})w_{i}+\frac{1}{2}(\sum_{i \in I_{j}}h_{i}+\lambda)w_{j}^{2}\Bigg]+\gamma T\end{split}$$定义:$$\begin{split}{} &amp; G_{i} = \sum_{i \in I_{j}}g_{i} \\{} &amp; H_{i} = \sum_{i \in I_{j}}h_{}\end{split}$$ $I_j$代表什么？它代表一个集合，集合中每个值代表一个训练样本的序号，整个集合就是被第t棵CART树分到了第j个叶子节点上的训练样本。 则上述目标等式可写为:$$Obj^{(t)}=\sum_{j=1}^{T}\Bigg[G_{i}w_{j}+\frac{1}{2}(H_{i}+\lambda)w_{j}^{2}\Bigg]+\gamma T$$假设树的结构是固定的，即函数$q(x)$确定，令函数$Obj^{(t)}$的一阶导数等于0，即可求得叶子节点j对应的值为：$$w_{j}^{ * }=-\frac{G_{i}}{H_{i}+\lambda}$$ 此时，目标函数的值为：$$Obj^{ * }=-\frac{1}{2}\sum_{j=1}^{T}\frac{G_{i}^{2}}{H_{i}+\lambda}+\gamma T$$ $Obj^{*}$它表示了这棵树的结构有多好，值越小，代表这样结构越好,也就是说，它是衡量第t棵CART树的结构好坏的标准。注意~注意~注意~，这个值仅仅是用来衡量结构的好坏的，与叶子节点的值可是无关的。为什么？请再仔细看一下$Obj^{*}$的推导过程。$Obj^{*}$只和$G_j$和$H_j$和T有关，而它们又只和树的结构(q(x))有关，与叶子节点的值可是半毛关系没有。如下图所示：这里，我们对$w^{*}_j$给出一个直觉的解释，以便能获得感性的认识。我们假设分到j这个叶子节点上的样本只有一个。那么，$w^{*}_j$就变成如下这个样子： 这个式子告诉我们，$w^{*}_j$的最佳值就是负的梯度乘以一个权重系数，该系数类似于随机梯度下降中的学习率。观察这个权重系数，我们发现，$h_j$越大，这个系数越小，也就是学习率越小。$h_j$越大代表什么意思呢？代表在该点附近梯度变化非常剧烈，可能只要一点点的改变，梯度就从10000变到了1，所以，此时，我们在使用反向梯度更新时步子就要小而又小，也就是权重系数要更小。 综上，为了便于理解，单颗决策树的学习过程可以大致描述为： 枚举所有可能的树结构q 用等式$Obj^{*}$为每个q计算其对应的分数$Obj^{*}$，分数越小说明对应的树结构越好 根据上一步的结果，找到最佳的树结构，用等式$w^{*}_j$为树的每个叶子节点计算预测值 然而，可能的树结构数量是无穷的，所以实际上我们不可能枚举所有可能的树结构。通常情况下，我们采用贪心策略来生成决策树的每个节点。 从深度为0的树开始，对每个叶节点枚举所有的可用特征 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益） 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集 回到第1步，递归执行到满足特定条件为止在上述算法的第二步，样本排序的时间复杂度为$O(nlogn)$，假设公用K个特征，那么生成一颗深度为K的树的时间复杂度为$O(dKnlogn)$。具体实现可以进一步优化计算复杂度，比如可以缓存每个特征的排序结果等。 如何计算每次分裂的收益呢？假设当前节点记为C,分裂之后左孩子节点记为L，右孩子节点记为R，则该分裂获得的收益定义为当前节点的目标函数值减去左右两个孩子节点的目标函数值之和：$Gain=Obj^{*}_{C}-Obj^{*}_{L}-Obj^{*}_{R}$,带入式子之后有:$$Gain=\frac{1}{2}\Bigg[\frac{G^{2}_{L}}{H_{L}+\lambda}+\frac{G^{2}_{R}}{H_{R}+\lambda}-\frac{(G_{L}+G_{R})^{2}}{H_{L}+H_{R}+\lambda}\Bigg]-\gamma$$其中，$-\gamma$项表示因为增加了树的复杂性（该分裂增加了一个叶子节点）带来的惩罚。 这个Gain实际上就是单节点的$obj^{*}$减去切分后的两个节点的树obj，Gain如果是正的，并且值越大，表示切分后obj越小于单节点的$obj^{*}$，就越值得切分。同时，我们还可以观察到，Gain的左半部分如果小于右侧的γ，则Gain就是负的，表明切分后obj反而变大了。γ在这里实际上是一个临界值，它的值越大，表示我们对切分后obj下降幅度要求越严。 注意：xgboost的切分操作和普通的决策树切分过程是不一样的。普通的决策树在切分的时候并不考虑树的复杂度，而依赖后续的剪枝操作来控制。xgboost在切分的时候就已经考虑了树的复杂度，就是那个γ参数。所以，它不需要进行单独的剪枝操作。 最后，总结一下GBDT的学习算法： 算法每次迭代生成一颗新的决策树 在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数$g_i$和二阶导数$h_i$ 通过贪心策略生成新的决策树，通过计算$w^{*}_{i}$计算每个叶节点对应的预测值 把新生成的树$f_{t}(x)$添加到模型中:$\hat{y}^{(t)}_{i}=\hat{y}^{(t-1)}_{i}+f_{t}(x)$ 通常在第四步，我们把模型更新公式替换为：$\hat{y}^{(t)}_{i}=\hat{y}^{(t-1)}_{i}+\varepsilon f_{t}(x)$,其中$\varepsilon$称之为步长或者学习率。增加$\varepsilon$因子的目的是为了避免模型过拟合。 六、GBDT小结GDBT本身并不复杂，不过要吃透的话需要对集成学习的原理，决策树原理和各种损失函树有一定的了解。由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。最后总结下GBDT的优缺点。GBDT主要的优点有： 可以灵活处理各种类型的数据，包括连续值和离散值。 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。GBDT的主要缺点有： 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。 七、XGB与GBDTXGB模型属于GBDT模型的升级,其主要与GBDT的区别: XGBoost在代价函数中加入了正则项，用于控制模型的复杂度(降低叶子节点个数及叶子节点的权重)。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性； 传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数； 和GBDT只支持CART作为基分类器之外，还支持线性分类器，在使用线性分类器的时候可以使用L1，L2正则化。 列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算； shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）； 在寻找最佳分割点时，考虑到传统的贪心算法效率较低，实现了一种近似贪心算法，用来加速和减小内存消耗，除此之外还考虑了稀疏数据集和缺失值的处理，对于特征的值有缺失的样本，XGBoost依然能自动找到其要分裂的方向。 XGBoost支持并行处理，XGBoost的并行不是在模型上的并行，而是在特征上的并行，将特征列排序后以block的形式存储在内存中，在后面的迭代中重复使用这个结构。这个block也使得并行化成为了可能，其次在进行节点分裂时，计算每个特征的增益，最终选择增益最大的那个特征去做分割，那么各个特征的增益计算就可以开多线程进行。 八、参考 集成学习总结]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
        <tag>GBM</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark基础笔记]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%B7%A5%E5%85%B7%2Fspark%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1、从Seq中构造DataFrame123456val df = Seq( (1,12345678,"this is a test"), (1,23456789, "another test"), (2,2345678,"2nd test"), (2,1234567, "2nd another test")).toDF("id","timestamp","data") 结果为:12345678+---+---------+----------------+| id|timestamp| data|+---+---------+----------------+| 1| 12345678| this is a test|| 1| 23456789| another test|| 2| 2345678| 2nd test|| 2| 1234567|2nd another test|+---+---------+----------------+ 2、DataFrame的joinspark datafrme提供了强大的JOIN操作。但是在操作的时候，经常发现会碰到重复列的问题。如下：如分别创建两个DF，其结果如下：1234val df = sc.parallelize(Array( ("one", "A", 1), ("one", "B", 2), ("two", "A", 3), ("two", "B", 4))).toDF("key1", "key2", "value")df.show() 12345678+----+----+-----+|key1|key2|value|+----+----+-----+| one| A| 1|| one| B| 2|| two| A| 3|| two| B| 4|+----+----+-----+ 1234val df2 = sc.parallelize(Array( ("one", "A", 5), ("two", "A", 6))).toDF("key1", "key2", "value2")df2.show() 123456+----+----+------+|key1|key2|value2|+----+----+------+| one| A| 5|| two| A| 6|+----+----+------+ 对其进行JOIN操作之后，发现多产生了KEY1和KEY2这样的两个字段。12val joined = df.join(df2, df("key1") === df2("key1") &amp;&amp; df("key2") === df2("key2"), "left_outer")joined.show() 12345678+----+----+-----+----+----+------+|key1|key2|value|key1|key2|value2|+----+----+-----+----+----+------+| two| A| 3| two| A| 6|| one| A| 1| one| A| 5|| two| B| 4|null|null| null|| one| B| 2|null|null| null|+----+----+-----+----+----+------+ 假如这两个字段同时存在，那么就会报错，如下：org.apache.spark.sql.AnalysisException: Reference ‘key2’ is ambiguous因此，网上有很多关于如何在JOIN之后删除列的，后来经过仔细查找，才发现通过修改JOIN的表达式，完全可以避免这个问题。而且非常简单。主要是通过Seq这个对象来实现。1df.join(df2, Seq[String]("key1", "key2"), "left_outer").show() 12345678+----+----+-----+------+|key1|key2|value|value2|+----+----+-----+------+| two| A| 3| 6|| one| A| 1| 5|| two| B| 4| null|| one| B| 2| null|+----+----+-----+------+ 3、DataFrame选中N列有时候我们需要从一个DataFrame中选出N列，这个选中的列名在一个list中，如下的伪代码表示:12var columnNames = getColumns(x) // Returns a List[Column]df.select(columns) //trying to get 一种可行的方式如下：1val result = df.select(columnNames.head, columnNames.tail: _*) 一种简洁的方式:12import org.apache.spark.sql.functions.coldf.select(columnNames.map(col): _*) 4、spark的DataFrame一列分割成多列背景:在工作需求中我目前遇到一个将一列展开成1900多列的一个任务，每个列的列名和其在表中那一列的数据一一对应。逻辑：将该列先split，然后在通过位置索引取值然后加上对应的列名进行展开。首先有如下数据feature_df:bill_feature中使用,分割之后为一个列表，大致有1900维度的数据，需要将其展开。123456789101112131415161718192021222324--------+--------+--------------------+|order_id|math_num| bill_feature|+--------+--------+--------------------+|282101x3| 847|0.105263157894736...||383107b1| 525|0.0,0.0,0.0,0.0,0...||493408c3| 630|0.208333333333333...||413001d9| 490|0.166666666666666...||563711e2| 602|0.148148148148148...||539906f2| 259|0.030303030303030...||466924g2| 1071|0.238095238095238...||258430f9| 1071|0.229729729729729...||370711a6| 959|0.246753246753246...||645012c2| 931|0.395833333333333...||574415t9| 588|0.153846153846153...||402916y9| 1022|0.220930232558139...||430205z9| 469|0.111111111111111...||266225g7| 693|0.090909090909090...||614527o5| 532|0.0625,1.57697780...||180837m9| 350|0.090909090909090...||381138k5| 854|0.285714285714285...||549847a4| 602|0.0,0.0,0.0,0.0,0...||491850p7| 637|0.44,0.3512465171...||380000i0| 833|0.120689655172413...|+--------+--------+--------------------+ 有如下两种方式: 使用withColumn(在1900多维度的情况下速度及其慢，不建议推荐，在进行其他复杂逻辑列的添加情况下使用且添加列少) 123456789import org.apache.spark.sql.&#123;DataFrame, SparkSession, functions&#125;result_df = result_df.withColumn("feature_list", functions.split(result_df.col("bill_feature"), ","))for(feature_name&lt;-feature_name_id_map.keySet)&#123; val add_col_udf=functions.udf((feature_list:Seq[String])=&gt;&#123; val index = feature_name_id_map(feature_name) feature_list(index).toString.toDouble &#125;) result_df = result_df.withColumn(feature_name,add_col_udf(result_df.col("feature_list")))&#125; 使用select的方式(面对1900多维度情况下，很快，首选推荐) 123456789import org.apache.spark.sql.&#123;DataFrame, SparkSession, functions&#125;import org.apache.spark.sql.types._val feature_id_name_map = feature_name_id_map.map(x =&gt; (x._2, x._1))result_df = result_df.withColumn("feature_list", functions.split(result_df.col("bill_feature"), ","))result_df = result_df.select( functions.col("order_id") +: functions.col("math_num") +: (0 until feature_name_id_map.size).map(i =&gt; functions.col("feature_list")(i).alias(feature_id_name_map(i)).cast(DoubleType)): _* )]]></content>
      <categories>
        <category>工程工具</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树(ID3 & C4.5 & CART)及正则剪枝]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%86%B3%E7%AD%96%E6%A0%91(ID3%20%26%20C4.5%20%26%20CART)%E5%8F%8A%E6%AD%A3%E5%88%99%E5%89%AA%E6%9E%9D%2F</url>
    <content type="text"><![CDATA[1、介绍决策树（Decision Tree）的思想是贪心(最优化分)与分治(子树划分)。构建决策树的目的是：随着划分过程的进行，使得决策树分支结点所包含的样本尽可能属于同一类别，即使得分类更准确。下图给出了一个决策树的简单例子:决策树模型在监督学习中非常常见，可用于分类（二分类、多分类）和回归。虽然将多棵弱决策树的Random Forest、GBDT等tree ensembel 模型更为常见，但是“完全生长”决策树因为其简单直观，具有很强的解释性，也有广泛的应用，而且决策树是tree ensemble 的基础，值得好好理解。一般而言一棵“完全生长”的决策树包含，特征选择、决策树构建、剪枝三个过程，本文主要介绍ID3、C4.5和CART这三种构造决策树的算法以及简要介绍树模型的正则和剪枝。 决策树的优点和缺点优点： 决策树算法中学习简单的决策规则建立决策树模型的过程非常容易理解， 决策树模型可以可视化，非常直观 应用范围广，可用于分类和回归，而且非常容易做多类别的分类 能够处理数值型和连续的样本特征 缺点： 很容易在训练数据中生成复杂的树结构，造成过拟合（overfitting）。剪枝可以缓解过拟合的负作用，常用方法是限制树的高度、叶子节点中的最少样本数量。 学习一棵最优的决策树被认为是NP-Complete问题。实际中的决策树是基于启发式的贪心算法建立的，这种算法不能保证建立全局最优的决策树。Random Forest 引入随机能缓解这个问题 2、ID3算法ID3算法最早是由罗斯昆（J. Ross Quinlan）于1975年在悉尼大学提出的一种分类预测算法，算法的核心是“信息熵”。ID3算法通过计算每个属性的信息增益，认为信息增益高的是好属性，每次划分选取信息增益最高的属性为划分标准，重复这个过程，直至生成一个能完美分类训练样例的决策树。 2.1、信息熵熵这个概念最早起源于物理学，在物理学中是用来度量一个热力学系统的无序程度，而在信息学里面，熵是对不确定性的度量。在1948年，香农引入了信息熵（information entropy），将其定义为离散随机事件出现的概率，一个系统越是有序，信息熵就越低，反之一个系统越是混乱，它的信息熵就越高。所以信息熵可以被认为是系统有序化程度的一个度量。对于有K个类别的分类问题来说，假定样本集合D中第 k 类样本所占的比例为$p_{k}(k=1,2,…,K)$,则样本集合D的信息熵定义为:$$H(D)=-\sum_{k=1}^{K}p_{k}log_{2}p_{k}$$ 2.2、信息增益定义：特征A对于数据集合D的信息增益为g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下集合D的条件熵H(D|A)只差，即$$g(D,A)=H(D)-H(D|A)$$一般的，熵H(D)与条件熵H(D|A)之差称为互信息，决策树学习中的信息增益等价于数据集中类与特征的互信息。通过以下例子计算信息增益:计算过程树下：(1) 首先计算经验熵H(D):$$H(D)=-\frac{9}{15}log_{2}\frac{9}{15}-\frac{6}{15}log_{2}\frac{6}{15}=0.971$$(2) 计算各特征对于数据集合D的信息增益，分别以$A_{1}、A_{2}、A_{3}、A_{4}$分别表示年龄、有工作、有自己的房子和信贷情况4个特征，则计算如下：计算$A_1$$$\begin{split}g(D,A_{1}){} &amp;=H(D)-[\frac{5}{15}H(D_{1})+\frac{5}{15}H(D_{2})+\frac{5}{15}H(D_{3})] \\ {} &amp;=0.971-[\frac{5}{15}(-\frac{2}{5}log_{2}\frac{2}{5}-\frac{3}{5}log_{2}\frac{3}{5}) \\ {} &amp; \quad +\frac{5}{15}(-\frac{3}{5}log_{2}\frac{3}{5}-\frac{2}{5}log_{2}\frac{2}{5}) \\ {} &amp; \quad +\frac{5}{15}(-\frac{1}{5}log_{2}\frac{1}{5}-\frac{4}{5}log_{2}\frac{4}{5})] \\ {} &amp;=0.971-0.888=0.083\end{split}$$这里的$A_{1}、A_{2}、A_{3}$分别是D中取值为青年、中年、老年的样本子集。类似的计算$A_{2}、A_{3}、A_{4}$，结果如下：$$\begin{split}g(D,A_{2}){} &amp;=H(D)-[\frac{5}{15}H(D_{1})+\frac{10}{15}H(D_{2})]=0.324 \\g(D,A_{3}){} &amp;=0.420 \\g(D,A_{4}){} &amp;=0.363\end{split}$$最后,比较各特征的信息增益，由于特征$A_3$(有自己的房子)的信息增益值最大，所以选择$A_3$做为最优特征。 2.3、算法流程和思想ID3算法流程如下：输入：数据集D，特征集A，阈值$\varepsilon$输出：决策树T (1) 若D中的所有实例都属于同一类$C_{k}$,则T为单节点树，并将类$C_{k}$做为该节点的类标记，返回T； (2) 若$A=\phi$，则T为单节点树，并将D中实例最大的类$C_{k}$做为该节点的类标记，返回T； (3) 否则按照上述例子计算A中各特征对D的信息增益，选择信息增益最大的特征$A_{g}$； (4) 如果$A_{g}$的信息增益小于阈值$\varepsilon$，则置T为单节点树，并将D中实例数最大的类$C_{k}$作为该节点的类标记，返回T; (5) 否则，对$A_{g}$的每一个可能只$a_{i}$，依据$A_{g}=a_{i}$将D分割若干非空子集$D_{i}$,将$D_{i}$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； (6) 对第i个子结点，以$D_{i}$为训练集，以$A-A_{g}$为特征集，递归的调用(1)~(5)步，得到子树$T_{i}$,返回$T_{i}$。 ID3算法的基本思想是：首先计算出原始数据集的信息熵，然后依次将数据中的每一个特征作为分支标准，并计算其相对于原始数据的信息增益，选择最大信息增益的分支标准来划分数据，因为信息增益越大，区分样本的能力就越强，越具有代表性。重复上述过程从而生成一棵决策树，很显然这是一种自顶向下的贪心策略。 3、C4.5算法ID3中使用信息增益其实是有一个缺点，那就是它偏向于具有大量值的属性–就是说在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的。所以在ID3算法上进行改进，C4.5使用了信息增益比来进行特征选择。 3.1、信息增益比信息增益值的大小是相对于训练数据集而言，并无绝对的意义，在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大，反之，信息增益值会变小。使用信息增益比可以对这一问题进行校正。定义： 特征集A对训练集D的信息增益比$g_{R}(D,A)$定义为其信息增益$g(D,A)$与训练数据集D的经验熵H(D)之比：$$g_{R}(D,A)=\frac{g(D,A)}{H(D)}$$ 3.2、算法流程和思想C4.5的算法流程和ID3的算法流程一致，只是选取特征的方式不同。C4.5算法思想C4.5算法是对ID3算法的改进，C4.5克服了ID3的2个缺点： 用信息增益选择属性时偏向于选择分枝比较多的属性值，即取值多的属性 不能处理连续属性 注意: 增益率准则对可取值数目较少的属性有所偏好，因此C4.5算法并不是直接选择增益率最大的属性作为分支标准，而是先从候选属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。 C4.5算法处理连续属性的方法是先把连续属性转换为离散属性再进行处理。虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的 4、CART算法CART（Classification And Regression Tree）算法既可以用于创建分类树，也可以用于创建回归树。CART算法的重要特点包含以下三个方面： 二分(Binary Split)：在每次判断过程中，都是对样本数据进行二分。CART算法是一种二分递归分割技术，把当前样本划分为两个子样本，使得生成的每个非叶子结点都有两个分支，因此CART算法生成的决策树是结构简洁的二叉树。由于CART算法构成的是一个二叉树，它在每一步的决策时只能是“是”或者“否”，即使一个feature有多个取值，也是把数据分为两部分 单变量分割(Split Based on One Variable)：每次最优划分都是针对单个变量。 剪枝策略：CART算法的关键点，也是整个Tree-Based算法的关键步骤。剪枝过程特别重要，所以在最优决策树生成过程中占有重要地位。有研究表明，剪枝过程的重要性要比树生成过程更为重要，对于不同的划分标准生成的最大树(Maximum Tree)，在剪枝之后都能够保留最重要的属性划分，差别不大。反而是剪枝方法对于最优树的生成更为关键。CART树生成就是递归的构建二叉决策树的过程，对回归使用平方误差最小化准则，对于分类树使用基尼指数(Gini index)准则，进行特征选择，生成二叉树。 4.1、回归树CART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。$$D={(x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3})….,(x_{n},y_{n})}$$ 选择最优切分变量j与切分点s： 遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中$R_m$是被划分的输入空间，$c_m$是空间$R_m$对应的固定输出值。$$min_{j,s}\big[min_{c_{i}}\sum_{x_{i} \in R_{i}(j,s)}(y_{i}-c_{i})^2 + min_{c_{2}}\sum_{x_{i} \in R_{i}(j,s)}(y_{i}-c_{1})^2\big]$$ 用选定的(j,s)对，划分区域并决定相应的输出值:$$R_{1}(j,s)={x|x^{(j)} \leq s},R_{2}(j,s)={x|x^{(j)} &gt; s} \\\hat{c}_{m} = \frac{1}{N_{m}}\sum_{x_{i} \in R_{m}(j,s)} y_{i} \\x \in R_{m},m=1,2,…..$$继续对两个子区域调用上述步骤，将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。$$f(x)=\sum_{m=1}^{M}\hat{c}_{m}I(x\in R_{m})$$当输入空间划分确定时，可以用 平方误差 来表示回归树对于训练数据的预测方法，用 平方误差 最小的准则求解每个单元上的最优输出值。$$\sum_{x_{i} \in R_{m}}(y_{i}-f(x_{i}))^2$$ 实例计算考虑下图所示的连续性变量，根据给定的数据点，考虑1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5切分点。对各切分点依次求出$R_1$,$R_2$,$c_1$,$c_2$及m(s)。例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中$c_1$,$c_2$,m(s)计算如下所示：依次改变(j,s)对，可以得到s及m(s)的计算结果，如下表所示。当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。回归树$T_{1}(x)$为然后我们利用$f_{1}(x)$拟合训练数据的残差，如下表所示用f1(x)拟合训练数据得到平方误差第二步求$T_{2}(x)$与求$T_{1}(x)$方法相同，只是拟合的数据是上表的残差。可以得到用f2(x)拟合训练数据的平方误差继续求得$T_3(x)、T_4(x)、T_5(x)、T_6(x)$，如下所示用$f_6(x)$拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么$f(x)=f_6(x)$便是所求的回归树。 4.2、分类树CART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为$p_k$，则概率分布的基尼指数定义为:根据基尼指数定义，可以得到样本集合D的基尼指数，其中$C_k$表示数据集D中属于第k类的样本子集。如果数据集D根据特征A在某一取值a上进行分割，得到$D_1,D_2$两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。 其中算法的停止条件有： 1、节点中的样本个数小于预定阈值， 2、样本集的Gini系数小于预定阈值（此时样本基本属于同一类）， 3、或没有更多特征。 实例计算针对上述离散型数据，按照体温为恒温和非恒温进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算$D_1,D_2$的基尼指数。然后计算得到特征体温下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。 4.3、分类树 VS 回归树提到决策树算法，很多想到的就是上面提到的ID3、C4.5、CART分类决策树。其实决策树分为分类树和回归树，前者用于分类，如晴天/阴天/雨天、用户性别、邮件是否是垃圾邮件，后者用于预测实数值，如明天的温度、用户的年龄等。 作为对比，先说分类树，我们知道ID3、C4.5分类树在每次分枝时，是穷举每一个特征属性的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值。按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 5、正则和剪枝树剪枝和CART剪枝决策树很容易发生过拟合，可以改善的方法有： 1、通过阈值控制终止条件，避免树形结构分支过细。 2、通过对已经形成的决策树进行剪枝来避免过拟合。 3、基于Bootstrap的思想建立随机森林。 剪枝(pruning)是解决决策树过拟合的主要手段，通过剪枝可以大大提升决策树的泛化能力。通常，剪枝处理可分为：预剪枝，后剪枝。 预剪枝：通过启发式方法，在生成决策树过程中对划分进行预测，若当前结点的划分不能对决策树泛化性能提升，则停止划分，并将其标记为叶节点 后剪枝：对已有的决策树，自底向上的对非叶结点进行考察，若该结点对应的子树替换为叶结点能提升决策树的泛化能力，则将改子树替换为叶结点 6、决策树算法总结上面我们对CART算法做了一个详细的介绍，CART算法相比C4.5算法的分类方法，采用了简化的二叉树模型，同时特征选择采用了近似的基尼系数来简化计算。当然CART树最大的好处是还可以做回归模型，这个C4.5没有。下表给出了ID3，C4.5和CART的一个比较总结。希望可以帮助大家理解。 算法 支持模型 树结构 特征选择 连续值处理 缺失值处理 剪枝 ID3 分类 多叉树 信息增益 不支持 不支持 不支持 C4.5 分类 多叉树 信息增益比 支持 支持 支持 CART 分类，回归 二叉树 基尼系数，均方差 支持 支持 支持 看起来CART算法高大上，那么CART算法还有没有什么缺点呢？有！主要的缺点我认为如下： 1）应该大家有注意到，无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1，这里不多介绍。 2）如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。 7、参考 《统计机器学习方法,李航》 决策树算法原理(下) 机器学习之分类与回归树(CART) CART 分类与回归树 决策树CART与ID3,C4.5联系与区别 决策树（ID3、C4.5、CART） 决策树(ID3 &amp; C4.5 &amp; CART)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>CART算法</tag>
        <tag>ID3</tag>
        <tag>C4.5</tag>
        <tag>剪枝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通俗理解之条件随机场(CRF)]]></title>
    <url>%2FNLP%2F%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3%E4%B9%8B%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA-CRF%2F</url>
    <content type="text"><![CDATA[1、简介在前面两篇博文中(条件随机场之基本概念与模型和条件随机场之学习和预测问题)介绍了CRF的理论知识，主要是统计学习方法中的一个笔记。其实只是看这两篇理论的文章还是很难理解的，最好的办法就是用一个现实的例子来说明它并使用通俗的方式把算法梳理一遍。通俗介绍CRF的文章不多，无意中看到了如何轻松愉快地理解条件随机场（CRF）和英文原文,看完后觉得对条件随机场的理解清晰了很多。以下的博客大多来自这两篇博文的摘抄！ 2、举例通俗说明拿NLP中的词性标注这个任务来说吧，比如目前我们需要对“Bob drank coffee at Starbucks”进行词性标注，注明每个单词的词性后是这样的：“Bob (名词) drank(动词) coffee(名词) at(介词) Starbucks(名词)”。下面，就用条件随机场来解决这个问题。 以上面的话为例，有5个单词，我们将：(名词，动词，名词，介词，名词) 作为一个标注序列，称为l，可选的标注序列有很多种，比如l还可以是这样：（名词，动词，动词，介词，名词） ，我们要在这么多的可选标注序列中，挑选出一个 最靠谱 的作为我们对这句话的标注。 怎么判断一个标注序列靠谱不靠谱呢？就我们上面展示的两个标注序列来说，第二个显然不如第一个靠谱，因为它把第二、第三个单词都标注成了动词，动词后面接动词，这在一个句子中通常是说不通的。假如我们给每一个标注序列打分，打分越高代表这个标注序列越靠谱，我们至少可以说，凡是标注中出现了动词后面还是动词的标注序列，要给它负分！！ 上面所说的动词后面还是动词就是一个特征函数，我们可以定义一个特征函数集合，用这个特征函数集合来为一个标注序列打分，并据此选出最靠谱的标注序列。也就是说，每一个特征函数都可以用来为一个标注序列评分，把集合中所有特征函数对同一个标注序列的评分综合起来，就是这个标注序列最终的评分值。 3、CRF中的特征函数在CRF中，每个特征函数都是一个输入函数，它接受四个参数 句子s（就是我们要标注词性的句子） i，用来表示句子s中第i个单词 $l_i$，表示要评分的标注序列给第i个单词标注的词性(当前词) $l_{i-1}$，表示要评分的标注序列给第i-1个单词标注的词性(前一个词) 它的输出值是0或者1,0表示要评分的标注序列不符合这个特征，1表示要评分的标注序列符合这个特征。 我们的特征函数仅仅依靠当前单词的标签和它前面的单词的标签对标注序列进行评判，这样建立的CRF也叫作线性链CRF，这是CRF中的一种简单情况。为简单起见，本文中我们仅考虑线性链CRF。 4、特征函数到概率定义好一组特征函数后，我们要给每个特征函数$f_j$赋予一个权重$λ_j$。现在，只要有一个句子s，有一个标注序列l，我们就可以利用前面定义的特征函数集来对l评分。$$socre(l|s)=\sum_{j=1}^{m}\sum_{i=1}^{n}\lambda_{j}f_{j}(s,i,l_{i},l_{i-1})$$上式中有两个求和，外面的求和用来求每一个特征函数$f_j$评分值的和，里面的求和用来求句子中每个位置的单词的的特征值的和。对这个分数进行 指数化 和 标准化，我们就可以得到标注序列l的概率值 $p(l|s)$，如下所示：$$p(l|s)=\frac{exp[score(l|s)]}{\sum_{\hat{l}}exp[score(\hat{l}|s)]}=\frac{exp[\sum_{j=1}^{m}\sum_{i=1}^{n}\lambda_{j}f_{j}(s,i,l_{i},l_{i-1})]}{\sum_{\hat{l}}exp[\sum_{j=1}^{m}\sum_{i=1}^{n}\lambda_{j}f_{j}(s,i,\hat{l}_{i},\hat{l}_{i-1})]}$$ 5、特征函数的几个例子前面我们已经举过特征函数的例子，下面我们再看几个具体的例子，帮助增强大家的感性认识:$$f_{1}(s,i,l_{i},l_{i-1})=1$$ 当$l_i$是“副词”并且第i个单词以“ly”结尾时，我们就让$f_1 = 1$，其他情况$f_1$为0。不难想到，$f_1$特征函数的权重$λ_1$应当是正的。而且$λ_1$越大，表示我们越倾向于采用那些把以“ly”结尾的单词标注为“副词”的标注序列。 $$f_{2}(s,i,l_{i},l_{i-1})=1$$ 如果i=1，$l_i=动词$，并且句子s是以“？”结尾时，$f_2=1$，其他情况$f_2=0$。同样，$λ_2$应当是正的，并且$λ_2$越大，表示我们越倾向于采用那些把问句的第一个单词标注为“动词”的标注序列。 $$f_{3}(s,i,l_{i},l_{i-1})=1$$ 当$l_{i-1}$是介词，$l_i$是名词时，$f_3 = 1$，其他情况$f_3=0$。$λ_3$也应当是正的，并且$λ_3$越大，说明我们越认为介词后面应当跟一个名词。 $$f_{4}(s,i,l_{i},l_{i-1})=1$$ 如果$l_i$和$l_{i-1}$都是介词，那么$f_4$等于1，其他情况$f_4=0$。这里，我们应当可以想到$λ_4$是负的，并且$λ_4$的绝对值越大，表示我们越不认可介词后面还是介词的标注序列。 目前一个条件随机场就这样建立起来了，总结一下：为了建一个条件随机场，首先要定义一个特征函数集，每个特征函数都以整个句子s，当前位置i，位置i和i-1的标签为输入。然后为每一个特征函数赋予一个权重，然后针对每一个标注序列l，对所有的特征函数加权求和，必要的话，可以把求和的值转化为一个概率值。 6、CRF与逻辑回归的比较观察公式：$$p(l|s)=\frac{exp[score(l|s)]}{\sum_{\hat{l}}exp[score(\hat{l}|s)]}=\frac{exp[\sum_{j=1}^{m}\sum_{i=1}^{n}\lambda_{j}f_{j}(s,i,l_{i},l_{i-1})]}{\sum_{\hat{l}}exp[\sum_{j=1}^{m}\sum_{i=1}^{n}\lambda_{j}f_{j}(s,i,\hat{l}_{i},\hat{l}_{i-1})]}$$是不是有点逻辑回归的味道？事实上，条件随机场是逻辑回归的序列化版本。逻辑回归是用于分类的对数线性模型，条件随机场是用于序列化标注的对数线性模型。 7、CRF与HMM的比较对于词性标注问题，HMM模型也可以解决。HMM的思路是用生成办法，就是说，在已知要标注的句子s的情况下，去判断生成标注序列l的概率，如下所示：$$p(l,s)=p(l_{1})\prod_{i}p(l_{i}|l_{i-1})p(w_{i}|l_{i})$$这里：$p(l_{i}|l_{i-1})$是转移概率，比如，$l_{i-1}$是介词，$l_i$是名词，此时的p表示介词后面的词是名词的概率。$p(w_i|l_i)$表示发射概率（emission probability），比如$l_i$是名词，$w_i$是单词“ball”，此时的p表示在是名词的状态下，是单词“ball”的概率。 那么，HMM和CRF怎么比较呢？答案是：CRF比HMM要强大的多，它可以解决所有HMM能够解决的问题，并且还可以解决许多HMM解决不了的问题。事实上，我们可以对上面的HMM模型取对数，就变成下面这样：$$logp(l,s)=logp(l_0)+\sum_{i}logp(l_i|l_{i-1})+\sum_{i}logp(w_{i}|l_{i})$$我们把这个式子与CRF的式子进行比较：$$score(l|s)=\sum_{j=1}^{m}\sum_{i=1}^{n}\lambda_{j}f_{j}(s,i,l_{i},l_{i-1})$$不难发现，如果我们把第一个HMM式子中的log形式的概率看做是第二个CRF式子中的特征函数的权重的话，我们会发现，CRF和HMM具有相同的形式。 换句话说，我们可以构造一个CRF，使它与HMM的对数形式相同。怎么构造呢？对于HMM中的每一个转移概率$p(l_i=y|l_{i-1}=x)$,我们可以定义这样的一个特征函数：$$f_{x,y}(s,i,l_i,l_{i-1})=1$$该特征函数仅当l_i = y,l_i-1=x时才等于1。这个特征函数的权重如下：$$w_{x,y}=\log p(l_i=y|l_{i-1}=x)$$同样的，对于HMM中的每一个发射概率，我们也都可以定义相应的特征函数，并让该特征函数的权重等于HMM中的log形式的发射概率。 HMM的相关原理如果需要详细了解可以参考这篇博文马尔可夫模型(HMM) 用这些形式的特征函数和相应的权重计算出来的$p(l|s)$和对数形式的HMM模型几乎是一样的！用一句话来说明HMM和CRF的关系就是这样：每一个HMM模型都等价于某个CRF 但是，CRF要比HMM更加强大，原因主要有两点： CRF可以定义数量更多，种类更丰富的特征函数。HMM模型具有天然具有局部性，就是说，在HMM模型中，当前的单词只依赖于当前的标签，当前的标签只依赖于前一个标签。这样的局部性限制了HMM只能定义相应类型的特征函数，我们在上面也看到了。但是CRF却可以着眼于整个句子s定义更具有全局性的特征函数，如这个特征函数：$$f_{2}(s,i,l_{i-1},l_{i})=1$$如果i=1，$l_i=动词$，并且句子s是以“？”结尾时，$f_2=1$，其他情况$f_2=0$。 CRF可以使用任意的权重 将对数HMM模型看做CRF时，特征函数的权重由于是log形式的概率，所以都是小于等于0的，而且概率还要满足相应的限制，如$$0 \leq p(w_{i}|l_{i}) \leq 1,\sum_{w}p(w_{i}=w|l_{1})=1$$但在CRF中，每个特征函数的权重可以是任意值，没有这些限制。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>随机条件场</tag>
        <tag>序列标注</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件随机场之学习和预测问题]]></title>
    <url>%2FNLP%2F%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E4%B9%8B%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%A2%84%E6%B5%8B%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[一、条件随机场的学习问题条件随机场模型实际上是定义在时序数据上的对数线形模型，其学习方法包括极大似然估计和正则化的极大似然估计。具体的优化实现算法有改进的迭代尺度法IIS、梯度下降法以及拟牛顿法。（其中，主流的CRF软件之CRF++采用了拟牛顿法+L-BFGS优化，所以着重看这种训练方法即可。） 1.1、改进的迭代尺度法已知训练数据集，由此可知经验概率分布$\hat(P)(X,Y)$,可以通过极大化训练数据的对数似然函数来求模型参数。训练数据的对数似然函数为$$L(w)=L_{\hat(P)}(P(w))=log\prod_{x,y}P_{w}(x|y)^{\hat{P}(x,y)}=\sum_{x,y}\hat{P}(x,y)logP_{w}(y|x)$$当是一个由$P_{w}(y|x)=\frac{exp(w\cdot F(y,x))}{Z_{w}(x)}$和$Z_{w}(x)=\sum_{y}exp(w\cdot F(y,x))$给出的条件随机场模型时，对数似然函数为$$\begin{split}L(w){} &amp; =\sum_{x,y}\hat{P}(x,y)logP_{w}(y|x)\\ {} &amp; =\sum_{x,y}\Bigg(\hat{P}(x,y)\sum_{k=1}^{K}w_{k}f_{k}(y,x)-\hat{P}(x,y)logZ_{w}(x)\Bigg)\\ {} &amp; =\sum_{j=1}^{N}\sum_{k=1}^{K}w_{k}f_{k}(y_{j},x_{j})-\sum_{j=1}^{N}logZ_{w}(x_{j})\end{split}$$改进的迭代尺度法通过迭代的方法不断优化对数似然函数改变量的下界，达到极大化对数似然函数的目的。假设模型的当前参数向量$w=(w_{1},w_{2},….,w_{k})^T$向量的增量为$\delta=(\delta_{1},\delta_{2},….,\delta_{k})^T$,更新向量参数为$w+\delta=(w_{1}+\delta_{1},w_{2}+\delta_{2},…,w_{k}+\delta_{k})^T$在每步迭代过程中，改进的迭代尺度法通过依次求解下面两个式子,得到$\delta=(\delta_{1},\delta_{2},….,\delta_{k})^T$。推导可参考《改进的迭代尺度法》关于转移特征$t_{k}$的更新方程为$$\begin{split}E_{\hat{P}}(t_{k}){} &amp; =\hat{P}(x,y)\sum_{i=1}^{n+1}t_{k}(y_{i-1},y_{i},x,i)\\ {} &amp; =\sum_{x,y}\hat{P}(x)P(y|x)\sum_{i=1}^{n+1}t_{k}(y_{i-1},y_{i},x,i)exp(\delta_{k}T(x,y)),k=1,2,….,K_{1}\end{split}$$关于状态特征$s_{l}$的更新方程为$$\begin{split}E_{\hat{P}}(s_{l}){} &amp; =\hat{P}(x,y)\sum_{i=1}^{n+1}s_{l}(y_{i},x,i)\\ {} &amp; =\sum_{x,y}\hat{P}(x)P(y|x)\sum_{i=1}^{n}s_{l}(y_{i},x,i)exp(\delta_{K_{1}+l}T(x,y)),k=1,2,….,K_{2}\end{split}$$这里，$T(x,y)$是在数据$(x,y)$中出现所有特征数的总和：$$T(x,y)=\sum_{k}f_{k}(y,x)=\sum_{k=1}^{K}\sum_{i=1}^{n+1}f_{k}(y_{i-1},y_{i},x,i)\quad\quad\quad(1.1)$$ 条件随机场模型学习的改进的迭代尺度法输入：特征函数$t_{1},t_{2},t_{3},….,t_{K_{1}}$,$s_{1},s_{2},…..,s_{K_{2}}$;经验分布$\hat{P}(x,y)$输出：参数估计值$\hat{w}$;模型$P_{\hat{w}}$(1) 对所有$k \in {1,2,…,K}$,取初值$w_{k}=0$(2) 对每一个$k \in {1,2,…,K}$:(a)当$k \in {1,2,…,K_{1}}$时，令$\delta_{k}$是方程$$\sum_{x,y}\hat{P}(x)P(y|x)\sum_{i=1}^{n+1}t_{k}(y_{i-1},y_{i},x,i)exp(\delta_{k}T(x,y))=E_{\hat{P}}[t_{k}]$$的解。当$k=K_{1}+l$，$l=1,2,…,K_{2}$时，令$\delta_{k+1}$是方程$$\sum_{x,y}\hat{P}(x)P(y|x)\sum_{i=1}^{n} s_{l}(y_{i},x,i)exp(\delta_{K_{1}+l}T(x,y))=E_{\hat{P}}[s_{l}]$$的解,式中$T(x,y$是由式子(1.1)给出。 (b)更新$w_{k}$值:$w_{k} \longleftarrow w_{k}+\delta_{k}$(c)如果不是所有的$w_{k}$都收敛，重复步骤(2)在$t_{k}$的更新方程和状态特征$s_{l}$的更新方程中$T(x,y$表示数据$(x,y)$的特征总数，对不同的数据$(x,y)$可能取值不同。为了处理这个问题，定义松弛特征$$s(x,y)=S-\sum_{i=1}^{n+1}\sum_{k=1}^{K}f_{k}(y_{i-1},y_{i},x,i)$$式中S是一个常数。选择足够大的常数S使得对训练数据集的所有数据$(x,y)$,$s(x,y)\geq0$,这时特征总数可取S。由转移特征$t_{k}$的更新方程,对于状态特征$t_{k}$和$delta_{k}$的更新方程是$$\sum_{x,y}\hat{P}(x,y)P(y|x)\sum_{i=1}^{n+1}t_{k}(y_{i-1},y_{i},x,i)exp(\delta_{k}S)=E_{\hat{P}}(t_{k})\\\delta_{k}=\frac{1}{S}log\frac{E_{\hat{P}}(t_{k})}{E_{p}(t_{k})}$$其中，$$E_{p}(t_{k})=\sum_{k}\hat{P}(x)\sum_{i=1}^{n+1}\sum_{y_{i-1},y_{i}}t_{k}(y_{i-1},y_{i},x,i)\frac{\alpha_{i-1}^{T}(y_{i-1}|x)M_{i}(y_{i-1},y_{i}|x)\beta_{i}(y_{i}|x)}{Z(x)}$$同样,由状态特征$s_{l}$的更新方程,对于状态特征$s_{l}$和$delta_{k}$的更新方程是$$\sum_{x,y}\hat{P}(x,y)P(y|x)\sum_{i=1}^{n}s_{l}(y_{i},x,i)exp(\delta_{k}S)=E_{\hat{P}}(s_{l})\\\delta_{k}=\frac{1}{S}log\frac{E_{\hat{P}}(s_{l})}{E_{p}(s_{l})}$$以上算法称为算法S。在算法S中需要使常数S取足够大，这样一来，每步迭代的增量向量会变大，算法收敛会变慢。算法T试图解决这个问题。算法T对每个观测序列x计算其特征总数最大值$T(x)$:$$T(x)=\max_{y}T(x,y)$$利用前向-后向递推公式，可以很容易地计算$T(x)=t$这时，关于转移特征参数的更新方程可以写成：$$\begin{split}E_{\hat{P}}(t_{k}){} &amp; =\sum_{x,y}\hat{P}(x)P(y|x)\sum_{i=1}^{n+1}t_{k}(y_{i-1},y_{i},x,i)exp(\delta_{k}T(x)) \\ {} &amp; =\sum_{x}\hat{P}(x)\sum_{y}P(y|x)\sum_{i=1}^{n+1}t_{k}(y_{i-1},y_{i},x,i)exp(\delta_{k}T(x)) \\ {} &amp; =\sum_{x}\hat{P}(x)a_{k,l}exp(\delta_{k}\cdot t)\\ {} &amp; =\sum_{t=0}^{T_{\max}}a_{k,l}\beta_{k}^{t}\end{split}$$这里,$a_{k,l}$是特征$t_{k}$的期望值，$\delta_{k}=log\beta_{k}$,$\beta_{k}$是多项式方程唯一的实根,可以用牛顿法求得。从而求得相关的$\delta_{k}$。 同样，关于状态特征的参数更新方程可以写成：$$\begin{split}E_{\hat{P}}(s_{l}){} &amp; =\sum_{x,y}\hat{P}(x)P(y|x)\sum_{i=1}^{n}s_{l}(y_{i},x,i)exp(\delta_{K_{1}+1}T(x)) \\ {} &amp; =\sum_{x}\hat{P}(x)\sum_{y}P(y|x)\sum_{i=1}^{n}s_{l}(y_{i},x,i)exp(\delta_{K_{1}+1}T(x)) \\ {} &amp; =\sum_{x}\hat{P}(x)b_{k,l}exp(\delta_{l}\cdot t)\\ {} &amp; =\sum_{t=0}^{T_{\max}}b_{k,l}\gamma_{l}^{i}\end{split}$$这里,$b_{k,l}$是特征$s_{l}$的期望值，$\delta_{l}=log\gamma_{l}$,$\gamma_{l}$是多项式方程唯一的实根,可以用牛顿法求得。从而求得相关的$\delta_{k}$。 1.2、拟牛顿法条件随机场模型学习还可以应用牛顿法或拟牛顿法。对于条件随机场模型$$P_{w}(y|x)=\frac{exp\Bigg(\sum_{i=1}^{n}w_{i}f_{i}(x,y)\Bigg)}{\sum_{y}exp\Bigg(\sum_{i=1}^{n}w_{i}f_{i}(x,y)\Bigg)}$$学习的优化目标函数是$$\min_{w \in R^n}f(w)=\sum_{x}\hat{P}(x)log\sum_{y}exp\Bigg(\sum_{i=1}^{n}w_{i}f_{i}(x,y)\Bigg)-\sum_{x,y}\hat{P}(x,y)\sum_{i=1}^{n}w_{i}f_{i}(x,y)$$其梯度函数是$$g(w)=\sum_{x,y}\hat{P}(x)P_{w}(y|x)f(x,y)-E_{\hat{P}}(f)$$拟牛顿法的BFGS算法如下。 算法(条件随机场模型学习的BFGS算法）输入：特征函数$f_{1},f_{2},….,f_{n}$;经验分布$\hat(P)(x,y)$;输出：最优参数值$\hat(w)$;最优模型$P_{\hat{w}}(y|x)$(1)选定初始点$w^{(0)}$,取$B_{0}$为为正定对称矩阵，置$k=0$(2)计算$g_{k}=g(w^{(k)})$,若$g_{k}=0$则停止计算；否则转(3)(3)由$B_{k}p_{k}=-g_{k}$求出$p_{k}$(4)一维搜索：求$\lambda_{k}$使得$$f(w^{(k)}+\lambda_{k}p_{k})=\min_{\lambda\geq0}f(w^{(k)}+\lambda p_{k})$$(5)置$w^{(k+1)}=w^{(k)}+\lambda_{k}p_{k}$(6)计算$g_{k+1}=g(w^{(k+1)})$，若$g_{k}=0$则停止计算；否则，按下式求出$B_{k+1}$:$$B_{k+1}=B_{k}+\frac{y_{k}y_{k}^{T}}{y_{k}^{T}\delta_{k}}-\frac{B_{k}\delta_{k}\delta_{k}^{T}B_{k}}{\delta_{k}^{T}B_{k}\delta_{k}}$$其中，$$y_{k}=g_{k+1}-g_{k},\delta_{k}=w^{(k+1)}-w^{(k)}$$(7)置$k=k+1$，转(3) 二、条件随机场的预测问题条件随机场的预测问题是给定条件随机场$P(Y|X)$和输入序列（观测序列）x,求条件概率最大的输出序列(标记序列)$y^{*}$即对观测序列进行标注。条件随机场的预测算法是著名的维特比算法。由式$P_{w}(y|x)=\frac{exp(w\cdot F(y,x))}{Z_{w}(x)}$可得:$$\begin{split}y^{*} {} &amp; =arg \max_{y}P_{w}(y|x)\\ {} &amp; =arg \max_{y}\frac{exp(w\cdot F(y,x))}{Z_{w}(x)}\\ {} &amp; =arg \max_{y}exp(w\cdot F(y,x))\\ {} &amp; =arg \max_{y}(w\cdot F(y,x))\end{split}$$于是，条件随机场的预测问题成为求非规范化概率最大的最优路径问题$$max_{y}(w\cdot F(y,x))$$这里，路径表示标记序列。其中，$$\begin{split}{} &amp; w=(w_{1},w_{2},….,w_{K})^{T} \\{} &amp; F(y,x)=(f_{1}(y,x),f_{2}(y,x),…..,f_{K}(y,x))^{T}\\{} &amp; f_{k}(y,x)=\sum_{i=1}^{n}f_{k}(y_{i-1},y_{i},x,i),k=1,2,….,K\end{split}$$注意，这时只需计算非规范化概率，而不必计算概率，可以大大提高效率。为了求解最优路径，将$\max_{y}(w\cdot F(y,x))$写成如下形式：$$\max_{y} \sum_{i=1}^{n}(w\cdot F_{i}(y_{i-1},y_{i},x))$$其中，$$F_{i}(y_{i-1},y_{i},x))=(f_{1}(y_{i-1},y_{i},x),f_{2}(y_{i-1},y_{i},x),…..,f_{K}(y_{i-1},y_{i},x))$$是局部特征向量。 下面叙述维特比算法。首先求出位置1的各个标记j=1,2,…，m的非规范化概率:$$\delta_l(j)=w\cdot F_{l}(y_{0}=start,y_{1}=j,x), j=1,2,…,m$$一般地，由递推公式，求出到位置i的各个标记$l=1,2,…,m$的非规范化概率的最大值，同时记录非规范化概率最大值的路径$$\begin{split}{} &amp; \delta_{i}(l)=\max_{1\leq j \leq m}{\delta_{i-1}(j)+w\cdot F_{i}(y_{i-1}=j,y_{i}=l,x)},l=1,2,….,m\\{} &amp; \psi_{i}(l)=arg \max_{1\leq j \leq m}{\delta_{i-1}(j)+w\cdot F_{i}(y_{i-1}=j,y_{i}=l,x)},l=1,2,….,m\end{split}$$直到$i=n$时终止。这时求得非规范化概率的最大值为$$max_{y}(w\cdot F(y,x))=\max_{1\leq j \leq m}\delta_{n}(j)$$及最优路径的终点$$y_{n}^{*}=arg \max_{1\leq j \leq m}\delta_{n}(j)$$由此最优路径终点返回,$$y_{i}^{*}=\psi_{i+1}(y_{i+1}^{*}),i=n-1,n-2,…,1$$求得最优路径$y^{*}=(y_{1}^{*},y_{1}^{*},y_{2}^{*}…,y_{n}^{*})^{T}$ 综上所述，得到条件随机场预测的维特比算法:输入：模型特征向量$F(y,x)$和权值向量$w$，观测序列$x=(x_{1},x_{2},…,x_{n})$输出：最优路径$y^{*}=(y_{1}^{*},y_{1}^{*},y_{2}^{*}…,y_{n}^{*})^{T}$(1) 初始化：$$\delta_l(j)=w\cdot F_{l}(y_{0}=start,y_{1}=j,x), j=1,2,…,m$$(2) 递推，对$i=2,3,…,n$$$\begin{split}{} &amp; \delta_{i}(l)=\max_{1\leq j \leq m}{\delta_{i-1}(j)+w\cdot F_{i}(y_{i-1}=j,y_{i}=l,x)},l=1,2,….,m\\{} &amp; \psi_{i}(l)=arg \max_{1\leq j \leq m}{\delta_{i-1}(j)+w\cdot F_{i}(y_{i-1}=j,y_{i}=l,x)},l=1,2,….,m\end{split}$$(3)终止$$max_{y}(w\cdot F(y,x))=\max_{1\leq j \leq m}\delta_{n}(j)y_{n}^{*}=arg \max_{1\leq j \leq m}\delta_{n}(j)$$4)返回路径$$y_{i}^{*}=\psi_{i+1}(y_{i+1}^{*}),i=n-1,n-2,…,1$$求得最优路径$y^{*}=(y_{1}^{*},y_{1}^{*},y_{2}^{*}…,y_{n}^{*})^{T}$ 三、总结与HMM区别 HMM是生成时模型，用到EM算法。CRF是判别式模型，一般方法是最大似然估计，结合梯度下降 HMM是假定满足HMM独立假设。CRF没有，所以CRF能容纳更多上下文信息。 CRF计算的是全局最优解，不是局部最优值。 CRF是给定观察序列的条件下，计算整个标记序列的联合概率。而HMM是给定当前状态，计算下一个状态。 CRF比较依赖特征的选择和特征函数的格式，并且训练计算量大四、参考 《统计学习方法》,李航 条件随机场(码农场) 果壳中的条件随机场(CRF In A Nutshell)]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>随机条件场</tag>
        <tag>序列标注</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件随机场之基本概念与模型]]></title>
    <url>%2FNLP%2F%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E4%B9%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[一、简介条件随机场模型是Lafferty等人在2001年在最大熵模型和隐马尔可夫模型的基础上提出的一种无向图模型，是一种基于标注和切分有序数据的条件概率模型。CRF最早是针对序列数据分析提出的，现已成功应用于自然语言处理、生物信息学、机器视觉及网络智能等领域。目前基于CRF的实现有CRF,FlexCRF,CRF++。 二、预备知识条件随机场比较复杂，因此也涉及到很多前置的预备知识:概率图模型、马尔可夫性、团与最大团等。 2.1、概率图模型(PGM)图是由结点和连接结点的边组成的集合。结点和边分别记作$v$和$e$，结点和边的集合分别记作$V$和$E$，图记作$G=(V，E)$，无向图是指边没有方向的图。概率图模型是一类用图的形式表示随机变量之间条件依赖关系的概率模型，是概率论与图论的结合。根据图中边有无方向，常用的概率图模型分为两类：有向图（贝叶斯网络、信念网络）、无向图（马尔可夫随机场、马尔可夫网络）。(1)有向图有向图的联合概率如下：$$P(X_{1},X_{2},……,X_{n})=\prod_{i=1}^{N}P(X_{i}|\pi(X_{i}))$$其中$\pi(X_{i})$是$X_{i}$的父节点如上图所示则有：$$P(X_{1},X_{2},X_{3},….,X_{5})=P(X_{1})P(X_{2}|X_{1})P(X_{3}|X_{2})P(X_{4}|X_{2})P(X_{5}|X_{3},X_{4})$$ (2)概率无向图模型设有联合概率分布$P(Y)$，由无向图$G=(V,E)$表示，在图G中，结点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率分布$P(Y)$满足成对、局部或全局马尔可夫性，就称此联合概率分布为概率无向图模型或马尔可夫随机场。尽管在给定每个节点的条件下，分配给该节点一个条件概率是可能的，无向图的无向性导致我们不能用条件概率参数化表示联合概率，而要从一组条件独立的原则中找出一系列局部函数的乘积来表示联合概率。 2.2、马尔可夫性2.2.1、成对马尔可夫性成对马尔可夫性:设$u$和$v$是无向图$G$中任意两个没有边链接的节点，节点$u$和$v$分别对应随机变量$Y_{u}$和$Y_{v}$,其他所有节点为$O$对应的随机变量组是$Y_{O}$。成对马尔可夫性是指在给定随机变量组$Y_{O}$的条件下随机变量$Y_{u}$和$Y_{v}$是条件独立的，即：$$P(Y_{u},Y_{v}|Y_{O})=P(Y_{u}|Y_{O})P(Y_{v}|Y_{O})$$ 2.2.2、局部马尔可夫性局部马尔可夫性：设$v\in V$是无向图$G$中任意一个结点，$W$是与$v$有边连接的所有结点，$O$是$v$、$W$以外的其他所有结点。$v$表示的随机变量是$Y_{v}$，$W$表示的随机变量组是$Y_{W}$，$O$表示的随机变量组是$Y_{O}$。如下图所示：局部马尔可夫性是指在给定随机变量组$Y_{W}$的条件下随机变量$Y_{v}$与随机变量$Y_{O}$是独立的，即：$$P(Y_{v},Y_{O}|Y_{W})=P(Y_{v}|Y_{W})P(Y_{O}|Y_{W})$$设在$P(Y_{O}|Y_{W})&gt;0$时等价的：$$P(Y_{v}|Y_{W})=P(Y_{v}|Y_{W},Y_{O})$$ 2.2.3、全局马尔可夫性全局马尔可夫性：设结点集合A,B是在无向图G中被结点集合C分开的任意结点集合，如下图所示。结点结合A,B和C所对应的随机变量组分别是$Y_{A}$,$Y_{B}$和$Y_{C}$。全局马尔可夫性是指给定随机变量组$Y_{C}$条件下随机变量组$Y_{A}$和$Y_{B}$是条件独立的，即$$P(Y_{A},Y_{B}|Y_{C})=P(Y_{A}|Y_{C})P(Y_{B}|Y_{C})$$上述成对、局部、全局马尔可夫性的定义是等价的。 2.3、团与最大团无向图G中任何两个结点均有边连接的结点子集成为团(clique)。若C是无向图G中的一个团，并且不能在加进任何一个G的结点使其成为一个更大的团，则成为此C为最大团(maximal clique).如下图所示的4个结点组成的无向图。途中由两个结点组成的团有5个：{$Y_{1},Y_{2}$},{$Y_{2},Y_{3}$},{$Y_{3},Y_{4}$},{$Y_{4},Y_{2}$}和{$Y_{1},Y_{3}$}.有两个最大团：{$Y_{1},Y_{2},Y_{3}$},{$Y_{2},Y_{3},Y_{4}$},而{$Y_{1},Y_{2},Y_{3},Y_{4}$}不是一个团，因为$Y_{1}$和$Y_{4}$没有边连接。将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积形式的操作，成为概率无向图的模型因子分解。给定概率无向图模型，设其无向图为G，C为G上的最大团，$Y_{C}$表示C对应的随机变量。那么概率无向图模型的联合概率P(Y)可写作所有最大团C上的函数$\Psi_{C}(Y_{C})$的乘积形式，即：$$P(Y)=\frac{1}{Z}\prod_{C}\Psi_{C}(Y_{C})$$其中，Z是规范化因子由下列式子给出：$$Z=\sum_{Y}\prod_{C}\Psi_{C}(Y_{C})$$规范化因子保证P(Y)构成一个概率分布。函数$\Psi_{C}(Y_{C})$称为势函数。这里要求势函数$\Psi_{C}(Y_{C})$是严格正的，通常定义为指函数:$$\Psi_{C}(Y_{C})=exp(-E(Y_{C}))$$概率无向图模型的因子分解由Hammersley-Cliford定理保证。 Hammersley-Cliford定理：概率无向图模型的联合概率分布P(Y)可以表示为如下形式：$$P(Y)=\frac{1}{Z}\prod_{C}\Psi_{C}(Y_{C})Z=\sum_{Y}\prod_{C}\Psi_{C}(Y_{C})$$其中，C是无向图的最大团，$Y_{C}$是C对应的结点对应的随机变量，$\Psi_{C}(Y_{C})$是C上定义的严格正函数，乘积是在无向图所有的最大团上进行的。 三、条件随机场3.1、定义设$G=(V,E)$是一个无向图，$Y={Y_{v}|v\in V}$是以G中节点为索引的随机变量$Y_{v}$构成的集合。在给定X的条件下，如果每个随机变量$Y_{v}$服从马尔可夫属性即$P(Y_{v}|X,Y_{u},u \neq v)=P(Y_{v}|X,Y_{u},u\sim v)$其中$u\sim v$表示u和v是相邻的边，则$(X,Y)$就构成一个随机条件场。 CRFs是在给定需要标定的观测序列的条件下，计算整个观测序列的联合概率，即求条件分布$P(S|O)$，而不是在给定当前的状态条件下，定义下一个状态的分布(HMM),即求联合分布P(S,O)。 3.2、线性CRFs模型(Linear-chain CRFs)3.2.1、参数化形式令$x={x_1,x_2,x_3,….,x_n}$为观测序列$y={y_1,y_2,y_3,….,y_n}$为有限状态集合根据随机场的基本理论：$$P(y|x,\lambda) \propto exp\Bigg(\sum_{i,j}\lambda_{j}t_{j}(y_{i-1},y_{i},x,i)+\sum_{i,k}\mu_{k}s_{k}(y_{i},x,i)\Bigg)$$ $t_{j}(y_{i-1},y_{i},x,i)$:对于观测序列的标记位置i-1与i之间的特征转移函数 $s_{k}(y_{i},x,i)$:观测序列的i位置的状态特征函数 $\lambda_{j}$和$\mu_{k}$是权值 $Z(x)$是规范因子 通常$t_{j}$和$s_{k}$取值为0和1。当满足特征条件的时候取值为1，否则为0。条件随机场完全由特征函数$t_{j}$、$s_{k}$和对应的权值$\lambda_{j}$、$\mu_{k}$决定。 上式子表示在给定输入序列$x$，对应输出序列$y$预测的条件概率 例子： 3.2.2、简化形式为简便起见，首先将特征转移函数和状态特征函数及其对应的权值用统一的符号表示，设有$N_{1}$个转移特征，$N_{2}$个状态特征，$N=N_{1}+N_{2}$,记$$f_{n}(y_{i-1},y_{i},x,i)=\begin{cases}t_{j}(y_{i-1},y_{i},x,i),n=1,2,…,N_{1}\\s_{k}(y_{i},x,i), n=N_{1}+k,k=1,2,,….,N_{2}\end{cases}$$然后，对特征转移和状态特征的各个位置的$i$求和，记作:$$f_{n}(y,x)=\sum_{i=1}^{N}f_{n}(y_{i-1},y_{i},x,i)$$用$w_{n}$表示特征$f_{n}(y,x)$的权值，即$$w_{n}=\begin{cases}\lambda_{j},n=1,2,…,N_{1}\\\mu_{k}, n=N_{1}+k,k=1,2,,….,N_{2}\end{cases}$$于是随机条件场可以表示为:$$P(y|x)=\frac{1}{Z(x)}exp\sum_{n=1}^{N}w_{n}f_{n}(y,x) \\Z(x)=\sum_{y}exp\sum_{n=1}^{N}w_{n}f_{n}(y,x)$$若以权值$w$表示权值向量，即$$w=(w_{1},w_{2},w_{3},….,w_{N})^T$$以$F(y,x)$表示全局特征向量，即：$$F(y,x)=(f_{1}(y,x),f_{2}(y,x),…..,f_{N}(y,x))^T$$则随机条件场可以写成向量$w$与$F(y,x)$的内积形式：$$P_{w}(y|x)=\frac{exp(w\cdot F(y,x))}{Z_{w}(x)}$$其中$$Z_{w}(x)=\sum_{y}exp(w\cdot F(y,x))$$ 四、条件随机场的概率计算问题条件随机场的概率计算问题是指在给定条件随机场P(Y|X)，输入序列$x$和输出序列$y$，计算条件概率$P(Y_{t}=y_{i}|x)$,$P(Y_{i-1},Y_{i}=y_{i}|x)$以及相对应的数学期望问题。为了方便起见，像隐马尔可夫模型那样，引进前向-后向向量，递归地计算以上概率及期望值。这样的算法称为前向-后向算法 4.1、前向-后向算法对每个指标$i=0,1,2….,n+1$，定义前向向量$\alpha_{i}(x)$：$$\alpha_{0}(y|x)=\begin{cases}1,y=start \\0,否则\end{cases}$$递推公式为$$\alpha_{i}^{T}(y_{i}|x)=\alpha_{i-1}^{T}(y_{i-1}|x)M_{i}(y_{i-1},y_{i}|x),i=1,2,…,n+1$$又可以表示为：$$\alpha_{i}^{T}(x)=\alpha_{i-1}^{T}(x)M_{i}(x)$$ $\alpha_{i}(x)$表示在位置i的标记是$y_{i}$并且到位置i的前部分标记序列的非规范化概率，$y_{i}$可取的值有m个，所以$\alpha_{i}(x)$是m维的列向量。同样，对每个指标$i=0,1,2….,n+1$定义后向向量$\beta_{i}(x)$:$$\beta_{n+1}(y_{n+1}|x)=\begin{cases}1,y_{n+1}=stop \\0,否则\end{cases} \\\beta_{i}(y_{i}|x)=M_{i}(y_{i},y_{i+1}|x)\beta_{i-1}(y_{i+1}|x)$$又可以表示为$$\beta_{i}(x)=M_{i+1}(x)\beta_{i+1}(x)$$$\beta_{i}(y_{i}|x)$表示在位置i的标记为$y_{i}$,并且从i+1到n的后部分标记序列的非规范化概率。由前向-后向向量定义不难得到：$$Z(x)=\alpha_{n}^{T}\cdot1=1^T\cdot\beta_{1}(x)$$这里$1$是元素均为1的m维列向量。 4.2、概率计算按照前向-后向向量的定义，很容易计算标记序列在位置i是$y_{i}$的条件概率和在位置i-1与i是标记$y_{i-1}$和$y_{i}$的条件概率：$$P(Y_{i}=y_{i}|x)=\frac{\alpha_{i}^{T}(y_{i}|x)\beta_{i}(y_{i}|x)}{Z(x)} \\P(Y_{i-1}=y_{i-1},Y_{i}=y_{i}|x)=\frac{\alpha_{i-1}^{T}(y_{i-1}|x)M_{i}(y_{i-1},y_{i}|x)\beta_{i}(y_{i}|x)}{Z(x)}$$其中$$Z(x)=\alpha_{n}^{T}\cdot1=1^T\cdot\beta_{1}(x)$$ 4.3、期望值的计算利用前向-后向向量，可以计算特征函数关于联合分布$P(X,Y)$和条件分布$P(Y|X)$的数学期望。特征函数$f_{k}$关于条件分布$F(Y|X)$的数学期望是$$\begin{multline}E_{P(Y|X)}[f_{k}]=\sum_{k}P(y|x)f_{k}(y,x)\\ =\sum_{i=1}^{n+1}\sum_{y_{i-1},y_{i}}f_{k}(y_{i-1},y_{i},x,i)\frac{\alpha_{i-1}^{T}(y_{i-1}|x)M_{i}(y_{i-1},y_{i}|x)\beta_{i}(y_{i}|x)}{Z(x)},k=1,2,3,….,K\end{multline}$$ 上文中的$f_{n}$变成$f_{k}$，且N=K 其中$$Z(x)=\alpha_{n}^{T}\cdot1=1^T\cdot\beta_{1}(x)$$ 假设经验分布$\hat{P}(x)$，特征函数$f_{k}$关于联合分布$F(X,Y)$的数学期望是$$\begin{split}E_{P(X,Y)}[f_{k}]{} &amp; =\sum_{x,y}P(X,Y)\sum_{i=1}^{n+1}f_{k}(y_{i-1},y_{i},x,i)\\ {} &amp; =\sum_{x}\hat{P}(x)\sum_{y}P(y|x)\sum_{i=1}^{n+1}f_{k}(y_{i-1},y_{i},x,i)\\ {} &amp; =\sum_{x}\hat{P}(x)\sum_{i=1}^{n+1}\sum_{y_{i-1},y_{i}}f_{k}(y_{i-1},y_{i},x,i)\frac{\alpha_{i-1}^{T}(y_{i-1}|x)M_{i}(y_{i-1},y_{i}|x)\beta_{i}(y_{i}|x)}{Z(x)},k=1,2,3,….,K\end{split}$$其中$$Z(x)=\alpha_{n}^{T}\cdot1=1^T\cdot\beta_{1}(x)$$这个式子是特征函数数学期望的一般计算公式。对于转移特征$t_{k}(y_{i-1},y_{i},x,i)$,$k=1,2,3,…,K_{1}$,可以将式子中的$f_{k}$换成$t_{k}$;对于状态特征，可以将式中的$f_{k}$换成$s_{l}$,表示为$s_{l}(y_{i},x,i)$,k=K1+l,$l=1,2,3,…,K_{2}$。有了这些式子，对于给定的观测序列x与标记序列y，可以通过一次前向扫描计算$\alpha_{i}$及$Z(x)$,通过一次后向扫描计算$\alpha_{i}$及$Z(x)$，通过一次后向扫描计算$\beta_{i}$。从而计算所有的概率和特征的期望 五、总结简单理解，条件随机场是给定随机变量X条件下，随机变量Y的马尔可夫随机场,在条件概率模型中$P(Y|X)$中，Y是输出变量，表示标记序列，X是输入变量，表示观测序列。训练时候利用训练数据库，通过极大似然估计得到条件概率模型，然后使用该模型预测。同时CRF是一个无向图的概率模型，顶点代表变量，顶点之间的边代表两个变量之间依赖关系。常用的是链式CRF结构，如此可以表达长距离依赖性，和交叠特征的能力。所有特征可以进行全局归一化，得到全局最优解。（因为同一特征在各个位置都有表示，可以将同一个特征在不同位置求和，将局部特征转化为全局特征，从而得到全局最优解。）上面所述内容已经将条件随机场的基本知识点、模型以及概率计算问题解决了。学习和预测问题将在下一篇博文条件随机场之学习和预测问题中解决。 六、参考 《统计学习方法》,李航 条件随机场(码农场) 果壳中的条件随机场(CRF In A Nutshell)]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>随机条件场</tag>
        <tag>序列标注</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马尔可夫模型-HMM]]></title>
    <url>%2FNLP%2F%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B-HMM%2F</url>
    <content type="text"><![CDATA[1、简介隐马尔可夫(Hidden Markov model)模型是一类基于概率统计的模型，是一种结构最简单的动态贝叶斯网，是一种重要的有向图模型。自上世纪80年代发展起来，在时序数据建模，例如：语音识别、文字识别、自然语言处理等领域广泛应用。隐马尔可夫模型涉及的变量、参数众多，应用也很广泛。 2、隐马尔可夫模型的基本概念2.1、隐马尔可夫模型的定义隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为状态序列（state sequence);每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（observation sequence)。序列的每一个位置又可以看作是一个时刻。 什么叫隐马尔可夫链呢？就是每个状态的转移只依赖于之前的n个状态，这个过程被称为1个n阶的模型，其中n是影响转移状态的数目。最简单的马尔可夫过程就是一阶过程，每一个状态的转移只依赖于其之前的那一个状态。用数学表达式表示就是：$$P(X_{n+1}=x|X_{1}=x_{1},X_{2}=x_{2},X_{3}=x_{3},….X_{n}=x_{n})=P(X_{n+1}=x|X_{n}=x_{n})$$隐马尔可夫模型由初始概率分布、状态转移概率分布、以及观测概率分布确定，隐马尔可夫模型的定义如下：设Q是所有可能的状态的集合，V是所有可能的观测的集合。$$Q={q_1,q_2,q_3,…..,q_N}, V={v_1,v_2,v_3,….,v_M}$$其中N是可能的状态数，M为可能的观测数。状态q是不可见的，观测v是可见的。应用到词性标注系统，词就是v，词性就是q。I是长度为T的状态序列，O是对应的观测序列。$$I=(i_1,i_2,…,i_T),O=(o_1,o_2,…,o_T)$$ 这可以想象为相当于给定了一个词（O）+词性（I）的训练集，于是我们手上有了一个可以用隐马尔可夫模型解决的实际问题。 定义A为状态转移概率矩阵：$$A=\Big[a_{ij}\Big]_{N \times N}$$ 其中，$$a_{tj}=P(i_{t+1}=q_j|i_{t}=q_i);i=1,2,…,N;j=1,2,…,N$$是在时刻t处于状态$q_i$的条件下在时刻t+1转移到状态$q_j$的概率。 这部分阐述的是马尔可夫的一阶过程，每一个状态的转移只依赖于其之前的那一个状态 B是观测概率矩阵:$$B=\Big[b_j(k)\Big]_{N \times M}$$其中，$$b_j(k)=P(o_t=v_k|i_{t}=q_j),k=1,2,….,M;j=1,2,…,N$$是在时刻t处于状态$q_j$的条件下生成观测$v_k$ 这实际上在作另一个假设，观测是由当前时刻的状态决定的，跟其他因素无关. $\pi$是初始状态概率向量：$$\pi=(\pi_{i})$$ 其中，$$\pi_{i}=P(i_1=q_i),i=1,2,…,N$$是时刻t=1处于状态$q_j$的概率。 隐马尔可夫模型由初始状态概率向量$\pi$、状态转移概率矩阵A和观测概率矩阵B决定。$\pi$和A决定状态序列，B决定观测序列。因此，隐马尔可夫模型$\lambda$可以用三元符号表示，即$$\lambda=(A,B,\pi)$$$(A,B,\pi)$称为隐马尔可夫模型的三要素。如果加上一个具体的状态集合Q和观测序列V，则构成了HMM的五元组。 状态转移概率矩阵A与初始状态概率向量$\pi$确定了隐藏的马尔可夫链，生成不可观测的状态序列。观测概率矩阵B确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。 从定义可知，隐马尔可夫模型作了两个基本假设：(1)齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关。$$p(i_t|i_{t-1},o_{t-1},….,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,…,T$$(2)观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。$$P(o_t|i_T,o_T,i_{T-1},o_{T-1},….,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},…,i_1,o_1)=p(o_t|i_t)$$ 2.2、定义的实例说明你本是一个城乡结合部修电脑做网站的小码农，突然春风吹来全民创业。于是跟村头诊所的老王响应总理号召合伙创业去了，有什么好的创业点子呢？对了，现在不是很流行什么“大数据”么，那就搞个“医疗大数据”吧，虽然只是个乡镇诊所……但管它呢，投资人就好这口。 数据从哪儿来呢？你把老王，哦不，是王老板的出诊记录都翻了一遍，发现从这些记录来看，村子里的人只有两种病情：要么健康，要么发烧。但村民不确定自己到底是哪种状态，只能回答你感觉正常、头晕或冷。有位村民是诊所的常客，他的病历卡上完整地记录了这三天的身体特征(正常、头晕或冷)，他想利用王老板的“医疗大数据”得出这三天的诊断结果(健康或发烧)。 这时候王老板掐指一算，说其实感冒这种病，只跟病人前一天的病情有关，并且当天的病情决定当天的身体感觉。 于是你一拍大腿，天助我也，隐马尔可夫模型的两个基本假设都满足了，于是统计了一下病历卡上的数据，撸了这么一串Python代码：123456789101112131415states = ('Healthy', 'Fever')observations = ('normal', 'cold', 'dizzy')start_probability = &#123;'Healthy': 0.6, 'Fever': 0.4&#125;transition_probability = &#123; 'Healthy': &#123;'Healthy': 0.7, 'Fever': 0.3&#125;, 'Fever': &#123;'Healthy': 0.4, 'Fever': 0.6&#125;&#125;emission_probability = &#123; 'Healthy': &#123;'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1&#125;, 'Fever': &#123;'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6&#125;&#125; states代表病情，observations表示最近三天观察到的身体感受，start_probability代表病情的分布，transition_probability是病情到病情的转移概率，emission_probability则是病情表现出身体状况的发射概率。以上列出的就是隐马的五元组。 3、隐马尔可夫模型的3个基本问題隐马尔可夫模型有3个基本问题： 概率计算问题。给定模型$(A,B,\pi)$观测序列$O=(o_1,o_2,….,o_T)$,计算在模型$\lambda$下观测序列O出现的概率$P(O|\lambda)$。 学习问题。己知观测序列$O=(o_1,o_2,….,o_T)$，估计模型$(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大。即用极大似然估计的方法估计参数。 预测问题，也称为解码（decoding)问题。已知模型$(A,B,\pi)$观测序列$O=(o_1,o_2,….,o_T)$求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,,,,,i_T)$。即给定观测序列，求最有可能的对应的状态序列。 下面各节将逐一介绍这些基本问题的解法。 3.1、概率计算算法计算观测序列概率有前向（forward)与后向（backward)算法，以及概念上可行但计算上不可行的直接计算法（枚举）。前向后向算法无非就是求第一个状态的前向概率或最后一个状态的后向概率，然后向后或向前递推即可。 3.1.1、直接计算法给定模型，求给定长度为T的观测序列的概率，直接计算法的思路是枚举所有的长度T的状态序列，计算该状态序列与观测序列的联合概率（隐状态发射到观测），对所有的枚举项求和即可。在状态种类为N的情况下，一共有$N^T$种排列组合，每种组合计算联合概率的计算量为T，总的复杂度为$O(TN^T)$，并不可取。 3.1.2、前向算法定义(前向概率） 给定隐马尔可夫模型$\lambda$，定义到时刻t为止的观测序列为$o_1,o_2,…,o_t$且状态为$q_i$的概率为前向概率，记作$$\alpha_t(i)=P(o_1,o_2,…,o_t,i_t=q_i|\lambda)$$可以递推地求得前向概率$\alpha_t(i)$及观测序列概率$P(O|\lambda)$。 算法(观测序列概率的前向算法）输入：隐马尔可夫模型$\lambda$，观测序列O；输出：观测序列概率$P(O|\lambda)$ (1)初值$$\alpha_1(i)=\pi_{i}b_{i}(o_1),i=1,2,….,N$$前向概率的定义中一共限定了两个条件，一是到当前为止的观测序列，另一个是当前的状态。所以初值的计算也有两项（观测和状态），一项是初始状态概率，另一项是发射到当前观测的概率。 (2)递推对$t=1,2,….,T-1$$$\alpha_{t+1}(i)=\Big[\sum_{j=1}^{N}\alpha_t(j)a_{ji}\Big]b_i(o_{t+1})$$每次递推同样由两部分构成，大括号中是当前状态为i且观测序列的前t个符合要求的概率，括号外的是状态i发射观测t+1的概率。 (3)终止$$P(O|\lambda)=\sum_{i=1}^{N}\alpha_T(i)$$由于到了时间T，一共有N种状态发射了最后那个观测，所以最终的结果要将这些概率加起来。由于每次递推都是在前一次的基础上进行的，所以降低了复杂度。准确来说，其计算过程如下图所示: 下方标号表示时间节点，每个时间点都有N种状态，所以相邻两个时间之间的递推消耗N^2次计算。而每次递推都是在前一次的基础上做的，所以只需累加O(T)次，所以总体复杂度是O(T)个$N^2$，即$O(N^{2}T)$。 前向算法Python实现123456789101112def _forward(self, obs_seq): N = self.A.shape[0] T = len(obs_seq) F = np.zeros((N,T)) F[:,0] = self.pi * self.B[:, obs_seq[0]] for t in range(1, T): for n in range(N): F[n,t] = np.dot(F[:,t-1], (self.A[:,n])) * self.B[n, obs_seq[t]] return F 代码和伪码的对应关系还是很清晰的，F对应alpha，HMM的三个参数与伪码一致。 3.1.3、后向算法定义(后向概率） 给定隐马尔可夫模型$\lambda$,定义在时刻t状态为$q_i$的条件下，从t+1到T的部分观测序列为$o_{t+1},o_{t+2},….,o_{T}$的概率为后向概率，记作$$\beta_{t}(i)=P(o_{t+1},o_{t+2},….,o_{T}|i_{T}=q_{i},\lambda)$$可以用递推的方法求得后向概率$\beta_t(i)$及观测序列概率$P(O|\lambda)$。 算法(观测序列概率的后向算法） 输入：隐马尔可夫模型$\lambda$，观测序列O；输出：观测序列概率$P(O|\lambda)$ (1)初值$$\beta_T(i)=1,i=1,2,….,N$$根据定义，从T+1到T的部分观测序列其实不存在，所以硬性规定这个值是1。 (2)对$t=T-1,T-2,….,1$$$beta_t(i)=\sum_{j=1}^{N}a_{ij}b_j(a_{t+1})\beta_{t+1}(j),i=1,2,…,N$$$a_{ij}$表示状态i转移到j的概率,$b_j$表示发射$O_{t+1}$,$\beta_{t+1}(j)$表示j后面的序列对应的后向概率。 (3)$$P(O|\lambda)=\sum_{i=1}^{N}\pi_{i}b_i(o_1)\beta_1(i)$$最后的求和是因为，在第一个时间点上有N种后向概率都能输出从2到T的观测序列，所以乘上输出O1的概率后求和得到最终结果。其计算过程如下图所示: 后向算法的Python实现123456789101112def _backward(self, obs_seq): N = self.A.shape[0] T = len(obs_seq) X = np.zeros((N,T)) X[:,-1:] = 1 for t in reversed(range(T-1)): for n in range(N): X[n,t] = np.sum(X[:,t+1] * self.A[n,:] * self.B[:, obs_seq[t+1]]) return X 3.2、学习算法隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还是只有观测序列，可以分别由监督学习与非监督学习实现，下面主要介绍监督学习算法，而非监督学习算法–Baum-Weich算法(EM算法学习HMM参数的算法)因为无监督学习并不靠谱,而且也比较复杂，所以就不介绍了。 监督学习方法 转移概率$a_{ij}$的估计设样本中时刻t处于状态i时刻t+1转移到状态j的频数为$A_{ij}$,那么状态转移概率$a_{ij}$的估计是$$\hat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}}$$很简单的最大似然估计。 拿分词来举例吧，如果标注集合为B、M、E、S四个，那么假设$a_{ij}$是状态B转到E的概率，则有$A_{ij}$为在状态为B$\rightarrow$状态E的样本数，$\sum_{j=1}^{N}A_{ij}$则为在状态为B的情况下转移到B、M、E、S的样本数之和。此时为:$$\hat{a}_{B \rightarrow E}=\frac{count(B \rightarrow E)}{count(B \rightarrow B)+count(B \rightarrow M)+count(B \rightarrow E)+count(B \rightarrow S)}$$ 观测概率的估计设样本中状态为j并观测为k的频数是$B_{jk}$那么状态为j观测为k的概率的估计是$$\hat{b}_j(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}},j=1,2,…,N;k=1,2,…,M$$ 同样拿分词来说，假定给定训练语料“我/S 来/B 到/E 北/B 京/E 大/B 学/M 城/E”，则有如下：状态集合：B、M、E、S观测集合(字的集合)：我、来、到、北、京、大、学、城假定$b_j(k)$为状态B $\rightarrow$ 观测“北”的概率，状态B的情况下的观测值有：来、北、大;则有:$$\hat{b}_B(北)=\frac{count(B \rightarrow 北)}{count(B \rightarrow 来)+count(B \rightarrow 北)+count(B \rightarrow 大)}$$ 初始状态概率$\pi_i$的估计$\hat{\pi_i}$为S个样本中初始状态为$q_i$的频率。 同样还是拿分词来举例，假定给以如下的训练语料：我/S 来/B 到/E 北/B 京/E 大/B 学/M 城/E吉/B 林/M 省/E 长/B 春/M 市/E 长/B 春/E 药B/ 店/E则初始状态的分布为{B:0.5,M:0,E:0,S:0.5}，其实就是计算每个句子的开头第一个字的状态的概率。 由于监督学习需要使用训练数据，而人工标注训练数据往往代价很高，有时就会利用非监督学习的方法。 3.3、预测算法维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划(dynamic programming)求概率最大路径（最优路径）。 算法(维特比算法） 输入：模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$输出：最优路径,即最优状态序列:$I^{*}=(i_{1}^{*},i_{2}^{*},…,i_{T}^{*})$ (1)初始化$$\begin{split}{} &amp; \delta_{1}(i)=\pi_{i}b_{i}(o_{1}),i=1,2,3,…,N \\{} &amp; \psi_{1}(i)=0,i=1,2,3,…,N\end{split}$$ (2)递推。对$t=2,3,…,T$$$\begin{split}{} &amp; \delta_{t}(i)=\max_{1 \leq j \leq N}[\delta_{t-1}(j)a_{ji}]b_{i}(o_{t}),i=1,2,…,N \\{} &amp; \psi_{t}(i)=arg \max_{1 \leq j \leq N}[\delta_{t-1}(j)a_{ji}],i=1,2,…,N\end{split}$$ $\delta_{t}(i)$: (3)终止$$\begin{split}{} &amp; P^{*}=\max_{1 \leq i \leq N}\delta_{T}(i){} &amp; i_{T}^{*}=arg \max_{1 \leq i \leq N}[\delta_{T}(i)]\end{split}$$ (4)最优路径回溯。对$t=T-1,T-2,…,1$$$i_{t}^{*}=\psi_{t+1}(i_{t+1}^{*})$$求得最优路径$I^{*}=(i_{1}^{*},i_{2}^{*},…,i_{T}^{*})$ 维特比算法Python实现 不管是监督学习，还是非监督学习，我们反正都训练出了一个HMM模型。现在诊所来了一位病人，他最近三天的病状是：1observations = ('normal', 'cold', 'dizzy') 如何用Viterbi算法计算他的病情以及相应的概率呢？1234567891011121314151617181920212223242526def viterbi(self, obs_seq): """ Returns ------- V : numpy.ndarray V [s][t] = Maximum probability of an observation sequence ending at time 't' with final state 's' prev : numpy.ndarray Contains a pointer to the previous state at t-1 that maximizes V[state][t] """ N = self.A.shape[0] T = len(obs_seq) prev = np.zeros((T - 1, N), dtype=int) # DP matrix containing max likelihood of state at a given time V = np.zeros((N, T)) V[:,0] = self.pi * self.B[:,obs_seq[0]] for t in range(1, T): for n in range(N): seq_probs = V[:,t-1] * self.A[:,n] * self.B[n, obs_seq[t]] prev[t-1,n] = np.argmax(seq_probs) V[n,t] = np.max(seq_probs) return V, prev 这应该是目前最简洁最优雅的实现了，调用方法如下：12345678910h = hmm.HMM(A, B, pi)V, p = h.viterbi(observations_index)print " " * 7, " ".join(("%10s" % observations_index_label[i]) for i in observations_index)for s in range(0, 2): print "%7s: " % states_index_label[s] + " ".join("%10s" % ("%f" % v) for v in V[s])print '\nThe most possible states and probability are:'p, ss = h.state_path(observations_index)for s in ss: print states_index_label[s],print p 输出结果如下：123456normal cold dizzyHealthy: 0.300000 0.084000 0.005880Fever: 0.040000 0.027000 0.015120The most possible states and probability areHealthy Healthy Fever 0.01512 对算法有疑问的可以参考这段动画，将代码单步一遍，什么都明白了： 4、jieba分词的马尔可夫模型jieba分词想必大家都知道，其分词模型分词部分就是使用了HMM完成的。在源码中有三个文件prob_trans.py为状态转移矩阵，prob_start.py为初始状态矩阵，prob_emit.py为发射矩阵。这三个文件都是训练好的模型文件。然后其使用维特比算法进行分词，我将其代码copy并进行适当修改打印出中间结果，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#!/usr/bin/python# -*- coding:utf-8 -*-#将相关的模型文件import进来from dataset.prob_emit import P as prob_emitfrom dataset.prob_start import P as prob_startfrom dataset.prob_trans import P as prob_trans# 由于采用的是jieba分词的训练数据，而其训练数据都是取了log的结果，所以后续公式中的'*'都会变为'+'MIN_FLOAT = -3.14e100PrevStatus = &#123; 'B': 'ES', 'M': 'MB', 'S': 'SE', 'E': 'BM'&#125;# 打印路径概率表def print_dptable(V): print " ", for i in range(len(V)): print "%7d" % i, print for y in V[0].keys(): print "%.5s: " % y, for t in range(len(V)): print "%.7s" % ("%f" % V[t][y]), printdef viterbi(obs, states, start_p, trans_p, emit_p): """ :param obs:观测序列 :param states:隐状态 :param start_p:初始概率（隐状态） :param trans_p:转移概率（隐状态） :param emit_p: 发射概率 （隐状态表现为显状态的概率） :return: """ # 路径概率表 V[时间][隐状态] = 概率 V = [&#123;&#125;] # 一个中间变量，代表当前状态是哪个隐状态 path = &#123;&#125; # 初始化初始状态 (t == 0) for y in states: # V[0][y] = start_p[y] * emit_p[y][obs[0]] V[0][y] = start_p[y] + emit_p[y].get(obs[0], MIN_FLOAT) path[y] = [y] # 对 t &gt; 0 跑一遍维特比算法 for t in range(1, len(obs)): V.append(&#123;&#125;) newpath = &#123;&#125; for y in states: # log(概率 隐状态) = log(前状态是y0的概率 * y0转移到y的概率 * y表现为当前状态的概率) (prob, state) = max([(V[t - 1][y0] + trans_p[y0].get(y,MIN_FLOAT) + emit_p[y].get(obs[t],MIN_FLOAT), y0) for y0 in PrevStatus[y]]) # 记录最大概率 V[t][y] = prob # 记录路径 newpath[y] = path[state] + [y] # 不需要保留旧路径 path = newpath print_dptable(V) (prob, state) = max([(V[len(obs) - 1][y], y) for y in "ES"]) return (prob, path[state])if __name__ == "__main__": query = u"吉林省长春市长春药店" (prob, tags) = viterbi(query,u"BMES",prob_start,prob_trans,prob_emit) print prob word_list = [word for word in query] word_tag_list= zip(word_list,tags) for word,tag in word_tag_list: print word,"/",tag 运行后结果如下： 上面标红的结果练成一串就是最优路径，但是上面的结果很容易给大家造成误导，误导就是以为是求每一步的最优解就是结果，其实只是一种巧合和，局部最优并不一定是全局最优。其实在维特比求解的过程中，参照动态规划的最短路径的思想回溯，求得全局最优解。 5、参考《统计学习方法》Viterbi algorithm(wiki)隐马尔可夫模型(码农场)jieba分词的源码]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>序列标注</tag>
        <tag>分词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词典分词之mmseg算法原理及其实现]]></title>
    <url>%2FNLP%2F%E8%AF%8D%E5%85%B8%E5%88%86%E8%AF%8D%E4%B9%8Bmmseg%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1、简介MMSeg是由台湾学者蔡志浩（Chih-Hao Tsai）于1996年提出的基于字符串匹配（亦称基于词典）的中文分词算法。词典分词应用广、可控、速度快，但也存在无法解决歧义问题，比如，“武汉市长江大桥”是应分词“武汉/市长/江大桥”还是“武汉市/长江/大桥”。基于此，有人提出了正向最大匹配策略，但是可能会出现分词错误的情况，比如：若词典中有“武汉市长”，则原句被分词成“武汉市长/江大桥”。单纯的最大匹配还是无法完美地解决歧义，因而MMSeg在正向最大匹配的基础上设计了四个启发式规则，解决了词典分词的歧义问题。 使用过MMSEG分词算法的人反馈都不错，觉得这个算法简单高效，而且还非常准确。作者声称这个规则达到了99.69%的准确率并且93.21%的歧义能被这个规则消除。 2、MMseg分词算法原理2.1、基本含义MMSeg的字符串匹配算法分为两种 Simple，即简单的正向匹配，根据开头的字，列出所有可能的结果，取最大匹配结果； Complex，匹配出所有的“三个词的词组”（即原文中的chunk，“词组”），即从某一既定的字为起始位置，得到所有可能的“以三个词为一组”的所有组合，设计了四个去歧义规则（Ambiguity Resolution Rules）指导分词。 接下我们主要讲Complex的原理。首先来理解一下chunk，它是MMSeg分词算法中一个关键的概念。Chunk中包含依据上下文分出的一组词和相关的属性，包括长度(Length)、平均长度(Average Length)、标准差的平方(Variance)和自由语素度(Degree Of Morphemic Freedom)。下表中列出了各个属性的含义： 属性 含义 长度(Length) chuck中各个词的长度之和 平均长度(Average Length) chuck中各个词的长度求平均 标准差的平方(Variance) chuck中各个词的长度求标准差 自由语素度(Degree Of Morphemic Freedom) 各单字词词频的对数之和 可能看到上面的一些概念会觉得有些模糊，下面我们通过一个简单的列子加深理解。其实chunk就是三个词为一组，在chunk确定之后，chunk的各种属性也就随之可以计算出来，比如”长春市长春药店”，我们可以得到很多chunk 长春_市_长春 长_春_市长 长春_市长_春药 长春市_长春_药店 …. 词典分解出来的可能更多(假定这些词都在词典中)。上面列举的三个词为一组的就是一个chunk，一旦chunk确定之后chunk的各种属性也就可以随之计算出来。 2.2、四个规则消除歧义的规则有四个，使用中依次用这四个规则进行过滤，直到只有一种结果或者第四个规则使用完毕，4条消歧规则包括： 规则1：取最大匹配的chunk (Rule 1: Maximum matching) 规则2：取平均词长最大的chunk (Rule 2: Largest average word length) 规则3：取词长标准差最小的chunk (Rule 3: Smallest variance of word lengths) 规则4：取单字词自由语素度之和最大的chunk (Rule 4: Largest sum of degree of morphemic freedom of one-character words) 接来下举例进行分析: 取最大匹配的chunk（Maximum matching (Chen &amp; Liu 1992)）有两种情况，分别对应于使用“simple”和“complex”的匹配方法。对“simple”匹配方法，选择长度最大的词，用在上文的例子中即选择“长春市”；对“complex”匹配方法，选择“词组长度最大的”那个词组，然后选择这个词组的第一个词，作为切分出的第一个词,所以在上文提到的例子中我们选择的chunk为“长春市_长春_药店”,切分出的词为“长春市” 从以上的情况来看貌似simple和complex没啥区别，如果是对“北京大学城”进行分词的话，“simple”第一个词结果为“北京大学”，“complex”第一个词结果为”北京”，正确的分词结果为“北京 大学城”。 对于有超过一个词组满足这个条件的则应用下一个规则。 取平均词长最大的chunk（Largest average word length (Chen &amp; Liu, 1992)）经过规则1过滤后，如果剩余的词组超过1个，那就选择平均词语长度最大的那个（平均词长＝词组总字数／词语数量）。比如：“李志博士兵”，经过以上规则过滤后，可能还剩下如下的chunk： 李志博_士兵(5/2=2.5)李志_博士_兵(5/3=1.67) 依据此规则就可以确定“李志博_士兵”这个chunk，但是对于上述的“北京大学城”此条规则并未起作用。 取词长标准差最小的chunk（Smallest variance of word lengths (Chen &amp; Liu, 1992)）由于词语长度的变化率可以由标准差反映，所以此处直接套用标准差公式即可。比如针对”研究生命起源“这个case,经过上面的规则过滤之后有: 研究_生命_起源 （标准差=sqrt(((2-2)^2+(2-2)^2+(2-2^2))/3)=0）研究生_命_起源 （标准差=sqrt(((2-3)^2+(2-1)^2+(2-2)^2)/3)=0.8165） 于是通过这一规则选择“研究_生命_起源”这个词组 取单字词自由语素度之和最大的chunk（Largest sum of degree of morphemic freedom of one-character words）这里的自由语素度可以用一个数学公式表达：log(frequency)，即词频的自然对数（这里log表示数学中的ln）。这个规则的意思是“计算词组中的所有单字词词频的自然对数，然后将得到的值相加，取总和最大的词组”。比如：“设施和服务”，经过上面的规则过滤之后，这个会有如下几种组合 设施_和服_务设施_和_服务 利用最后一个规则 第一条中的“务”和第二条中的“和”，从直观看，显然是“和”的词频在日常场景下要高，这依赖一个词频字典和的词频决定了的分词。假设“务”作为单字词时候的频率是30，“和”作为单字词时候的频率是100，对30和100取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施_和_服务”。也许会问为什么要对“词频”取自然对数呢？可以这样理解，词组中单字词词频总和可能一样，但是实际的效果并不同，比如A_BBB_C （单字词词频，A:3， C:7）DD_E_F （单字词词频，E:5，F:5）表示两个词组，A、C、E、F表示不同的单字词，如果不取自然对数，单纯就词频来计算，那么这两个词组是一样的（3+7=5+5），但实际上不同的词频范围所表示的效果也不同，所以这里取自然对数，以表区分（ln(3)+ln(7) &lt; ln(5)+ln(5)， 3.0445&lt;3.2189)。 从上面的叙述来看MMseg算法是将一个句子尽可能长和尽可能均匀的切分，这与中文的语法习惯比较相符。同时，分词效果与词典的关系较大，尤其是词典中单字词的频率。“simple”只是用了上述的第一个规则，其实等同于向前最大匹配算法，“complex”以上的四个规则都是用了。实际使用中，一般都是使用complex的匹配方法＋四个规则过滤。在使用MMseg词典分词算法的时候尽量使用领域词典对该领域进行分词，如：计算机类词库、医疗词库、金融词库、旅游类词库等，这样效果会好很多。总的来说MMseg是一个简单、快速、有效的分词方法。 3、算法拓展上文基本上将MMseg算法的原理讲解了一遍，但是其实上述的规则并未消除掉所有的歧义，比如：“购买文学艺术作品”按照上述的“complex”模式进行分词可能经过四个规则过滤后还会剩余一下2个chunk: 购买_文学_艺术作品 购买_文学艺术_作品 如果词典中有“文学艺术作品”,那么对于这个语句使用MMseg分词是没问题的。那么如果词典中没有呢，至少我加载jieba分词的词典中是没有的，是使用该词典进行MMseg分词,经过四个规则过滤之后剩下以上两个chunk。那么如何对以上的两个chunk进行优选呢。受 规则4 的启发，我们是否在以上的规则基础之上加入 第五条规则：(5) 计算chunk自由语素度，取最大的那个chunk，即$$max(\sum_{i=0}^{N} log(freq_{w_i})),N=2$$所以，针对上述例子中,如果使用jieba分词词典,各个词在词典中的信息为：购买(4275)、文学(6890)、艺术作品(3)、文学艺术(3)、作品(9248)则有: $$\begin{split}f(购买||文学||艺术作品){} &amp; =log(freq_{购买})+log(freq_{文学})+log(freq_{艺术作品}) \\ {} &amp; =log(4275)+log(6890)+log(3) \\ {} &amp; =18.397\end{split}$$ $$\begin{split}f(购买||文学艺术||作品){} &amp; =log(freq_{购买})+log(freq_{文学艺术})+log(freq_{作品}) \\ {} &amp; =log(4275)+log(3)+log(9248) \\ {} &amp; =18.691\end{split}$$则按照规则5，则最后取“购买_文学艺术_作品”这个chunk，就解决了经过4个规则过滤之后的还剩多个chunk的情况。当然，以上加的这条规则为个人的一个思路，因为在实际中确确实实遇到了这个问题。其实加规则的思路还有很多，比如： 直接计算chunk的trigram的概率，因为chunk刚好是三个词组合，缺点是计算量大。 依据chunk的第一个词的log(freq),取该值的最大那个chunk，因为chunk过滤完成之后也只是决定了当前的分词结果为哪个词。其实以上思路的都是从算法规则上添加条件过滤，解决问题的方式还可以从词典入手，比如针对以上的例子加入“文学艺术作品”这个词汇。 拓展中所涉及的内容都是个人的一个理解和遇到问题的一个思路，将其列举了出来，并不为MMseg论文中的内容。如果有不妥之处希望大家批评指正。 4、算法demo实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338#!/usr/bin/python# -*- coding:utf-8 -*-from __future__ import unicode_literalsimport jsonimport mathimport os, sysimport reclass Mmseg(object): def __init__(self): self.MIN_FLOAT = -3.14e100 self.dic_file = "user_dic.json" self.user_dic = json.load(open(self.dic_file, "r")) self.maxlenth = 100 # 最大匹配长度 self.re_eng = re.compile('[a-zA-Z0-9]', re.U) # 英文数字的正则 self.re_han_default = re.compile("([\u4E00-\u9FD5a-zA-Z0-9+#&amp;\._]+)", re.U) # 汉字的正则 self.re_skip_default = re.compile("(\r\n|\s)", re.U) # 符号的正则 self.re_han_cut_all = re.compile("([\u4E00-\u9FD5]+)", re.U) self.re_skip_cut_all = re.compile("[^a-zA-Z0-9+#\n]", re.U) self.show_chunks_flag=False def get_DAG(self,sentence): """ 构建有向无环图,使用词典中的词进行最大匹配 :param sentence: :return: """ dag = &#123;&#125; for index, word in enumerate(sentence): for j in range(index, len(sentence)): cur_word = u"".join(sentence[index:j + 1]) if cur_word in self.user_dic: if index in dag: dag[index].append(j) else: dag[index] = [j] else: #是否break决定着最大或者最小切分 # break if (j - index) &gt;= self.maxlenth: break return dag def maxLengthFilter(self,chunks): """ 规则1：取最大长度匹配的chunk,即chunk中各词的长度之和取最大的chunk :param chunks: :return: """ chunks_dict = &#123;&#125; for chunk in chunks: words_length = sum([(end - begin + 1) for begin, end in chunk]) # 词的长度 if words_length in chunks_dict: chunks_dict[words_length].append(chunk) else: chunks_dict[words_length] = [chunk] chunks_dict_sorted = sorted(chunks_dict.items(), key=lambda x: x[0], reverse=True) filted_chunks = chunks_dict_sorted[0][1] # 取最长的chunks return filted_chunks def averageMaxLengthFilter(self,chunks): """ 规则2：取平均长度最大的,即chunk中(词的长度/chunk的切分个数) :param chunks: :return: """ chunks_dict = &#123;&#125; for chunk in chunks: chunk_size = float(len(chunk)) # chunk的切分个数 words_length = float(sum([(end - begin + 1) for begin, end in chunk])) average_length = float(words_length / chunk_size) if average_length in chunks_dict: chunks_dict[average_length].append(chunk) else: chunks_dict[average_length] = [chunk] chunks_dict_sorted = sorted(chunks_dict.items(), key=lambda x: x[0], reverse=True) filted_chunks = chunks_dict_sorted[0][1] # 取最长的chunks return filted_chunks def standardDeviationFilter(self,chunks): """ 规则3:取标准差平方最小的 标准差平方:[(x1-x)^2+(x2-x)^2 + ...+(xn-x)^2]/n ,其中x为平均值 :param chunks: :return: """ chunks_dict = &#123;&#125; for chunk in chunks: chunk_size = float(len(chunk)) # chunk的切分个数 average_words_size = float(sum([(end - begin + 1) for begin, end in chunk]) / chunk_size) chunk_sdq = float(0) # chunk的标准差平方 for (begin, end) in chunk: cur_size = float(end - begin + 1) chunk_sdq += float((cur_size - average_words_size) ** 2) chunk_sdq = float(chunk_sdq / chunk_size) if chunk_sdq in chunks_dict: chunks_dict[chunk_sdq].append(chunk) else: chunks_dict[chunk_sdq] = [chunk] chunks_dict_sorted = sorted(chunks_dict.items(), key=lambda x: x[0], reverse=False) filted_chunks = chunks_dict_sorted[0][1] # 取最长的chunks return filted_chunks def logFreqFilterSingle(self,chunks,sentence): """ 规则4:取单字自由度之和最大的chunk 自由度：log(frequency)，即词频取自然对数 :param chunks: :return: """ chunks_dict = &#123;&#125; for chunk in chunks: log_freq = 0 for (begin,end) in chunk: if begin==end: log_freq += math.log(float(self.user_dic.get(sentence[begin:end+1],1))) if log_freq in chunks_dict: chunks_dict[log_freq].append(chunk) else: chunks_dict[log_freq] = [chunk] chunks_dict_sorted = sorted(chunks_dict.items(), key=lambda x: x[0], reverse=True) filted_chunks = chunks_dict_sorted[0][1] # 取最长的chunks return filted_chunks def logFreqFilterMutiWord(self, chunks, sentence): """ 规则5:取非单字的词语自由度之和最大的chunk 自由度：log(frequency)，即词频取自然对数 :param chunks: :return: """ chunks_dict = &#123;&#125; for chunk in chunks: log_freq = 0 for (begin, end) in chunk: if begin != end: log_freq += math.log(float(self.user_dic.get(sentence[begin:end + 1], 1))) if log_freq in chunks_dict: chunks_dict[log_freq].append(chunk) else: chunks_dict[log_freq] = [chunk] chunks_dict_sorted = sorted(chunks_dict.items(), key=lambda x: x[0], reverse=True) filted_chunks = chunks_dict_sorted[0][1] # 取最长的chunks return filted_chunks def chunksFilter(self,chunks,sentence): """ 过滤规则 :param chunks: :param sentence: :return: """ if len(chunks) == 1: return chunks # 1、取words长度之后最大的chunks new_chunks = self.maxLengthFilter(chunks) if len(new_chunks) == 1: return new_chunks # 2、取平均word长度最大的chunks new_chunks = self.averageMaxLengthFilter(new_chunks) if len(new_chunks) == 1: return new_chunks # 3、取平均方差最小的chunks new_chunks = self.standardDeviationFilter(new_chunks) if len(new_chunks) == 1: return new_chunks # 4、取单字自由度组合最大的chunk new_chunks = self.logFreqFilterSingle(new_chunks,sentence) if len(new_chunks) == 1:return new_chunks # 5、取非单字的所有词的组合自由语素之和最大的 new_chunks = self.logFreqFilterMutiWord(new_chunks, sentence) if len(new_chunks) != 1: print(sentence,"=====") raise Exception("chunk划分最终的结果不唯一") return new_chunks def get_chunks(self,DAG, begin_index, text_length,sentence): """ 给出所有的词和开始的位置,拿到所有的chunk :param DAG:一句话的在字典中的最大匹配的位置 :param begin_index:从sentence中的某个起始位置开始构建chunk :param text_length:文本的最大长度 :return:返回该位置生成的三个chunk组成chunks的所有组合 """ chunks = [] chunk = [] if begin_index &gt; text_length: raise Exception("begin index out of sentcen length!!!") for i in DAG.get(begin_index, [begin_index]): # 有的话就是词词典中的匹配，否则就是单字本身 if (i + 1) &gt; text_length - 1: # 到了sentence的结尾,后面无可用字或词 chunks.append([(begin_index, i)]) break for j in DAG.get(i + 1, [i + 1]): if (j + 1) &gt; text_length - 1: # 到了sentence的结尾,后面无可用字或词 chunks.append([(begin_index, i), (i + 1, j)]) break for k in DAG.get(j + 1, [j + 1]): chunk.append((begin_index, i)) chunk.append((i + 1, j)) chunk.append((j + 1, k)) chunks.append(chunk) chunk = [] chunks = self.chunksFilter(chunks,sentence) if self.show_chunks_flag: self.show_chunks(sentence,chunks) return chunks def cut_DAG(self,sentence): """ 对于划分后的句子，进行切分然后分词 :param sentence: :return: """ self.sentence = sentence text_length = len(sentence) DAG = self.get_DAG(sentence) index = 0 buff = "" while True: if index&gt;text_length-1:break chunks = self.get_chunks(DAG,index,text_length,sentence) begin,end = chunks[0][0] word = "".join(sentence[begin:end+1]) if self.re_eng.match(word) and len(word)==1: #对英文和数字进行处理 buff += word else: if buff: yield buff buff = "" yield word index = end+1 if buff: #末尾为数字或英文 yield buff buff = "" def cut_re(self,sentence,cut_all = False): """ 文本的分词切分 :param sentence:输入进来的文本 :param cut_all: 全切分(暂不支持) :return:返回切分的词 """ if cut_all: re_han = self.re_han_cut_all re_skip = self.re_skip_cut_all else: re_han = self.re_han_default re_skip = self.re_skip_default blocks = self.re_han_default.split(sentence) for blk in blocks: #按照符号划分然后在针对划分后的分词 if not blk: continue if re_han.match(blk): for word in self.cut_DAG(blk): yield word else: tmp = re_skip.split(blk) for x in tmp: if re_skip.match(x): yield x elif not cut_all: for xx in x: yield xx else: yield x def cut(self,sentence): sentence = self.strdecode(sentence) for word in self.cut_DAG(sentence): yield word def show_chunks(self,sentence, chunks,word=None): """ 打印使用，调试手段 :param sentence: :param chunks: :return: """ for chunk in chunks: chunk_words = [] for (begin, end) in chunk: word = "".join(sentence[begin:end + 1]) chunk_words.append(word) print("\t".join(chunk_words)) if word!=None:print(word) print("=======================") def strdecode(self,sentence,text_type=str): if not isinstance(sentence, text_type): try: sentence = sentence.decode('utf-8') except UnicodeDecodeError: sentence = sentence.decode('gbk', 'ignore') return sentencedef test_one(): mmseg = Mmseg() mmseg.show_chunks_flag = False mmseg.user_dic[u"交易性金融资产"] = 1 mmseg.user_dic[u"融通基金a"] = 1 mmseg.user_dic[u"出了"] = 100 print(mmseg.user_dic[u"中出"]) query = "持股基金包含融通基金a，银行存款，交易性金融资产" # print(json.dumps(re.split("[, ， 。 ；]",query),ensure_ascii=False)) # print(json.dumps(re.findall("[, ， 。 ；]", query), ensure_ascii=False)) print("/".join(list(mmseg.cut(query)))) query = "购买文学艺术作品" print("/".join(list(mmseg.cut(query)))) query = "吉林省长春市长春药店,我来到了北京大学城,看到了北京大学生的技术和服务很厉害。工信处女干事每月经过楼下都要亲口交代，2015年5月机器需要维护。我们中出了一个汉奸。" print("/".join(list(mmseg.cut(query)))) mmseg.user_dic[u"李志博"] = 100 mmseg.user_dic[u"李志"] = 100 query = "董事长包含李志博士" print("/".join(list(mmseg.cut(query)))) mmseg.user_dic[u"市销率(ps)"] = 100 print("/".join(list(mmseg.cut("市销率(ps)")))) print("/".join(list(mmseg.cut("吉林省长春市长春药店"))))if __name__ == "__main__": test_one() 上述代码中将jieba分词的词典中的word和freq转换为{word:freq}的json文件，则运行程序的结果为: 3持股/基金/包含/融通基金a/，/银行存款/，/交易性金融资产购买/文学艺术/作品吉林省/长春市/长春/药店/,/我/来到/了/北京/大学城/,/看到/了/北京/大学生/的/技术/和/服务/很/厉害/。/工信处/女干事/每月/经过/楼下/都/要/亲口/交代/，/2015/年/5/月/机器/需要/维护/。/我们/中出/了/一个/汉奸/。董事长/包含/李志/博士市销率(ps)吉林省/长春市/长春/药店 算法原理很简单，以上代码为python的一个简单实现。如果想做成java版本话，这个可以作为一个简单的参照! 5、总结 优点：MMseg是一个简单快速有效的词典分词算法，无需训练语料，算法效果来说很不错。有些句子在使用jieba的词典的情况下比jieba的分词结果都要好。比如:“我来到了北京大学城”，使用jieba分词和mmseg分词的结果为： 我/来到/了/北京/大学城(MMseg结果)我/来到/了/北京大学/城(jieba结果) 缺点：有着词典分词的缺点就是对于未登录词无法解决。其实也存在也使用词典分词也无法解决的反例，比如：把手抬起来，正确的分词是把_手_抬起_来,但经过本算法的规则过滤为把手_抬起_来。 对于未登录词可以使用模型(HMM、CRF、BI-LSTM)来解决。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>分词</tag>
        <tag>词典分词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、EM算法简介EM算法，指的是最大期望算法(Expectation Maximization Algorithm)是一种迭代算法，在统计学中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。最大期望算法经过两个步骤交替进行计算：计算期望(E)，利用概率模型参数的现有估计值，计算隐藏变量的期望；最大化(M)，利用E步上求得的隐藏变量的期望，对参数模型进行最大似然估计；M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。所以称为EM算法(Expectation Maximization Algorithm)。 二、预备知识在了解EM算法前我们先需要理解两个知识点：极大似然估计和Jensen不等式。 2.1、极大似然估计极大似然估计方法（Maximum Likelihood Estimate，MLE）也称为最大概似估计或最大似然估计，是求估计的另一种方法，最大概似是1821年首先由德国数学家高斯（C. F. Gauss）提出。最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。 (1) 举例说明：经典问题——学生身高问题 我们需要调查我们学校的男生和女生的身高分布。 假设你在校园里随便找了100个男生和100个女生。他们共200个人。将他们按照性别划分为两组，然后先统计抽样得到的100个男生的身高。假设他们的身高是服从正态分布的。但是这个分布的均值$\mu$和方差$\sigma^2$我们不知道，这两个参数就是我们要估计的。记作$\theta=[\mu,\sigma]^T$。问题：我们知道样本所服从的概率分布的模型和一些样本，需要求解该模型的参数,如下图所示： 已知的：样本服从的分布模型、随机抽取的样本 未知的：模型的参数 根据已知条件，通过极大似然估计，求出未知参数。总的来说：极大似然估计就是用来估计模型参数的统计学方法。 (2) 如何估计 问题数学化：设样本集$X=x_1,x_2,…,x_N$,其中N=100 ，$p(x_i;\theta)$为概率密度函数，表示抽到男生$x_i$的身高的概率。由于100个样本之间独立同分布，所以我同时抽到这100个男生的概率就是他们各自概率的乘积，也就是样本集X中各个样本的联合概率，用下式表示：$$L(\theta)=L(x_1,x_2,….,x_n;\theta)=\prod_{i=1}^{n}p(x_i;\theta),\theta \in \Theta$$这个概率反映了，在概率密度函数的参数是$\theta$时，得到X这组样本的概率。 我们需要找到一个参数$\theta$，使得抽到X这组样本的概率最大，也就是说需要其对应的似然函数$L(\theta)$最大。满足条件的$\theta$叫做$\theta$的最大似然估计量，记为：$$\hat{\theta}=argmaxL(\theta)$$ (3) 求最大似然函数估计值的一般步骤首先，写出似然函数：$$L(\theta)=L(x_1,x_2,….,x_n;\theta)=\prod_{i=1}^{n}p(x_i;\theta),\theta \in \Theta$$然后，对似然函数取对数：$$H(\theta)=logL(\theta)=log\prod_{i=1}^{n}p(x_i;\theta)=\sum_{i=1}^{n}logp(x_i;\theta)$$ 在此我们取了一个对数似然，有两点优势，一个就是可以将乘法转换为加法，第二个取对数是单调上升函数，取最大似然和取对数最大似然目标是一致的 (4)求最大似然函数估计值的一般步骤 写出似然函数； 对似然函数取对数，并整理； 求导数，令导数为0，得到似然方程； 解似然方程，得到的参数即为所求； 2.2、Jensen不等式Jensen不等式的意义是：如果对于所有的实数x，f(x)为凸函数(即f(x)的二阶导大于等于0)，则函数的期望大于等于期望的函数(当且仅当x为常量时取等号),即$$E(f(x))\ge f(E(x))$$或者写成凸函数条件表达式的形式，在这个表达式式中，t相当于$x_1$的概率,(1−t)相当于$x_2$的概率：$$tf(x_1)+(1−t)f(x_2)\ge f(tx_1+(1−t)x_2); t\in{0,1}$$ Jensen不等式应用于凹函数时，不等号方向反向。当且仅当X是常量时，Jensen不等式等号成立。 三、EM算法详解还是回到上文中身高问题，假设男生、女生的身高分别服从各自的正态分布，则每个样本是从哪个分布抽取的，我们目前是不知道的。这个时候，对于每一个样本，就有两个方面需要猜测或者估计： 这个身高数据是来自于男生还是来自于女生？男生、女生身高的正态分布的参数分别是多少？EM算法要解决的问题正是这两个问题。 3.1、EM算法推导给定的训练样本是$X={x_1,…,x_m}$，样例间独立，我们想找到每个样例隐含的类别$z$，能使得$p(x,z)$最大。$p(x,z)$的最大似然估计如下：$$H(\theta)=\sum_{i=1}^{m}logp(x_i;\theta)=\sum_{i=1}^{m}log\sum_{z_i}p(x_i,z_i;\theta)$$ 第一步是对极大似然取对数，第二步是对每个样例的每个可能类别z求联合分布概率和。但是直接求$\theta$一般比较困难，因为有隐藏变量$z$存在，但是一般确定了$z$后，求解就容易了。 EM是一种解决存在隐含变量优化问题的有效方法。既然不能直接最大化$H(\theta)$，我们可以不断地建立$H(\theta)$的下界(E步)，然后优化下界(M步)。对于每一个样例$i$，让$Q_i$表示该样例隐含变量$z$的某种分布，$Q_i$满足的条件是$\sum_{z}Q_i(z)=1,Q_i(z)\ge 0$。（如果$z$是连续性的，那么$Q_i$是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量$z$是身高，那么就是连续的高斯分布。如果按照隐藏变量是男女，那么就是伯努利分布了。可以由前面阐述的内容得到下面的公式： $$\begin{split}\sum_{i}logp(x_i;\theta) {} &amp; =\sum_{i}log\sum_{z_i}p(x_i,z_i)\quad\quad\quad\quad(1)\\ {} &amp; =\sum_{i}log\sum_{z_i}Q_i(z_i)\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\quad(2)\\ {} &amp; \ge\sum_{i}\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\quad(3)\end{split}$$(1)到(2)比较直接，就是分子分母同乘以一个相等的函数。(2)到(3)利用了Jensen不等式，考虑到$log(x)$是凹函数（二阶导数小于0），而且$\sum_{z_i}Q_i(z_i)\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}$就是$[p(x_i,z_i;\theta)/Q_i(z_i)]$的期望。详细了解参考博客the EM algorithm 上述过程可以看作是对$logL(\theta)$即$L(\theta)$）求了下界。对于$Q_i(z_i)$的选择，有多种可能，那么哪种更好呢？假设$\theta$已经给定，那么$logL(\theta)$的值就取决于$Q_i(z_i)$和$p(x_i,z_i)$了。我们可以通过调整这两个概率使下界不断上升，以逼近$logL(\theta)$的真实值，那么什么时候算是调整好了呢？当不等式变成等式时，说明我们调整后的概率能够等价于$logL(\theta)$了。按照这个思路，我们要找到等式成立的条件。根据Jensen不等式，要想让等式成立，需要让随机变量变成常数值，这里得到：$$\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}=c$$c为常数，不依赖于$z_i$。对此式做进一步推导：由于$\sum_{z_i}Q_i(z_i)=1$，则有$\sum_{z_i}p(x_i,z_i;θ)=c$（多个等式分子分母相加不变，则认为每个样例的两个概率比值都是$c$），因此得到下式：$$Q(z_i)=\frac{p(x_i,z_i;\theta)}{\sum_{z_i}p(x_i,z_i;\theta)}=\frac{p(x_i,z_i;\theta)}{p(x_i;\theta)}=p(z_i|x_i;\theta)$$ 至此，我们推出了在固定其他参数$\theta$后，$Q_i(z_i)$的计算公式就是后验概率，解决了$Q_i(z_i)$如何选择的问题。这一步就是E步，建立$logL(\theta)$的下界。接下来的M步，就是在给定$Q_i(z_i)$后，调整$\theta$，去极大化$logL(\theta)$的下界（在固定$Q_i(z_i)$后，下界还可以调整的更大）。 3.2、EM算法流程(1)初始化分布参数θ；(2) 重复以下E、M步骤直到收敛 E步骤：根据参数θ初始值或上一次迭代所得参数值来计算出隐性变量的后验概率（即隐性变量的期望），作为隐性变量的现估计值：$$Q_i(z_i):=p(z_i|x_i;\theta)$$ M步骤：将似然函数最大化以获得新的参数值：$$\theta:=arg\max_{\theta}\sum_{i}\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}$$ 3.3、EM算法优缺点 优点：算法简单，稳定上升的步骤能非常可靠地找到“最优的收敛值。 缺点：当所要优化的函数不是凸函数时，EM算法容易给出局部最佳解，而不是最优解；对初始值敏感：EM算法需要初始化参数θ，而参数θ的选择直接影响收敛效率以及能否得到全局最优解。 EM算法的应用：k-means算法是EM算法思想的体现，E步骤为聚类过程，M步骤为更新类簇中心。GMM（高斯混合模型）也是EM算法的一个应用。 四、参考 (EM算法)The EM Algorithm 机器学习系列之EM算法 《统计机器学习方法》，李航 《机器学习》，周志华]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
        <tag>EM算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信息熵及其相关]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E4%BF%A1%E6%81%AF%E7%86%B5%E5%8F%8A%E5%85%B6%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[一、信息熵根据香农(Shannon)给出的信息熵公式，对于任意一个随机变量X，它的信息熵定义如下，单位为比特(bit)：$$ H(X) = -\sum_{i=1}^{m}p(x_i)logp(x_i)$$在物理界中熵是描述事物无序性的参数，熵越大则越混乱,类似的在信息论中熵表示随机变量的不确定程度。下图描述了各种熵的关系： 二、条件熵设X,Y为两个随机变量，X的取值为 $x_1,x_2,…,x_m $ ,Y 的取值为$ y_1,y_2,…y_n $，则在X 已知的条件下Y的条件熵记做$H(Y|X)$:$$H(Y|X)=\sum_{i}^{m}P(Y|X=x_{i}) \\=-\sum_{i=1}^{m}p(x_i)\sum_{j=1}^{n}p(y_{j}|x_{i})logp(y_{j}|x_{i}) \\=-\sum_{i=1}^{m}\sum_{j=1}^{n}p(y_{j},x_{i})logp(y_{j}|x_{i}) \\=-\sum_{x_{i},y_{j}}p(x_{i},y_{j})logp(y_{j}|x_{i})$$ 注意，这个条件熵，不是指在给定某个数（某个变量为某个值）的情况下，另一个变量的熵是多少，变量的不确定性是多少？而是期望！因为条件熵中X也是一个变量，意思是在一个变量X的条件下（变量X的每个值都会取），另一个变量Y熵对X的期望。 三、联合熵设X,Y为两个随机变量，X的取值为 $x_1,x_2,…,x_m $ ,Y 的取值为$ y_1,y_2,…y_n $，则其联合熵定义为:$$H(X,Y)=-\sum_{i}^{m}\sum_{j}^{n}p(x_{i},y_{j})logp(x_{i},y_{j})$$ 联合熵与条件熵的关系$$H(Y|X)=H(X,Y)-H(X) \\H(X|Y)=H(X,Y)-H(Y)$$ 联合熵满足几个性质 ： $H(Y|X)&gt;=max(H(X),H(Y))$ $H(Y|X)&lt;=H(X)+H(Y)$ $H(X,Y)&gt;=0$ 四、相对熵(KL散度)相对熵，又称为KL距离，是Kullback-Leibler散度（Kullback-Leibler Divergence）的简称。它主要用于衡量相同事件空间里的两个概率分布的差异。其定义如下：$$D(P||Q)=\sum_{x \in X}P(x)log\frac{P(X)}{Q(X)}$$ 相对熵（KL-Divergence KL散度）: 用来描述两个概率分布 P 和 Q 差异的一种方法。 它并不具有对称性，这就意味着:$$D(P||Q) \neq D(Q||P)$$ KL 散度并不满足距离的概念，因为 KL 散度不是对称的,且不满足三角不等式。对于两个完全相同的分布，他们的相对熵为0，$D(P||Q)$与函数P和函数Q之间的相似度成反比，可以通过最小化相对熵来使函数Q逼近函数P，也就是使得估计的分布函数接近真实的分布。KL可以用来做一些距离的度量工作，比如用来度量topic model得到的topic分布的相似性。 五、互信息(MI)对于随机变量 X,Y 其互信息可表示为$MI(X,Y)$:$$MI(X,Y)=\sum_{i=1}^{m}\sum_{j=1}^{n}p(x_{i},y_{j})log_{2}\frac{p(x_{i},y_{j})}{p(x_{i})p(y_{j})} $$ 与联合熵分布的区别：$$H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y) \\MI(X,Y)=H(X)−H(Y|X)=H(Y)−H(X|Y)$$ 六、点互信息(PMI)可以看到使用PMI衡量两个变量之间的相关性，比如两个词，两个句子。原理公式为：$$PMI(x;y)=log\frac{p(x,y)}{p(x)p(y)}=log\frac{p(x|y)}{p(x)}=log\frac{p(y|x)}{p(y)} $$在概率论中，如果x和y无关，$p(x,y)=p(x)p(y)$；如果x和y越相关，$p(x,y)$和$p(x)p(y)$的比就越大。从后两个条件概率可能更好解释，在y出现的条件下x出现的概率除以单看x出现的概率，这个值越大表示x和y越相关。 log取自信息论中对概率的量化转换（对数结果为负，一般要再乘以-1，当然取绝对值也是一样的）在自然语言处理中可以用于新词或新的短语发现。 七、交叉熵设随机变量X的真实分布为p，用q分布来近似p ，则随机变量X的交叉熵定义为：$$H(p,q)=E_{p}[−logq]=−\sum_{i=1}^{m}p(x_{i})logq(x_{i})$$形式上可以理解为使用q来代替p求信息熵了。交叉熵用作损失函数时，q即为所求的模型,可以得到其与相对熵的关系： $$H(p,q)=-\sum_{x}p(x)logq(x) \\ =-\sum p(x)log\frac{q(x)}{p(x)}p(x) \\ =-\sum p(x)logp(x)-\sum_{x}p(x)log\frac{q(x)}{p(x)} \\ =H(p)+D(p||q)$$ 可见分布p与q的交叉熵等于p的熵加上p与q的KL距离，所以交叉熵越小，D(P||Q)越小，即分布q与p越接近，这也是相对熵的一个意义。 逻辑回归的损失函数:$J(\theta)=-[yln(h_{\theta}(x))+(1-y)ln(1-h_{\theta}(x))]$softmax的交叉熵损失函数:$CrossEntropy=-\sum_{i=1}^{m}y_ilog(p_i)$ 交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。在特征工程中，可以用来衡量两个随机变量之间的相似度。在语言模型中（NLP）中，由于真实的分布p是未知的，在语言模型中，模型是通过训练集得到的，交叉熵就是衡量这个模型在测试集上的正确率。 八、信息增益及信息增益比信息增益，是一种衡量样本特征重要性的方法。 特征A对训练数据集D的信息增益g(D,A) ，定义为集合D的经验熵H(D)与特征A在给定条件下D的经验条件熵H(D|A)之差，即$$g(D,A)=H(D)-H(D|A)$$可见信息增益与互信息类似，然后是信息增益比： $$g_{R}(D,A = \frac{g(D,A)}{H(D)} $$ 八、参考 信息熵 Information Theory 《统计自然语言处理第二版》 宗成庆]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
        <tag>信息熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git使用]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%B7%A5%E5%85%B7%2Fgit%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、git基本使用1.1、创建新仓库12cd target_pathgit init 1.2、从远端仓库拉代码 以创建一个本地仓库的克隆版本 1git clone /path/to/repository 远端服务器上的仓库 1git clone username@host:/path/to/repository 1.3、添加与提交12git add &lt;filename&gt; #添加某个文件git add * # 添加所有 1git commit -m "代码提交信息" #提交到本地 你的改动现在已经在本地仓库的 HEAD 中了 1.4、添加远程仓库和推送如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，执行如下命令：12git remote add origin &lt;server&gt;git remote add origin git@github.com:yourname/code.git #示例 将这些改动提交到远端仓库1git push origin master 可以把 master 换成你想要推送的任何分支 1.5、分支分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是“默认的”。在其他分支上进行开发，完成后再将它们合并到主分支上。 查看分支： 1git branch 创建分支： 1git branch &lt;name&gt; 切换分支： 1git checkout &lt;name&gt; 创建+切换分支： 1git checkout -b &lt;name&gt; 合并某分支到当前分支： 1git merge &lt;name&gt; 删除分支： 1git branch -d &lt;name&gt; 1.6、更新与合并更新最新代码1234git checkout branch_one #切换到最新的分支branch_onegit pull # 更新最新代码git checkout branch_two #切换到最新的分支branch_twogit merge branch_one #branch_two合并branch_one 这时候就需要你修改这些文件来人肉合并这些 冲突（conflicts） 了。改完之后，你需要执行如下命令以将它们标记为合并成功123git add .git commit -m "冲突解决"git push origin branch_two #更新合并后的代码到远端 在合并改动之前，也可以使用如下命令查看：1git diff &lt;source_branch&gt; &lt;target_branch&gt; 二、使用示例 创建一个新的仓库 123456git clone git@github.com:yourname/code.gitcd codetouch README.mdgit add README.mdgit commit -m "add README"git push -u origin master 从已知文件夹上传到仓库 12345678cd existing_foldergit initgit add .git commit -m "Initial commit"git remote add origin git@github.com:yourname/code.git#获取远程库与本地同步合并（如果远程库不为空必须做这一步，否则后面的提交会失败）git pull --rebase origin mastergit push -u origin master 已知的本地仓库 1234cd existing_repogit remote add origin git@github.com:xiachi/code.gitgit push -u origin --allgit push -u origin --tags 合并分支 123456789git add .git commit -m "合并前先提交代码"git push origin cur_branch #推送到远端git checkout branch_one #切换分支git pull #更新branh_one的代码git checkout cur_branch #切换到之前的分支git merge branch_one #合并#人肉解决冲突git push origin cur_branch 三、git链接多个账号在很多情况下我们在公司使用git需要连接公司的账号，但是如果这个时候需要连接自己的github账号的时候，这个时候就需要在自己电脑上使用git链接两个账号或者多个账号 3.1、生产两个ssh-key12ssh-keygen -t rsa -C &quot;one@gmail.com&quot; #你的github账号ssh-keygen -t rsa -C &quot;two@gmail.com&quot; #你的github账号 注意： 不要一路回车，分别在第一个对话即需要输入的时候,输入文件名（如:id_rsa_one和id_rsa_two），这样会生成对应的文件名的公钥和私钥(如：id_rsa_one、id_rsa_one.pub、id_rsa_two、id_rsa_two.pub) 如果执行的路径不在~/.ssh路径中执行的话，需要把文件拷贝到期目录下 3.2、添加私钥打开ssh-agent 如果你是github官方的bash： 1ssh-agent -s 如果你是其它，比如msysgit： 1eval $(ssh-agent -s) 添加私钥12ssh-add ~/.ssh/id_rsa_onessh-add ~/.ssh/id_rsa_two 3.3、创建或者修改config文件在~/.ssh中如果没有config则使用touch config创建该文件，有的话则进行如下修改: 网上demo示例 1234567891011121314151617181920212223# one(one@gmail.com) Host one.github.com HostName github.com PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa_one User one# two(two@ gmail.com) Host two.github.com HostName github.com PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa_two User two 自己电脑的示例(配置了gitlab和两个github账号) 123456789101112131415Host github_one #和github的名字一致HostName github_one.github.comUser gitIdentityFile /Users/xiachi/.ssh/id_rsa_ucasHost mayexia #mayexiaHostName mayexia.github.comUser gitIdentityFile /Users/xiachi/.ssh/id_rsa_350Host gitlab.comHostName gitlab.comUser gitIdentityFile /Users/xiachi/.ssh/id_rsa 最后想连接github的话，需要将对应的公钥添加到github的ssh秘钥中,添加后便可使用git连接到github了。 四、git基本命令大全12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849git add file_name # 向暂存区添加文件git branch # 查看目前git仓库中已有的分支git branch new_branch_name # 创建分支，无分支起点默认在当前提交上创建分支git branch -d branch_name # 删除分支git branch -D branch_name # 强制删除分支git checkout branch_name # 切换分支git checkout -b branch_name # 新建并切换到该分支git checkout --file_name # 丢弃工作区的修改git commit -m "commit_log" # 保存暂存区记录git commit -am "commit_log" # 保存目录中已被跟踪的文件的暂存区记录git clone remote_repo_url [file_path] # 克隆远程仓库到本地git diff # 比较工作目录和暂存区的差异git diff HEAD^ # 比较工作目录与仓库中最近一次的提交间的差异git diff -cached # 比较暂存区和仓库中最近一次的提交间的差异git fetch remote_repo_name # 从远程仓库抓取最新数据到本地但不与本地分支进行合并git init # 初始化仓库git log # 查看提交日志git log --pretty=short # 只显示提交信息的第一行git log file_name # 只显示指定目录、文件的日志git log -p # 显示文件的改动git log --graph # 用图形查看git log --pretty=oneline # 查看简要信息git merge branch_name # 在 master 分支下进行，合并分支git merge --no-ff -m "merge_log" branch_name # 禁用 Fast forward 模式，并生成一个 commitgit pull origin branch_name # 从远程仓库抓取最新数据并自动与本地分支进行合并git pull --rebase origin branch_name # 第一次拉取的时候先将本地与远程同步git push origin branch_name # 将本地仓库推送到远程仓库中git push -u origin branch_name # 第一次推送时将本地与远程关联起来git push -f origin branch_name # 强制同步远程与本地仓库git push origin tag_name # 将标签推送到远程仓库git rm file_name # 删除仓库文件git reset --hard HEAD^ # 回退到上一个版本git reset --hard commit_id # 回退到指定的版本git reflog # 查看提交命令git reset HEAD -- file_name # 撤销暂存区具体文件的修改git remote # 查看本地已经添加的远程仓库git remote -v # 可以一并查看远程仓库的地址git remote show remote_repo_name # 查看远程仓库信息git remote add origin remote_repo_url # 在本地添加远程仓库git remote rm remote_repo_name # 删除本地添加的远程仓库git remote rename old_name new_name # 重命名远程仓库git status # 查看仓库状态git stash # 隐藏工作现场git stash list # 查看工作现场git stash apply # 恢复工作现场git stash drop # 删除 stash 内容git stash pop # 恢复现场并删除 stash 内容git show commit -commit_id # 查看指定 id 的提交信息git show -all # 显示提交历史 五、参考 git使用简易指南 git教程(百易) 基于git的源代码管理模型(git flow) git的详细教程(廖雪峰)]]></content>
      <categories>
        <category>工程工具</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归---正则、lasso和岭回归、逻辑回归]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2Flasso%E5%92%8C%E5%B2%AD%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[一、前言岭回归与Lasso回归的出现是为了解决线性回归出现的过拟合以及在通过正规方程方法求解θ的过程中出现的$(X^TX)$不可逆这两类问题的，这两种回归均通过在损失函数中引入正则化项来达到目的。 注：在日常机器学习任务中，如果数据集的特征比样本点还多， $(X^TX)^{-1}$ 的时候会出错。岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入$\lambda$限制了所有$\theta^2$之和，通过引入该惩罚项，能够减少不重要的参数,这个技术在统计学上也叫作缩减（shrinkage）。和岭回归类似，另一个缩减（Shrinkage）LASSO 也加入了正则项对回归系数做了限定。 在线性回归中如果参数$\theta$过大、特征过多就会很容易造成过拟合，如下如所示： 二、加入正则为了防止过拟合(θ过大),在目标函数J(θ)后添加复杂度惩罚因子，即正则项来防止过拟合。正则项可以使用l1-norm(lasso)、l2-norm(Ridge)，或结合l1-norm、l2-norm(Elastic Net)。 lasso:使用l1-norm正则 $$J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta^Tx^{(i)})^2+\lambda\sum_{j}^{n}|\theta_j|$$LASSO有特征选择、降维的作用 Ridge:使用l2-norm正则 $$J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta^Tx^{(i)})^2+\lambda\sum_{j}^{n}\theta_j^2$$ 结合l1-norm、l2-norm进行正则 $$J(\theta)=\frac{1}{2}\sum_{i}^{m}(y^{(i)}-\theta^Tx^{(i)})^2+\lambda(\rho\sum_{j}^{n}|\theta_j|+(1-\rho)\sum_{j}^{n}\theta_j^2)$$结合了LASSO的特征选择作用，和Ridge较好的效果 三、解释lasso特征选择上图中左边为lasso回归，右边为岭回归。红色的椭圆和蓝色的区域的切点就是目标函数的最优解，我们可以看到，如果是圆，则很容易切到圆周的任意一点，但是很难切到坐标轴上，因此没有稀疏；但是如果是菱形或者多边形，则很容易切到坐标轴上，因此很容易产生稀疏的结果。这也说明了为什么L1范式会是稀疏的。这样就解释了为什么lasso可以进行特征选择。岭回归虽然不能进行特征筛选，但是对$\theta$的模做约束，使得它的数值会比较小，很大程度上减轻了overfitting的问题。 注：岭回归：消除共线性；模的平方处理；Lasso回归：压缩变量，起降维作用；模处理 四、逻辑回归4.1 模型介绍在前面我们已经介绍了线性回归，其表达式如下:$$z=\theta_0 + \theta_1 x_1 + \theta_2 x_2+….+\theta_n x_n=\theta^T x$$而对于Logistic Regression来说，其思想也是基于线性回归（Logistic Regression属于广义线性回归模型）。其公式如下：$$h_{\theta}(x)=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-\theta^{T}x}}$$其中$y=\frac{1}{1+e^{-x}}$被称作sigmoid函数，我们可以看到，Logistic Regression算法是将线性函数的结果映射到了sigmoid函数中。sigmoid的函数图形如下:我们可以看到，sigmoid的函数输出是介于(0,1)之间的，中间值是0.5，于是之前的公式$h_{\theta}(x)$的含义就很好理解了，因为$h_{\theta}(x)$输出是介于(0,1)之间，也就表明了数据属于某一类别的概率，例如 ：$h_{\theta}(x)&lt;0.5$ 则说明当前数据属于A类；$h_{\theta}(x)&gt;0.5$则说明当前数据属于B类。所以我们可以将sigmoid函数看成样本数据的概率密度函数。 4.2 原理推导有了上面的公式，我们接下来需要做的就是怎样去估计参数$\theta$了。$\theta$函数的值有特殊的含义，它表示$h_{\theta}(x)$结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为:$$\begin{split}{} &amp; P(y=1|x;\theta)=h_{\theta}(x) \\{} &amp; P(y=0|x;\theta)=1-h_{\theta}(x)\end{split}$$接下来可以使用概率论中极大似然估计的方法去求解损失函数，首先得到概率函数为：$$P(y|x;\theta)=(h_\theta(x))^y * (1-h_\theta(x))^{1-y}$$因为样本数据(m个)独立，所以它们的联合分布可以表示为各边际分布的乘积,取似然函数为： $$L(\theta)=\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\theta) =\prod_{i=1}^{m}(h_\theta(x^{(i)}))^{y^{(i)}} * (1-h_\theta(x^{(i)}))^{1-y^{(i)}}$$取对数似然函数： $$\begin{split}l(\theta)=log(L(\theta)) {} &amp; =\sum_{i=1}^{m}log((h_\theta(x^{(i)}))^{y^{(i)}})+log((1-h_\theta(x^{(i)}))^{1-y^{(i)}}) \\ {} &amp; =\sum_{i=1}^{m}y^{(i)}log(h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))\end{split}$$最大似然估计就是要求得使$l(\theta)$取最大值时的$\theta$，这里可以使用梯度上升法求解。我们稍微变换一下：$$J(\theta)=-\frac{1}{m}l(\theta)$$因为乘了一个负的系数$-\frac{1}{m}$，然后就可以使用梯度下降算法进行参数求解了。 以上的$l(\theta)便是逻辑回归的损失函数了$ 4.3、逻辑回归与交叉熵线性回归我们采用均方误差（MSE）作为损失函数，而逻辑回归（分类算法）采用交叉熵（Cross Entropy）。逻辑回归得到的$h_{\theta}(x)$可以看作是分类结果的概率。可以这么想，每个样本中的分类结果，在样本空间形成一个结果的分布。当有一个新的样本加入进来，把它对应到样本空间后，可以在已有的分布下对应到它的分类结果。假设样本的真实分布为sin(x)，而我们最开始的假设分布为cos(x)，我们的目标是使假设分布无限接近真实分布甚至与真实分布重合。要描述两种分布之间的差异程度，我们引入交叉熵：概率分布一：p(x) ；概率分布二：q(x)$$Hp(q)=\sum p(x)log_{2}^{\frac{1}{q(x)}}=-\sum p(x)log_{2}^{q(x)}$$交叉熵描述了两个不同的概率分布p和q的差异程度，两个分布差异越大，则交叉熵的差异越大。详情 交叉熵和熵的关系，它们差异越大，两个分布差异越大，则这两个关系的差异就越大。如果两个分布是一样的，则交叉熵和熵的差异是 0。我们把这个差异叫做相对熵 (Kullback–Leibler divergence), 也叫 KL 距离，KL 散度。Dq(p)=Hq(p)−H(p)。 它就像一个距离度量一样，描述两个概率分布的“距离” 逻辑回归的loss为:$J(\theta)=-[yln(h_{\theta}(x))+(1-y)ln(1-h_{\theta}(x))]$$h_{\theta}(x)$表示分类结果为1的概率,$1-h_{\theta}(x)$表示分类结果为0的概率。当分类结果为1时,y=1,即真实分布中结果为1的概率是1，结果为0的概率为0。逻辑回归采用交叉熵作为代价函数，原理就是，模型的分布尽可能的接近样本的概率分布，用这样的模型进行预测才是最优化和最准确的。 交叉熵：softmax回归的交叉熵损失函数：$$CrossEntropy=-\sum_{i=1}^{m}y_ilog(p_i)$$其中$p_i$是softmax回归求得的结果,$y_i$是实际的类别 五、参考 《机器学习课件》–邹博 《CSDN博客-lasso回归》 CSDN博客-Logistic Regression（逻辑回归）原理及公式推导 逻辑回归与交叉熵]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归---线性回归]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[一、回归回归是监督学习的一个重要问题，回归用于预测输入变量和输出变量之间的关系。回归模型是表示输入变量到输出变量之间映射的函数。回归问题的学习等价于函数拟合：使用一条函数曲线使其很好的拟合已知函数且很好的预测未知数据,分为模型的学习和预测两个过程,模型学习是基于给定的训练数据集构建一个模型，预测是根据新的输入数据预测相应的输出。回归的分类： 按照输入变量的个数可以分为一元回归和多元回归； 按照输入变量和输出变量之间关系的类型，可以分为线性回归和非线性回归 二、线性回归2.1、介绍1、单变量线性回归（一元回归）：形如$h(x)=\theta_0+\theta_1*x_1$,如下图所示2、多变量线性回归(多元回归)：形如：$h(x)=\theta_0+\theta_1x_1+\theta_2x_2=\theta^TX$，如下图所示3、多项式回归（Polynomial Regression）：形如$h(x)=\theta_0+\theta_1x_1+\theta_2x_2^2+\theta_3x_3^3$或者$h(x)=\theta_0+\theta_1x_1+\theta_2\sqrt{x_2}$,但是我们可以令$x_2=x_2^2$，$x_3=x_3^3$，于是又将其转化为了线性回归模型。虽然不能说多项式回归问题属于线性回归问题，但是一般我们就是这么做的。所以最终通用表达式就是：$$h_\theta(x)=\theta^TX=\theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+….+\theta_nx_n$$ 2.2、目标函数1、依据最小二乘法获取目标函数我们需要训练一条直线尽量去拟合数据集中的每个点，但是一条直线去拟合所有的点都在这条直线上肯定不现实，所以我们希望这些点尽量离这条直线近一点即去找每个点和直线的距离最小的那条直线。对于每个点直线预测出来的值和实际值都会有一个偏差($y_i-(\theta_0+\theta_1*x_1)$),在训练的过程中目标就是是的loss最小，那么如何最小化误差，使用最小二乘法可以解决该问题: $$loss=\dfrac{1}{N}\sum_i^N(y_i-(\theta_1x_1+\theta_0))^2=\dfrac{1}{N}\sum_i^m(y^{(i)}-\theta^Tx^{(i)})^2$$ 注：最小二乘法又称最小平方法，它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。 2、依据最大似然估计获取目标函数现在假设我们有m个样本，我们假设有：$y^{(i)}=\theta_Tx^{(i)}+\varepsilon^{(i)}$误差$\varepsilon^{(i)}$（$ 1\leq i \leq m$）是独立同分布的，服从均值为0，方差为某定制$\sigma^2$的高斯分布，原因：中心极限定理 中心极限定理 的意义：实际问题中，很多随机现象可以看做众多因素的独立影响的综合反应，往往近似服从正态分布，比如：城市耗电量（大量用户的耗电量总和）、测量误差（许多观察不到的微小误差总和）注：应用前提是多个随机变量的和，有些问题是乘性误差，需要鉴别或者取对数后使用 依据上面的结论，则有:$$p(\varepsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\varepsilon^{(i)})^2}{2\sigma^2})$$带入$\varepsilon^{(i)}$后有：$$p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\dfrac{(y^{(i)}-(\theta^Tx^{(i)}))^2}{2\sigma^2})$$表示在$\theta$给定的时候，给出一个对应的$x^{(i)}$的时候会算出对应的$y^{(i)}$的概率密度，因为样本是独立同分布，则$p(y_1y_2y_3….y_m)$的联合概率为各自的边缘概率的乘积，则有：$$L(\theta)=\prod_i^mp(y^{(i)}|x^{(i)};\theta)=\prod_i^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\dfrac{(y^{(i)}-(\theta^Tx^{(i)}))^2}{2\sigma^2})$$$L(\theta)$即为似然函数。 根据极大似然估计的定义,我们需要$L(\theta)$最大，那么我们怎么才能是的这个值最大呢？两边取对数对这个表达式进行化简如下： 注:在此我们取了一个对数似然，有两点优势，一个就是可以将乘法转换为加法，第二个取对数是单调上升函数，取最大似然和取对数最大似然目标是一致的 需要$L(\theta)$最大，也就是只要满足上述公式中最后一项的后半部分最小。 注：因为对于$L(\theta)$而言变量为$\theta$，所以$mlog\frac{1}{\sqrt{2\pi}\sigma}$是一个常数 则令： $$J(\theta)=\frac{1}{2}\sum_i^m(y^{(i)}-\theta^Tx^{(i)})^2$$ 则若需要$L(\theta)$最大，则只需$J(\theta)$最小即可，则目标函数转换为$J(\theta)$，也即损失函数。 所以，我们最后由极大似然估计推导得出，我们希望 $J(\theta)$最小，而这刚好就是最小二乘法做的工作。而回过头来我们发现，之所以最小二乘法有道理，是因为我们之前假设误差项服从高斯分布，假如我们假设它服从别的分布，那么最后的目标函数的形式也会相应变化。好了，上边我们得到了有极大似然估计或者最小二乘法，我们的模型求解需要最小化目标函数$J(\theta)$，那么我们的$\theta$到底怎么求解呢？有没有一个解析式可以表示$\theta$? 2.3、目标函数求解依据上面的推倒，我们已经建立了目标函数，最终我们需要依据目标函数 $J(\theta)$来求解$\theta$即模型的参数。求解方式有两种，一种是解析解，另外一种方式是使用梯度下降的方式求解。 2.3.1、解析解目标函数为： 我们需要最小化目标函数，关心$\theta$取什么值的时候使得目标函数取得最小值。而目标函数连续，那么$\theta$一定为目标函数的驻点，所以我们求导寻找驻点。求导可得： 最终我们得到参数 $\theta$的解析式： $X^TX$ 是一个半正定矩阵，所以若$X^TX$ 不可逆或为了防止过拟合，我们增加$\lambda$扰动，得到 从另一个角度来看，这相当与给我们的线性回归参数增加一个惩罚因子，这是必要的，我们数据是有干扰的，不正则的话有可能数据对于训练集拟合的特别好，但是对于新数据的预测误差很大。 2.3.2、梯度下降求解上述给出了解析解的求法，如果特征规模很大，超过了1000维，那么矩阵的逆是不方便求的，因此我们可以使用梯度下降算法来做。求解过程如下：(1) 初始化θ(随机初始化)(2) 沿着负梯度方向迭代，更新后的θ使得J(θ)更小： $$\theta_j=\theta_j-\alpha\frac{\partial J(\theta)}{\partial \theta}$$其中$\alpha$是步长、学习率，超参数，可以指定一个较小的步长，也可以使用折半查找、回溯、拟牛顿来求一个动态的$\alpha$。然后求解$\frac{\partial J(\theta)}{\partial \theta}$如下： 梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。 梯度下降算法分为 批量梯度下降 、随机梯度下降 和 mini-bach梯度下降 方式。 批量梯度下降 方式如下：其中有个∑从i到m加和，也就是说每更新一次都需要那全部的样本做一次梯度下降，这样的话其实效率比较低。 随机梯度下降 方式如下：这种方法每次是拿一个样本来迭代一次梯度，效率会比较高 mini-bach梯度下降每次拿到若干个样本的平均梯度作为更新方向，属于 批量梯度下降 和 随机梯度下降 的折中方式，在CNN中就是使用的该方式。 2.4、解析求解和梯度下降求解对比 梯度下降Gradient Descent 正规方程 Normal Equation 缺点： 1、需要选择学习率$\alpha$2、需要多次迭代3、特征值范围相差太大时，需要特征缩放 优点： 1、不需要选择学习率$\alpha$2、不需要多次迭代3、不需要特征缩放(feature scaling) 优点：当特征维度n很大时，能够很好工作 缺点：当特征维度n很大时，运行很慢 三、参考《统计机器学习方法》–李航《机器学习课件》–邹博《Machine Learning》 –Andrew NG]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客搭建]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%B7%A5%E5%85%B7%2FHexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[一、前言现有的博客产品已经非常多了，比如：博客园、CSDN、简书等等，在这些平台都可以发布自己的博客内容，为何还需要自己搭建博客？我个人感觉是这些平台后台编写markdown会很费劲，其次内容是在其他的平台上不可控，然后可能这些界面的风格不是太喜欢。所以作为一名搬砖的码农，还是走点技术流的路线吧。你看到的本博客就是使用hexo框架搭建的，其实目前有很多人使用这个搭建个人博客或者团队的技术博客。比如阿里的中间件博客、一些技术博客(nlp内容居多)就是使用hexo搭建的。Hexo官网：https://hexo.io/zh-cn/ 安装hexo只需要执行以下命令即可(如果没安装npm可以自行百度安装)：1npm install hexo-cli -g 二、使用hexo创建博客经过上述步骤我们已经安装好了hexo了，然后我们创建一个文件夹blog，让后执行命令1234cd bloghexo initnpm installhexo server 在浏览器中输入(http://localhost:4000/)就可以看到以下的界面 三、更换hexo主题hexo默认的主题是landspace,我们选择next主题(github)、官网来进行主题更换示例，该主题也是github上人气最高的主题之一，本博客选择的就是该主题。在_config.yml中修改theme: next,然后重启hexo服务，就可以看到如下画面:在项目中有两个_config.yml，一个是主题的在文件夹themes/next下，另外一个在博客的根目录下在博客的_config.yml中配置如下:12345678# Sitetitle: 马野的博客subtitle:description:keywords:author: 马野language: zh-Hanstimezone: 在主题_config.yml中对配置如下修改:1234567891011121314151617menu: home: / about: /about tags: /tags categories: /categories archives: /archivesscheme: Mistauto_excerpt: enable: true length: 150mathjax: enable: true per_page: false cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML 然后执行12hexo new page &quot;tags&quot;hexo new page &quot;categories&quot; 然后在tags的index.md中添加type: &quot;tags&quot;，在categories同理,则可以看到现在的界面新建一篇文章运行指令hexo new “your tittle”，然后在source/_post/目录中看到自己新建的文章，格式是md格式的，现在我们可以使用markdown语法来编写我们的博客。 四、关于公式和markdown的图床如何解决在上述的主题配置中已经把mathjax设置生效了，如果还不能生效则尝试安装npm install hexo-math。关于图床之前网上很多都是说使用七牛云做为图床，个人感觉比较麻烦而且不易管理。其实可以直接把图和博客一起发布到GitHub-pages,方便而且比较好管理。在source中建立文件夹imgs,然后将图片放到文件夹中，比如放入的图片为test.png，则在需要引入的markdown中这样引入![test_png](/imgs/test.png)即可。然后刷新就可以看到图了，后面即使部署到github-pages也是可以生效的。 五、将博客部署到github-pagesGitHub Pages 是通过我们网站托管和发布的公开网页。你可以通过 Automatic Page Generator 在线创建和发布GitHub Pages。如果你更喜欢本地操作，你可以使用Mac或者Windows平台的GitHub App，或者使用命令行。 5.1、git链接github5.1.1、生成ssh key手先需要本地安装git,然后需要与github链接需要生成ssh key,生成方式如下，：12cd ~/.ssh ssh-keygen -t rsa -C "yours@gmail.com" #你的github账号 注意： 不要一路回车，分别在第一个对话即需要输入的时候,输入文件名（如:id_rsa_one），这样会生成对应的文件名的公钥和私钥(如：id_rsa_one、id_rsa_one.pub) 如果执行的路径不在~/.ssh路径中执行的话，需要把文件拷贝到期目录下 5.1.2、添加公钥到github然后登陆github，点击自己头像，点击setting，如下:然后在点击SSH and GPG Keys，如下：然后点击new ssh key将公钥(id_rsa_one.pub)中的内容填到其中即可，就会出现如下: 5.1.3、添加私钥打开ssh-agent 如果你是github官方的bash 1ssh-agent -s 如果你是其它，比如msysgit： 1eval $(ssh-agent -s) 添加私钥1ssh-add ~/.ssh/id_rsa_one 这个时候你的本地的git就可以连接上github啦。 5.2、创建github-pages工程需要让博客能够在公网访问需要在你的GitHub账号中新建一个github-pages的Repositories。点击自身头像并点击your profile然后在点击Repositories后点击new 就会出现以下界面：我自己的用户名为Mayexia,也就是红色框框的前面一部分,所有后面就应该填写Mayexia.github.io（注意：一定要同自己的用户名一致） 5.3、发布博客并管理源码在博客的源_config.yml中修改deploy配置如下：1234deploy: type: git repo: git@github.com:yours/yours.github.io.git #修改成自己的repo的地址 branch: master 然后在执行如下命令: 123npm install hexo-deployer-git --save #先安装hexo部署插件hexo clean #清楚缓存hexo g -d #生成并部署 然后在浏览器中输入yours.github.io(替换成自己的)，就可以看到你的博客就搭建完成啦。博客虽然发布了，我们可以访问到了，但是上传到github-pages是hexo渲染后的静态的web文件，我们的写博客的源码还是需要备份和管理的。既然我们已经连到了github，我们可以使用github来对我们的博客的源码进行版本管理和备份，这样写完一篇博文后立马可以发布到github-pages同时也可以将源码上传到github进行版本管理。 六、next主题配置因为网上有很多很详细很全的配置教程，我就不在这里讲述如何配置主题了，以下两个博文写的很详细，可以按照自己的喜好进行配置。 hexo的next主题个性化教程：打造炫酷网站 Hexo的Next主题详细配置 七、Hexo常用命令1234567hexo new "postName" #新建文章hexo new page "pageName" #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）hexo deploy #将.deploy目录部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本 八、关于markdown编辑器网上也有很多markdown编辑器,其实我个人觉得使用hexo编写博客好处就是可以随便找一个编辑器，然后在本地将博客服务启动起来，随时修改就可以看得到效果，不用担心线下和线上的markdown语法不兼容的问题，而且插件比较多可以满足个性化的定制。编辑器的话我个人倾向于使用atom编辑器来撰写博文然后发布。markdown的语法学习链接： markdown公式语法 markdown基本语法 九、其他使用next主题的时候发现markdown的表格时候需要与文字中间空一行才会生效。我看到网上有相关的话题说使用GitHub搭建博客是不道德的事情，个人觉得这不是什么不道德的事情，毕竟GitHub官方也出了个gitbook可以让大家编写电子书籍发布到gitbook上,或许还是支持这么干的呢(个人猜想啦)，希望这篇博文能够帮助到大家搭建一个属于自己的博客，享受分享知识的乐趣。]]></content>
      <categories>
        <category>工程工具</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客改动的参考链接]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%B7%A5%E5%85%B7%2F%E5%8D%9A%E5%AE%A2%E6%94%B9%E5%8A%A8%E7%9A%84%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[主题改动 LaTEX的改动 hexo的next主题个性化教程：打造炫酷网站 Hexo的Next主题详细配置 hexo + next主题高级配置 访问统计显示 hexo博客的SEO优化 hexo 公式 markdown语法参考 markdown公式语法 markdown基本语法 markdwon数学公式简要 markdown希腊字母对应表 添加博客评论 Hexo使用Valine评论系统不显示自定义头像的解决方案 为你的Hexo加上评论系统-Valine]]></content>
      <categories>
        <category>工程工具</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
